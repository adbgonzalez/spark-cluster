services:
  master:
    image: adbgonzalez/hadoop-spark:3.5
    container_name: spark-master
    environment:
      - SPARK_ROLE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark

    ports:
      - "8081:8080"  # Puerto para la interfaz web de Spark
      - "4041:4040"
      - "7077:7077"
      - "18080:18080"
      - "9870:9870"
      - "9000:9000"
    volumes:
      - ./spark_data:/opt/spark/spark_data
      - ./spark-logs:/opt/spark/logs
      - ./namenode_data:/usr/local/hadoop/data/namenode/
      - ./start-hdfs.sh:/start-hdfs.sh
    networks:
      - network_cluster
  worker-1:
    image: adbgonzalez/hadoop-spark:3.5
    container_name: spark-worker-1
    environment:
      - SPARK_ROLE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=/opt/spark/logs/history
    volumes:
      - ./spark_data:/opt/spark/spark_data
      - ./spark-logs:/opt/spark/logs
      - ./datanode1_data:/usr/local/hadoop/data/datanode/
    ports:
      - "8082:8080"  # Puerto para la interfaz web de Spark
      - "10071:50070"

    depends_on:
      - master
    networks:
      - network_cluster
  worker-2:
    image: adbgonzalez/hadoop-spark:3.5
    container_name: spark-worker-2
    environment:
      - SPARK_ROLE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=/opt/spark/logs/history
    volumes:
      - ./spark_data:/opt/spark/spark_data
      - ./spark-logs:/opt/spark/logs
      - ./datanode2_data:/usr/local/hadoop/data/datanode/
    ports:
      - "8083:8080"  # Puerto para la interfaz web de Spark
 
    depends_on:
      - master
    networks:
      - network_cluster

  jupyter-notebook:
    image: adbgonzalez/spark-notebook:3.5  # Aquí usamos la nueva imagen
    container_name: jupyter-notebook
    ports:
      - "8888:8888"  # Puerto para Jupyter
      - "9999:9999"
      - "4042:4040"
      - "18082:18080"
    networks:
      - network_cluster
    environment:
      - SPARK_EVENTLOG_ENABLED=true
      - SPARK_EVENTLOG_DIR=/opt/spark/logs/history
      - GRANT_SUDO=yes
      - NOTEBOOK_ARGS=--allow-root
      - SPARK_MASTER_URL=spark://spark-master:7077  # Conectar con el master
      - SHELL=/bin/bash
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./work:/home/jovyan/work  # Montamos el directorio de trabajo
      - ./spark-logs:/opt/spark/logs

    depends_on:
      - master  # Espera a que el master esté disponible antes de iniciarse
  resourcemanager:
    image: adbgonzalez/hadoop-spark:3.5
    container_name: resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"
    networks:
      - network_cluster
    command: yarn resourcemanager
  nodemanager: 
    image: adbgonzalez/hadoop-spark:3.5
    container_name: nodemanager
    hostname: nodemanager
    networks:
      - network_cluster
    command: yarn nodemanager


volumes:
  spark_data:
networks:
  network_cluster:


