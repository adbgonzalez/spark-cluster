{"Event":"SparkListenerLogStart","Spark Version":"3.5.5"}
{"Event":"SparkListenerResourceProfileAdded","Resource Profile Id":0,"Executor Resource Requests":{"memory":{"Resource Name":"memory","Amount":1024,"Discovery Script":"","Vendor":""},"offHeap":{"Resource Name":"offHeap","Amount":0,"Discovery Script":"","Vendor":""}},"Task Resource Requests":{"cpus":{"Resource Name":"cpus","Amount":1.0}}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"7bbc4ec40e69","Port":32801},"Maximum Memory":455501414,"Timestamp":1741611462735,"Maximum Onheap Memory":455501414,"Maximum Offheap Memory":0}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/usr/lib/jvm/java-17-openjdk-amd64","Java Version":"17.0.14 (Debian)","Scala Version":"version 2.12.18"},"Spark Properties":{"spark.executor.extraJavaOptions":"-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","spark.driver.host":"7bbc4ec40e69","spark.serializer.objectStreamReset":"100","spark.history.fs.logDirectory":"file:///opt/spark/logs/history","spark.eventLog.enabled":"true","spark.driver.port":"35815","spark.rdd.compress":"True","spark.app.name":"01-rdd1","spark.scheduler.mode":"FIFO","spark.submit.pyFiles":"","spark.ui.showConsoleProgress":"true","spark.app.submitTime":"1741611460517","spark.app.startTime":"1741611460776","spark.executor.id":"driver","spark.driver.extraJavaOptions":"-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","spark.submit.deployMode":"client","spark.master":"spark://spark-master:7077","spark.eventLog.dir":"file:///opt/spark/logs/history","spark.app.id":"app-20250310125742-0000"},"Hadoop Properties":{"hadoop.service.shutdown.timeout":"30s","yarn.resourcemanager.amlauncher.thread-count":"50","yarn.sharedcache.enabled":"false","fs.s3a.connection.maximum":"96","yarn.nodemanager.numa-awareness.numactl.cmd":"/usr/bin/numactl","fs.viewfs.overload.scheme.target.o3fs.impl":"org.apache.hadoop.fs.ozone.OzoneFileSystem","fs.s3a.impl":"org.apache.hadoop.fs.s3a.S3AFileSystem","yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms":"1000","yarn.timeline-service.timeline-client.number-of-async-entities-to-merge":"10","hadoop.security.kms.client.timeout":"60","hadoop.http.authentication.kerberos.principal":"HTTP/_HOST@LOCALHOST","mapreduce.jobhistory.loadedjob.tasks.max":"-1","yarn.resourcemanager.application-tag-based-placement.enable":"false","mapreduce.framework.name":"local","yarn.sharedcache.uploader.server.thread-count":"50","yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min":"3600","yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern":"^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$","tfile.fs.output.buffer.size":"262144","yarn.app.mapreduce.am.job.task.listener.thread-count":"30","yarn.nodemanager.node-attributes.resync-interval-ms":"120000","yarn.nodemanager.container-log-monitor.interval-ms":"60000","hadoop.security.groups.cache.background.reload.threads":"3","yarn.resourcemanager.webapp.cross-origin.enabled":"false","fs.AbstractFileSystem.ftp.impl":"org.apache.hadoop.fs.ftp.FtpFs","fs.viewfs.overload.scheme.target.gs.impl":"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS","hadoop.registry.secure":"false","hadoop.shell.safely.delete.limit.num.files":"100","mapreduce.job.acl-view-job":" ","fs.s3a.s3guard.ddb.background.sleep":"25ms","fs.s3a.retry.limit":"7","mapreduce.jobhistory.loadedjobs.cache.size":"5","fs.s3a.s3guard.ddb.table.create":"false","fs.viewfs.overload.scheme.target.s3a.impl":"org.apache.hadoop.fs.s3a.S3AFileSystem","yarn.nodemanager.amrmproxy.enabled":"false","yarn.timeline-service.entity-group-fs-store.with-user-dir":"false","mapreduce.shuffle.pathcache.expire-after-access-minutes":"5","mapreduce.input.fileinputformat.split.minsize":"0","yarn.resourcemanager.container.liveness-monitor.interval-ms":"600000","yarn.resourcemanager.client.thread-count":"50","io.seqfile.compress.blocksize":"1000000","yarn.nodemanager.runtime.linux.docker.allowed-container-runtimes":"runc","fs.viewfs.overload.scheme.target.http.impl":"org.apache.hadoop.fs.http.HttpFileSystem","yarn.resourcemanager.nodemanagers.heartbeat-interval-slowdown-factor":"1.0","yarn.sharedcache.checksum.algo.impl":"org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl","yarn.nodemanager.amrmproxy.interceptor-class.pipeline":"org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor","yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size":"10485760","mapreduce.reduce.shuffle.fetch.retry.interval-ms":"1000","mapreduce.task.profile.maps":"0-2","yarn.scheduler.include-port-in-node-name":"false","yarn.nodemanager.admin-env":"MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX","yarn.resourcemanager.node-removal-untracked.timeout-ms":"60000","mapreduce.am.max-attempts":"2","hadoop.security.kms.client.failover.sleep.base.millis":"100","mapreduce.jobhistory.webapp.https.address":"0.0.0.0:19890","yarn.node-labels.fs-store.impl.class":"org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore","yarn.nodemanager.collector-service.address":"${yarn.nodemanager.hostname}:8048","fs.trash.checkpoint.interval":"0","mapreduce.job.map.output.collector.class":"org.apache.hadoop.mapred.MapTask$MapOutputBuffer","yarn.resourcemanager.node-ip-cache.expiry-interval-secs":"-1","hadoop.http.authentication.signature.secret.file":"*********(redacted)","hadoop.jetty.logs.serve.aliases":"true","yarn.resourcemanager.placement-constraints.handler":"disabled","yarn.timeline-service.handler-thread-count":"10","yarn.resourcemanager.max-completed-applications":"1000","yarn.nodemanager.aux-services.manifest.enabled":"false","yarn.resourcemanager.system-metrics-publisher.enabled":"false","yarn.resourcemanager.placement-constraints.algorithm.class":"org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm","yarn.sharedcache.webapp.address":"0.0.0.0:8788","fs.s3a.select.input.csv.quote.escape.character":"\\\\","yarn.resourcemanager.delegation.token.renew-interval":"*********(redacted)","yarn.sharedcache.nm.uploader.replication.factor":"10","hadoop.security.groups.negative-cache.secs":"30","yarn.app.mapreduce.task.container.log.backups":"0","mapreduce.reduce.skip.proc-count.auto-incr":"true","fs.viewfs.overload.scheme.target.swift.impl":"org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem","hadoop.security.group.mapping.ldap.posix.attr.gid.name":"gidNumber","ipc.client.fallback-to-simple-auth-allowed":"false","yarn.nodemanager.resource.memory.enforced":"true","yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.enable-batch":"false","yarn.client.failover-proxy-provider":"org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider","yarn.timeline-service.http-authentication.simple.anonymous.allowed":"true","ha.health-monitor.check-interval.ms":"1000","yarn.nodemanager.runtime.linux.runc.host-pid-namespace.allowed":"false","hadoop.metrics.jvm.use-thread-mxbean":"false","ipc.[port_number].faircallqueue.multiplexer.weights":"8,4,2,1","yarn.acl.reservation-enable":"false","yarn.resourcemanager.store.class":"org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore","yarn.app.mapreduce.am.hard-kill-timeout-ms":"10000","fs.s3a.etag.checksum.enabled":"false","yarn.nodemanager.container-metrics.enable":"true","ha.health-monitor.rpc.connect.max.retries":"1","yarn.timeline-service.client.fd-clean-interval-secs":"60","yarn.resourcemanager.nodemanagers.heartbeat-interval-scaling-enable":"false","yarn.resourcemanager.nodemanagers.heartbeat-interval-ms":"1000","hadoop.common.configuration.version":"3.0.0","fs.s3a.s3guard.ddb.table.capacity.read":"0","yarn.nodemanager.remote-app-log-dir-suffix":"logs","yarn.nodemanager.container-log-monitor.dir-size-limit-bytes":"1000000000","yarn.nodemanager.windows-container.cpu-limit.enabled":"false","yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed":"false","file.blocksize":"67108864","hadoop.http.idle_timeout.ms":"60000","hadoop.registry.zk.retry.ceiling.ms":"60000","yarn.scheduler.configuration.leveldb-store.path":"${hadoop.tmp.dir}/yarn/system/confstore","yarn.sharedcache.store.in-memory.initial-delay-mins":"10","mapreduce.jobhistory.principal":"jhs/_HOST@REALM.TLD","mapreduce.map.skip.proc-count.auto-incr":"true","fs.s3a.committer.name":"file","mapreduce.task.profile.reduces":"0-2","hadoop.zk.num-retries":"1000","yarn.webapp.xfs-filter.enabled":"true","fs.viewfs.overload.scheme.target.hdfs.impl":"org.apache.hadoop.hdfs.DistributedFileSystem","seq.io.sort.mb":"100","yarn.scheduler.configuration.max.version":"100","yarn.timeline-service.webapp.https.address":"${yarn.timeline-service.hostname}:8190","yarn.resourcemanager.scheduler.address":"${yarn.resourcemanager.hostname}:8030","yarn.node-labels.enabled":"false","yarn.resourcemanager.webapp.ui-actions.enabled":"true","mapreduce.task.timeout":"600000","yarn.sharedcache.client-server.thread-count":"50","hadoop.security.groups.shell.command.timeout":"0s","hadoop.security.crypto.cipher.suite":"AES/CTR/NoPadding","yarn.nodemanager.elastic-memory-control.oom-handler":"org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler","yarn.resourcemanager.connect.max-wait.ms":"900000","fs.defaultFS":"file:///","yarn.minicluster.use-rpc":"false","ipc.[port_number].decay-scheduler.decay-factor":"0.5","fs.har.impl.disable.cache":"true","yarn.webapp.ui2.enable":"false","io.compression.codec.bzip2.library":"system-native","yarn.webapp.filter-invalid-xml-chars":"false","yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs":"600","fs.s3a.select.input.csv.record.delimiter":"\\n","fs.s3a.change.detection.source":"etag","ipc.[port_number].backoff.enable":"false","yarn.nodemanager.distributed-scheduling.enabled":"false","mapreduce.shuffle.connection-keep-alive.timeout":"5","yarn.resourcemanager.webapp.https.address":"${yarn.resourcemanager.hostname}:8090","yarn.webapp.enable-rest-app-submissions":"true","mapreduce.jobhistory.address":"0.0.0.0:10020","yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs":"*********(redacted)","yarn.is.minicluster":"false","yarn.nodemanager.address":"${yarn.nodemanager.hostname}:0","fs.abfss.impl":"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem","fs.AbstractFileSystem.s3a.impl":"org.apache.hadoop.fs.s3a.S3A","mapreduce.task.combine.progress.records":"10000","yarn.resourcemanager.epoch.range":"0","yarn.resourcemanager.am.max-attempts":"2","yarn.nodemanager.runtime.linux.runc.image-toplevel-dir":"/runc-root","yarn.nodemanager.linux-container-executor.cgroups.hierarchy":"/hadoop-yarn","fs.AbstractFileSystem.wasbs.impl":"org.apache.hadoop.fs.azure.Wasbs","yarn.timeline-service.entity-group-fs-store.cache-store-class":"org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore","yarn.nodemanager.runtime.linux.runc.allowed-container-networks":"host,none,bridge","fs.ftp.transfer.mode":"BLOCK_TRANSFER_MODE","ipc.server.log.slow.rpc":"false","ipc.server.reuseaddr":"true","fs.ftp.timeout":"0","yarn.resourcemanager.node-labels.provider.fetch-interval-ms":"1800000","yarn.router.webapp.https.address":"0.0.0.0:8091","yarn.nodemanager.webapp.cross-origin.enabled":"false","fs.wasb.impl":"org.apache.hadoop.fs.azure.NativeAzureFileSystem","yarn.resourcemanager.auto-update.containers":"false","yarn.app.mapreduce.am.job.committer.cancel-timeout":"60000","yarn.scheduler.configuration.zk-store.parent-path":"/confstore","yarn.nodemanager.default-container-executor.log-dirs.permissions":"710","yarn.app.attempt.diagnostics.limit.kc":"64","fs.viewfs.overload.scheme.target.swebhdfs.impl":"org.apache.hadoop.hdfs.web.SWebHdfsFileSystem","yarn.client.failover-no-ha-proxy-provider":"org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider","fs.s3a.change.detection.mode":"server","ftp.bytes-per-checksum":"512","yarn.nodemanager.resource.memory-mb":"-1","fs.AbstractFileSystem.abfs.impl":"org.apache.hadoop.fs.azurebfs.Abfs","yarn.timeline-service.writer.flush-interval-seconds":"60","fs.s3a.fast.upload.active.blocks":"4","yarn.resourcemanager.submission-preprocessor.enabled":"false","hadoop.security.credential.clear-text-fallback":"true","yarn.nodemanager.collector-service.thread-count":"5","ipc.[port_number].scheduler.impl":"org.apache.hadoop.ipc.DefaultRpcScheduler","fs.azure.secure.mode":"false","mapreduce.jobhistory.joblist.cache.size":"20000","fs.ftp.host":"0.0.0.0","yarn.timeline-service.writer.async.queue.capacity":"100","yarn.resourcemanager.fs.state-store.num-retries":"0","yarn.resourcemanager.nodemanager-connect-retries":"10","yarn.nodemanager.log-aggregation.num-log-files-per-app":"30","hadoop.security.kms.client.encrypted.key.cache.low-watermark":"0.3f","fs.s3a.committer.magic.enabled":"true","yarn.timeline-service.client.max-retries":"30","dfs.ha.fencing.ssh.connect-timeout":"30000","yarn.log-aggregation-enable":"false","yarn.system-metrics-publisher.enabled":"false","mapreduce.reduce.markreset.buffer.percent":"0.0","fs.AbstractFileSystem.viewfs.impl":"org.apache.hadoop.fs.viewfs.ViewFs","yarn.resourcemanager.nodemanagers.heartbeat-interval-speedup-factor":"1.0","mapreduce.task.io.sort.factor":"10","yarn.nodemanager.amrmproxy.client.thread-count":"25","ha.failover-controller.new-active.rpc-timeout.ms":"60000","yarn.nodemanager.container-localizer.java.opts":"-Xmx256m","mapreduce.jobhistory.datestring.cache.size":"200000","mapreduce.job.acl-modify-job":" ","yarn.nodemanager.windows-container.memory-limit.enabled":"false","yarn.timeline-service.webapp.address":"${yarn.timeline-service.hostname}:8188","yarn.app.mapreduce.am.job.committer.commit-window":"10000","yarn.nodemanager.container-manager.thread-count":"20","yarn.minicluster.fixed.ports":"false","hadoop.tags.system":"YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL","yarn.cluster.max-application-priority":"0","yarn.timeline-service.ttl-enable":"true","mapreduce.jobhistory.recovery.store.fs.uri":"${hadoop.tmp.dir}/mapred/history/recoverystore","hadoop.caller.context.signature.max.size":"40","ipc.[port_number].decay-scheduler.backoff.responsetime.enable":"false","yarn.client.load.resource-types.from-server":"false","ha.zookeeper.session-timeout.ms":"10000","ipc.[port_number].decay-scheduler.metrics.top.user.count":"10","tfile.io.chunk.size":"1048576","fs.s3a.s3guard.ddb.table.capacity.write":"0","yarn.dispatcher.print-events-info.threshold":"5000","mapreduce.job.speculative.slowtaskthreshold":"1.0","io.serializations":"org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization","hadoop.security.kms.client.failover.sleep.max.millis":"2000","hadoop.security.group.mapping.ldap.directory.search.timeout":"10000","yarn.scheduler.configuration.store.max-logs":"1000","yarn.nodemanager.node-attributes.provider.fetch-interval-ms":"600000","fs.swift.impl":"org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem","yarn.nodemanager.local-cache.max-files-per-directory":"8192","hadoop.http.cross-origin.enabled":"false","hadoop.zk.acl":"world:anyone:rwcda","yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache":"10","mapreduce.map.sort.spill.percent":"0.80","yarn.timeline-service.entity-group-fs-store.scan-interval-seconds":"60","yarn.node-attribute.fs-store.impl.class":"org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore","fs.s3a.retry.interval":"500ms","yarn.timeline-service.client.best-effort":"false","yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled":"*********(redacted)","hadoop.security.group.mapping.ldap.posix.attr.uid.name":"uidNumber","fs.AbstractFileSystem.swebhdfs.impl":"org.apache.hadoop.fs.SWebHdfs","yarn.nodemanager.elastic-memory-control.timeout-sec":"5","fs.s3a.select.enabled":"true","mapreduce.ifile.readahead":"true","yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms":"300000","yarn.timeline-service.reader.webapp.address":"${yarn.timeline-service.webapp.address}","yarn.resourcemanager.placement-constraints.algorithm.pool-size":"1","yarn.timeline-service.hbase.coprocessor.jar.hdfs.location":"/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar","hadoop.security.kms.client.encrypted.key.cache.num.refill.threads":"2","yarn.resourcemanager.scheduler.class":"org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler","yarn.app.mapreduce.am.command-opts":"-Xmx1024m","fs.s3a.metadatastore.fail.on.write.error":"true","hadoop.http.sni.host.check.enabled":"false","mapreduce.cluster.local.dir":"${hadoop.tmp.dir}/mapred/local","io.mapfile.bloom.error.rate":"0.005","fs.client.resolve.topology.enabled":"false","yarn.nodemanager.runtime.linux.allowed-runtimes":"default","yarn.sharedcache.store.class":"org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore","ha.failover-controller.graceful-fence.rpc-timeout.ms":"5000","ftp.replication":"3","fs.getspaceused.jitterMillis":"60000","hadoop.security.uid.cache.secs":"14400","mapreduce.job.maxtaskfailures.per.tracker":"3","fs.s3a.metadatastore.impl":"org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore","io.skip.checksum.errors":"false","yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts":"3","yarn.timeline-service.webapp.xfs-filter.xframe-options":"SAMEORIGIN","fs.s3a.connection.timeout":"200000","yarn.app.mapreduce.am.webapp.https.enabled":"false","mapreduce.job.max.split.locations":"15","yarn.resourcemanager.nm-container-queuing.max-queue-length":"15","yarn.resourcemanager.delegation-token.always-cancel":"*********(redacted)","hadoop.registry.zk.session.timeout.ms":"60000","yarn.federation.cache-ttl.secs":"300","mapreduce.jvm.system-properties-to-log":"os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name","yarn.resourcemanager.opportunistic-container-allocation.nodes-used":"10","yarn.timeline-service.entity-group-fs-store.active-dir":"/tmp/entity-file-history/active","mapreduce.shuffle.transfer.buffer.size":"131072","yarn.timeline-service.client.retry-interval-ms":"1000","yarn.timeline-service.flowname.max-size":"0","yarn.http.policy":"HTTP_ONLY","fs.s3a.socket.send.buffer":"8192","fs.AbstractFileSystem.abfss.impl":"org.apache.hadoop.fs.azurebfs.Abfss","yarn.sharedcache.uploader.server.address":"0.0.0.0:8046","yarn.resourcemanager.delegation-token.max-conf-size-bytes":"*********(redacted)","hadoop.http.authentication.token.validity":"*********(redacted)","mapreduce.shuffle.max.connections":"0","yarn.minicluster.yarn.nodemanager.resource.memory-mb":"4096","mapreduce.job.emit-timeline-data":"false","yarn.nodemanager.resource.system-reserved-memory-mb":"-1","hadoop.kerberos.min.seconds.before.relogin":"60","mapreduce.jobhistory.move.thread-count":"3","yarn.resourcemanager.admin.client.thread-count":"1","yarn.dispatcher.drain-events.timeout":"300000","ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds":"10s,20s,30s,40s","fs.s3a.buffer.dir":"${hadoop.tmp.dir}/s3a","hadoop.ssl.enabled.protocols":"TLSv1.2","mapreduce.jobhistory.admin.address":"0.0.0.0:10033","yarn.log-aggregation-status.time-out.ms":"600000","fs.s3a.accesspoint.required":"false","mapreduce.shuffle.port":"13562","yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory":"10","yarn.nodemanager.health-checker.interval-ms":"600000","yarn.resourcemanager.proxy.connection.timeout":"60000","yarn.router.clientrm.interceptor-class.pipeline":"org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor","yarn.resourcemanager.zk-appid-node.split-index":"0","ftp.blocksize":"67108864","yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions":"read","yarn.router.rmadmin.interceptor-class.pipeline":"org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor","yarn.nodemanager.log-container-debug-info.enabled":"true","yarn.resourcemanager.activities-manager.app-activities.max-queue-length":"100","yarn.resourcemanager.application-https.policy":"NONE","yarn.client.max-cached-nodemanagers-proxies":"0","yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms":"20","yarn.nodemanager.delete.debug-delay-sec":"0","yarn.nodemanager.pmem-check-enabled":"true","yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage":"90.0","mapreduce.app-submission.cross-platform":"false","yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms":"10000","yarn.nodemanager.container-retry-minimum-interval-ms":"1000","hadoop.security.groups.cache.secs":"300","yarn.federation.enabled":"false","yarn.workflow-id.tag-prefix":"workflowid:","fs.azure.local.sas.key.mode":"false","ipc.maximum.data.length":"134217728","fs.s3a.endpoint":"s3.amazonaws.com","mapreduce.shuffle.max.threads":"0","yarn.router.pipeline.cache-max-size":"25","yarn.resourcemanager.nm-container-queuing.load-comparator":"QUEUE_LENGTH","yarn.resourcemanager.resource-tracker.nm.ip-hostname-check":"false","hadoop.security.authorization":"false","mapreduce.job.complete.cancel.delegation.tokens":"*********(redacted)","fs.s3a.paging.maximum":"5000","nfs.exports.allowed.hosts":"* rw","yarn.nodemanager.amrmproxy.ha.enable":"false","fs.AbstractFileSystem.gs.impl":"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS","mapreduce.jobhistory.http.policy":"HTTP_ONLY","yarn.sharedcache.store.in-memory.check-period-mins":"720","hadoop.security.group.mapping.ldap.ssl":"false","fs.s3a.downgrade.syncable.exceptions":"true","yarn.client.application-client-protocol.poll-interval-ms":"200","yarn.scheduler.configuration.leveldb-store.compaction-interval-secs":"86400","yarn.timeline-service.writer.class":"org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl","ha.zookeeper.parent-znode":"/hadoop-ha","yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms":"60000","yarn.nodemanager.log-aggregation.policy.class":"org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy","mapreduce.reduce.shuffle.merge.percent":"0.66","hadoop.security.group.mapping.ldap.search.filter.group":"(objectClass=group)","yarn.resourcemanager.placement-constraints.scheduler.pool-size":"1","yarn.resourcemanager.activities-manager.cleanup-interval-ms":"5000","yarn.nodemanager.resourcemanager.minimum.version":"NONE","mapreduce.job.speculative.speculative-cap-running-tasks":"0.1","yarn.admin.acl":"*","ipc.[port_number].identity-provider.impl":"org.apache.hadoop.ipc.UserIdentityProvider","yarn.nodemanager.recovery.supervised":"false","yarn.sharedcache.admin.thread-count":"1","yarn.resourcemanager.ha.automatic-failover.enabled":"true","yarn.nodemanager.container-log-monitor.total-size-limit-bytes":"10000000000","mapreduce.reduce.skip.maxgroups":"0","mapreduce.reduce.shuffle.connect.timeout":"180000","yarn.nodemanager.health-checker.scripts":"script","yarn.resourcemanager.address":"${yarn.resourcemanager.hostname}:8032","ipc.client.ping":"true","mapreduce.task.local-fs.write-limit.bytes":"-1","fs.adl.oauth2.access.token.provider.type":"*********(redacted)","mapreduce.shuffle.ssl.file.buffer.size":"65536","yarn.resourcemanager.ha.automatic-failover.embedded":"true","yarn.nodemanager.resource-plugins.gpu.docker-plugin":"nvidia-docker-v1","fs.s3a.s3guard.consistency.retry.interval":"2s","fs.s3a.multipart.purge":"false","yarn.scheduler.configuration.store.class":"file","yarn.resourcemanager.nm-container-queuing.queue-limit-stdev":"1.0f","mapreduce.job.end-notification.max.attempts":"5","mapreduce.output.fileoutputformat.compress.codec":"org.apache.hadoop.io.compress.DefaultCodec","yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled":"false","ipc.client.bind.wildcard.addr":"false","yarn.resourcemanager.webapp.rest-csrf.enabled":"false","ha.health-monitor.connect-retry-interval.ms":"1000","yarn.nodemanager.keytab":"/etc/krb5.keytab","mapreduce.jobhistory.keytab":"/etc/security/keytab/jhs.service.keytab","fs.s3a.threads.max":"64","yarn.nodemanager.runtime.linux.docker.image-update":"false","mapreduce.reduce.shuffle.input.buffer.percent":"0.70","fs.viewfs.overload.scheme.target.abfss.impl":"org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem","yarn.dispatcher.cpu-monitor.samples-per-min":"60","hadoop.security.token.service.use_ip":"*********(redacted)","yarn.nodemanager.runtime.linux.docker.allowed-container-networks":"host,none,bridge","yarn.nodemanager.node-labels.resync-interval-ms":"120000","hadoop.tmp.dir":"/tmp/hadoop-${user.name}","mapreduce.job.maps":"2","mapreduce.jobhistory.webapp.rest-csrf.custom-header":"X-XSRF-Header","mapreduce.job.end-notification.max.retry.interval":"5000","yarn.log-aggregation.retain-check-interval-seconds":"-1","yarn.resourcemanager.resource-tracker.client.thread-count":"50","yarn.nodemanager.containers-launcher.class":"org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher","yarn.rm.system-metrics-publisher.emit-container-events":"false","yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size":"10000","yarn.resourcemanager.ha.automatic-failover.zk-base-path":"/yarn-leader-election","io.seqfile.local.dir":"${hadoop.tmp.dir}/io/local","fs.s3a.s3guard.ddb.throttle.retry.interval":"100ms","fs.AbstractFileSystem.wasb.impl":"org.apache.hadoop.fs.azure.Wasb","mapreduce.client.submit.file.replication":"10","mapreduce.jobhistory.minicluster.fixed.ports":"false","fs.s3a.multipart.threshold":"128M","yarn.resourcemanager.webapp.xfs-filter.xframe-options":"SAMEORIGIN","mapreduce.jobhistory.done-dir":"${yarn.app.mapreduce.am.staging-dir}/history/done","ipc.server.purge.interval":"15","ipc.client.idlethreshold":"4000","yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage":"false","mapreduce.reduce.input.buffer.percent":"0.0","yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold":"1","yarn.nodemanager.webapp.rest-csrf.enabled":"false","fs.ftp.host.port":"21","ipc.ping.interval":"60000","yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size":"10","yarn.resourcemanager.admin.address":"${yarn.resourcemanager.hostname}:8033","file.client-write-packet-size":"65536","ipc.client.kill.max":"10","mapreduce.reduce.speculative":"true","hadoop.security.key.default.bitlength":"128","mapreduce.job.reducer.unconditional-preempt.delay.sec":"300","yarn.nodemanager.disk-health-checker.interval-ms":"120000","yarn.nodemanager.log.deletion-threads-count":"4","fs.s3a.committer.abort.pending.uploads":"true","yarn.webapp.filter-entity-list-by-user":"false","yarn.resourcemanager.activities-manager.app-activities.ttl-ms":"600000","ipc.client.connection.maxidletime":"10000","mapreduce.task.io.sort.mb":"100","yarn.nodemanager.localizer.client.thread-count":"5","io.erasurecode.codec.rs.rawcoders":"rs_native,rs_java","io.erasurecode.codec.rs-legacy.rawcoders":"rs-legacy_java","yarn.sharedcache.admin.address":"0.0.0.0:8047","yarn.resourcemanager.placement-constraints.algorithm.iterator":"SERIAL","yarn.nodemanager.localizer.cache.cleanup.interval-ms":"600000","hadoop.security.crypto.codec.classes.aes.ctr.nopadding":"org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec","mapreduce.job.cache.limit.max-resources-mb":"0","fs.s3a.connection.ssl.enabled":"true","yarn.nodemanager.process-kill-wait.ms":"5000","mapreduce.job.hdfs-servers":"${fs.defaultFS}","yarn.app.mapreduce.am.webapp.https.client.auth":"false","hadoop.workaround.non.threadsafe.getpwuid":"true","fs.df.interval":"60000","ipc.[port_number].decay-scheduler.thresholds":"13,25,50","fs.s3a.multiobjectdelete.enable":"true","yarn.sharedcache.cleaner.resource-sleep-ms":"0","yarn.nodemanager.disk-health-checker.min-healthy-disks":"0.25","hadoop.shell.missing.defaultFs.warning":"false","io.file.buffer.size":"65536","fs.viewfs.overload.scheme.target.wasb.impl":"org.apache.hadoop.fs.azure.NativeAzureFileSystem","hadoop.security.group.mapping.ldap.search.attr.member":"member","hadoop.security.random.device.file.path":"/dev/urandom","hadoop.security.sensitive-config-keys":"*********(redacted)","fs.s3a.s3guard.ddb.max.retries":"9","fs.viewfs.overload.scheme.target.file.impl":"org.apache.hadoop.fs.LocalFileSystem","hadoop.rpc.socket.factory.class.default":"org.apache.hadoop.net.StandardSocketFactory","yarn.intermediate-data-encryption.enable":"false","yarn.resourcemanager.connect.retry-interval.ms":"30000","yarn.nodemanager.container.stderr.pattern":"{*stderr*,*STDERR*}","yarn.scheduler.minimum-allocation-mb":"1024","yarn.app.mapreduce.am.staging-dir":"/tmp/hadoop-yarn/staging","mapreduce.reduce.shuffle.read.timeout":"180000","hadoop.http.cross-origin.max-age":"1800","io.erasurecode.codec.xor.rawcoders":"xor_native,xor_java","fs.s3a.s3guard.consistency.retry.limit":"7","fs.s3a.connection.establish.timeout":"5000","mapreduce.job.running.map.limit":"0","yarn.minicluster.control-resource-monitoring":"false","hadoop.ssl.require.client.cert":"false","hadoop.kerberos.kinit.command":"kinit","yarn.federation.state-store.class":"org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore","mapreduce.reduce.log.level":"INFO","hadoop.security.dns.log-slow-lookups.threshold.ms":"1000","mapreduce.job.ubertask.enable":"false","adl.http.timeout":"-1","yarn.resourcemanager.placement-constraints.retry-attempts":"3","hadoop.caller.context.enabled":"false","hadoop.security.group.mapping.ldap.num.attempts":"3","yarn.nodemanager.vmem-pmem-ratio":"2.1","hadoop.rpc.protection":"authentication","ha.health-monitor.rpc-timeout.ms":"45000","yarn.nodemanager.remote-app-log-dir":"/tmp/logs","hadoop.zk.timeout-ms":"10000","fs.s3a.s3guard.cli.prune.age":"86400000","yarn.nodemanager.resource.pcores-vcores-multiplier":"1.0","yarn.nodemanager.runtime.linux.sandbox-mode":"disabled","yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size":"10","fs.viewfs.overload.scheme.target.webhdfs.impl":"org.apache.hadoop.hdfs.web.WebHdfsFileSystem","fs.s3a.committer.threads":"8","hadoop.zk.retry-interval-ms":"1000","hadoop.security.crypto.buffer.size":"8192","yarn.nodemanager.node-labels.provider.fetch-interval-ms":"600000","mapreduce.jobhistory.recovery.store.leveldb.path":"${hadoop.tmp.dir}/mapred/history/recoverystore","yarn.client.failover-retries-on-socket-timeouts":"0","fs.s3a.ssl.channel.mode":"default_jsse","yarn.nodemanager.resource.memory.enabled":"false","fs.azure.authorization.caching.enable":"true","hadoop.security.instrumentation.requires.admin":"false","yarn.nodemanager.delete.thread-count":"4","mapreduce.job.finish-when-all-reducers-done":"true","hadoop.registry.jaas.context":"Client","yarn.timeline-service.leveldb-timeline-store.path":"${hadoop.tmp.dir}/yarn/timeline","io.map.index.interval":"128","yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms":"100","fs.abfs.impl":"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem","mapreduce.job.counters.max":"120","mapreduce.jobhistory.webapp.rest-csrf.enabled":"false","yarn.timeline-service.store-class":"org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore","mapreduce.jobhistory.move.interval-ms":"180000","fs.s3a.change.detection.version.required":"true","yarn.nodemanager.localizer.fetch.thread-count":"4","yarn.resourcemanager.scheduler.client.thread-count":"50","hadoop.ssl.hostname.verifier":"DEFAULT","yarn.timeline-service.leveldb-state-store.path":"${hadoop.tmp.dir}/yarn/timeline","mapreduce.job.classloader":"false","mapreduce.task.profile.map.params":"${mapreduce.task.profile.params}","ipc.client.connect.timeout":"20000","hadoop.security.auth_to_local.mechanism":"hadoop","yarn.timeline-service.app-collector.linger-period.ms":"60000","yarn.nm.liveness-monitor.expiry-interval-ms":"600000","yarn.resourcemanager.reservation-system.planfollower.time-step":"1000","yarn.resourcemanager.proxy.timeout.enabled":"true","yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms":"600000","yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed":"true","yarn.webapp.api-service.enable":"false","yarn.nodemanager.recovery.enabled":"false","mapreduce.job.end-notification.retry.interval":"1000","fs.du.interval":"600000","fs.ftp.impl":"org.apache.hadoop.fs.ftp.FTPFileSystem","yarn.nodemanager.container.stderr.tail.bytes":"4096","yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled":"true","hadoop.security.group.mapping.ldap.read.timeout.ms":"60000","hadoop.security.groups.cache.warn.after.ms":"5000","file.bytes-per-checksum":"512","mapreduce.outputcommitter.factory.scheme.s3a":"org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory","hadoop.security.groups.cache.background.reload":"false","yarn.nodemanager.container-monitor.enabled":"true","yarn.nodemanager.elastic-memory-control.enabled":"false","net.topology.script.number.args":"100","mapreduce.task.merge.progress.records":"10000","yarn.nodemanager.localizer.address":"${yarn.nodemanager.hostname}:8040","yarn.timeline-service.keytab":"/etc/krb5.keytab","mapreduce.reduce.shuffle.fetch.retry.timeout-ms":"30000","yarn.resourcemanager.rm.container-allocation.expiry-interval-ms":"600000","yarn.nodemanager.container-executor.exit-code-file.timeout-ms":"2000","mapreduce.fileoutputcommitter.algorithm.version":"1","yarn.resourcemanager.work-preserving-recovery.enabled":"true","mapreduce.map.skip.maxrecords":"0","yarn.sharedcache.root-dir":"/sharedcache","fs.s3a.retry.throttle.limit":"20","hadoop.http.authentication.type":"simple","fs.viewfs.overload.scheme.target.oss.impl":"org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem","mapreduce.job.cache.limit.max-resources":"0","mapreduce.task.userlog.limit.kb":"0","ipc.[port_number].weighted-cost.handler":"1","yarn.resourcemanager.scheduler.monitor.enable":"false","ipc.client.connect.max.retries":"10","hadoop.registry.zk.retry.times":"5","yarn.nodemanager.resource-monitor.interval-ms":"3000","yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices":"auto","mapreduce.job.sharedcache.mode":"disabled","yarn.nodemanager.webapp.rest-csrf.custom-header":"X-XSRF-Header","mapreduce.shuffle.listen.queue.size":"128","yarn.scheduler.configuration.mutation.acl-policy.class":"org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy","mapreduce.map.cpu.vcores":"1","yarn.log-aggregation.file-formats":"TFile","yarn.timeline-service.client.fd-retain-secs":"300","fs.s3a.select.output.csv.field.delimiter":",","yarn.nodemanager.health-checker.timeout-ms":"1200000","hadoop.user.group.static.mapping.overrides":"dr.who=;","fs.azure.sas.expiry.period":"90d","fs.s3a.select.output.csv.record.delimiter":"\\n","mapreduce.jobhistory.recovery.store.class":"org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService","fs.viewfs.overload.scheme.target.https.impl":"org.apache.hadoop.fs.http.HttpsFileSystem","fs.s3a.s3guard.ddb.table.sse.enabled":"false","yarn.resourcemanager.fail-fast":"${yarn.fail-fast}","yarn.resourcemanager.proxy-user-privileges.enabled":"false","yarn.router.webapp.interceptor-class.pipeline":"org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST","yarn.nodemanager.resource.memory.cgroups.soft-limit-percentage":"90.0","mapreduce.job.reducer.preempt.delay.sec":"0","hadoop.util.hash.type":"murmur","yarn.nodemanager.disk-validator":"basic","yarn.app.mapreduce.client.job.max-retries":"3","fs.viewfs.overload.scheme.target.ftp.impl":"org.apache.hadoop.fs.ftp.FTPFileSystem","mapreduce.reduce.shuffle.retry-delay.max.ms":"60000","hadoop.security.group.mapping.ldap.connection.timeout.ms":"60000","mapreduce.task.profile.params":"-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s","yarn.app.mapreduce.shuffle.log.backups":"0","yarn.nodemanager.container-diagnostics-maximum-size":"10000","hadoop.registry.zk.retry.interval.ms":"1000","yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms":"1000","fs.AbstractFileSystem.file.impl":"org.apache.hadoop.fs.local.LocalFs","yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds":"-1","mapreduce.jobhistory.cleaner.interval-ms":"86400000","hadoop.registry.zk.quorum":"localhost:2181","yarn.nodemanager.runtime.linux.runc.allowed-container-runtimes":"runc","mapreduce.output.fileoutputformat.compress":"false","yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs":"*********(redacted)","fs.s3a.assumed.role.session.duration":"30m","hadoop.security.group.mapping.ldap.conversion.rule":"none","hadoop.ssl.server.conf":"ssl-server.xml","fs.s3a.retry.throttle.interval":"100ms","seq.io.sort.factor":"100","fs.viewfs.overload.scheme.target.ofs.impl":"org.apache.hadoop.fs.ozone.RootedOzoneFileSystem","yarn.sharedcache.cleaner.initial-delay-mins":"10","mapreduce.client.completion.pollinterval":"5000","hadoop.ssl.keystores.factory.class":"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory","yarn.app.mapreduce.am.resource.cpu-vcores":"1","yarn.timeline-service.enabled":"false","yarn.nodemanager.runtime.linux.docker.capabilities":"CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE","yarn.acl.enable":"false","yarn.timeline-service.entity-group-fs-store.done-dir":"/tmp/entity-file-history/done/","hadoop.security.group.mapping.ldap.num.attempts.before.failover":"3","mapreduce.task.profile":"false","hadoop.prometheus.endpoint.enabled":"false","yarn.resourcemanager.fs.state-store.uri":"${hadoop.tmp.dir}/yarn/system/rmstore","mapreduce.jobhistory.always-scan-user-dir":"false","fs.s3a.metadatastore.metadata.ttl":"15m","yarn.nodemanager.opportunistic-containers-use-pause-for-preemption":"false","yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user":"nobody","yarn.timeline-service.reader.class":"org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl","yarn.resourcemanager.configuration.provider-class":"org.apache.hadoop.yarn.LocalConfigurationProvider","yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold":"1","yarn.resourcemanager.configuration.file-system-based-store":"/yarn/conf","mapreduce.job.cache.limit.max-single-resource-mb":"0","yarn.nodemanager.runtime.linux.docker.stop.grace-period":"10","yarn.resourcemanager.resource-profiles.source-file":"resource-profiles.json","mapreduce.job.dfs.storage.capacity.kill-limit-exceed":"false","yarn.nodemanager.resource.percentage-physical-cpu-limit":"100","mapreduce.jobhistory.client.thread-count":"10","tfile.fs.input.buffer.size":"262144","mapreduce.client.progressmonitor.pollinterval":"1000","yarn.nodemanager.log-dirs":"${yarn.log.dir}/userlogs","yarn.resourcemanager.opportunistic.max.container-allocation.per.am.heartbeat":"-1","fs.automatic.close":"true","yarn.resourcemanager.delegation-token-renewer.thread-retry-interval":"*********(redacted)","fs.s3a.select.input.csv.quote.character":"\"","yarn.nodemanager.hostname":"0.0.0.0","ipc.[port_number].cost-provider.impl":"org.apache.hadoop.ipc.DefaultCostProvider","yarn.nodemanager.runtime.linux.runc.manifest-to-resources-plugin":"org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin","yarn.nodemanager.remote-app-log-dir-include-older":"true","yarn.nodemanager.resource.memory.cgroups.swappiness":"0","ftp.stream-buffer-size":"4096","yarn.fail-fast":"false","yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep":"100","yarn.timeline-service.app-aggregation-interval-secs":"15","hadoop.security.group.mapping.ldap.search.filter.user":"(&(objectClass=user)(sAMAccountName={0}))","ipc.[port_number].weighted-cost.lockshared":"10","yarn.nodemanager.container-localizer.log.level":"INFO","yarn.timeline-service.address":"${yarn.timeline-service.hostname}:10200","mapreduce.job.ubertask.maxmaps":"9","fs.s3a.threads.keepalivetime":"60","mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore":"GET,OPTIONS,HEAD","mapreduce.task.files.preserve.failedtasks":"false","yarn.app.mapreduce.client.job.retry-interval":"2000","ha.failover-controller.graceful-fence.connection.retries":"1","fs.s3a.select.output.csv.quote.escape.character":"\\\\","yarn.resourcemanager.delegation.token.max-lifetime":"*********(redacted)","hadoop.kerberos.keytab.login.autorenewal.enabled":"false","yarn.timeline-service.client.drain-entities.timeout.ms":"2000","yarn.nodemanager.resource-plugins.fpga.vendor-plugin.class":"org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin","yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms":"1000","yarn.timeline-service.entity-group-fs-store.summary-store":"org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore","mapreduce.reduce.cpu.vcores":"1","mapreduce.job.encrypted-intermediate-data.buffer.kb":"128","fs.client.resolve.remote.symlinks":"true","yarn.nodemanager.webapp.https.address":"0.0.0.0:8044","hadoop.http.cross-origin.allowed-origins":"*","mapreduce.job.encrypted-intermediate-data":"false","yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled":"true","fs.s3a.executor.capacity":"16","yarn.timeline-service.entity-group-fs-store.retain-seconds":"604800","yarn.resourcemanager.metrics.runtime.buckets":"60,300,1440","yarn.timeline-service.generic-application-history.max-applications":"10000","yarn.nodemanager.local-dirs":"${hadoop.tmp.dir}/nm-local-dir","mapreduce.shuffle.connection-keep-alive.enable":"false","yarn.node-labels.configuration-type":"centralized","fs.s3a.path.style.access":"false","yarn.nodemanager.aux-services.mapreduce_shuffle.class":"org.apache.hadoop.mapred.ShuffleHandler","yarn.sharedcache.store.in-memory.staleness-period-mins":"10080","fs.adl.impl":"org.apache.hadoop.fs.adl.AdlFileSystem","yarn.resourcemanager.application.max-tags":"10","hadoop.domainname.resolver.impl":"org.apache.hadoop.net.DNSDomainNameResolver","yarn.resourcemanager.nodemanager.minimum.version":"NONE","mapreduce.jobhistory.webapp.xfs-filter.xframe-options":"SAMEORIGIN","yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled":"false","net.topology.impl":"org.apache.hadoop.net.NetworkTopology","io.map.index.skip":"0","yarn.timeline-service.reader.webapp.https.address":"${yarn.timeline-service.webapp.https.address}","fs.ftp.data.connection.mode":"ACTIVE_LOCAL_DATA_CONNECTION_MODE","mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed":"true","fs.azure.buffer.dir":"${hadoop.tmp.dir}/abfs","yarn.scheduler.maximum-allocation-vcores":"4","hadoop.http.cross-origin.allowed-headers":"X-Requested-With,Content-Type,Accept,Origin","yarn.nodemanager.log-aggregation.compression-type":"none","yarn.timeline-service.version":"1.0f","yarn.ipc.rpc.class":"org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC","mapreduce.reduce.maxattempts":"4","yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.batch-size":"1000","hadoop.security.dns.log-slow-lookups.enabled":"false","mapreduce.job.committer.setup.cleanup.needed":"true","hadoop.security.secure.random.impl":"org.apache.hadoop.crypto.random.OpensslSecureRandom","mapreduce.job.running.reduce.limit":"0","fs.s3a.select.errors.include.sql":"false","fs.s3a.connection.request.timeout":"0","ipc.maximum.response.length":"134217728","yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore":"GET,OPTIONS,HEAD","mapreduce.job.token.tracking.ids.enabled":"*********(redacted)","hadoop.caller.context.max.size":"128","yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed":"false","yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed":"false","hadoop.registry.system.acls":"sasl:yarn@, sasl:mapred@, sasl:hdfs@","yarn.nodemanager.recovery.dir":"${hadoop.tmp.dir}/yarn-nm-recovery","fs.s3a.fast.upload.buffer":"disk","mapreduce.jobhistory.intermediate-done-dir":"${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate","yarn.app.mapreduce.shuffle.log.separate":"true","yarn.log-aggregation.debug.filesize":"104857600","fs.s3a.max.total.tasks":"32","fs.s3a.readahead.range":"64K","hadoop.http.authentication.simple.anonymous.allowed":"true","fs.s3a.attempts.maximum":"20","hadoop.registry.zk.connection.timeout.ms":"15000","yarn.resourcemanager.delegation-token-renewer.thread-count":"*********(redacted)","yarn.resourcemanager.delegation-token-renewer.thread-timeout":"*********(redacted)","yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size":"10000","yarn.nodemanager.aux-services.manifest.reload-ms":"0","yarn.nodemanager.emit-container-events":"true","yarn.resourcemanager.resource-profiles.enabled":"false","yarn.timeline-service.hbase-schema.prefix":"prod.","fs.azure.authorization":"false","mapreduce.map.log.level":"INFO","ha.failover-controller.active-standby-elector.zk.op.retries":"3","yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs":"20","mapreduce.output.fileoutputformat.compress.type":"RECORD","yarn.resourcemanager.leveldb-state-store.path":"${hadoop.tmp.dir}/yarn/system/rmstore","yarn.timeline-service.webapp.rest-csrf.custom-header":"X-XSRF-Header","mapreduce.ifile.readahead.bytes":"4194304","yarn.sharedcache.app-checker.class":"org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker","yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users":"true","yarn.nodemanager.resource.detect-hardware-capabilities":"false","mapreduce.cluster.acls.enabled":"false","mapreduce.job.speculative.retry-after-no-speculate":"1000","fs.viewfs.overload.scheme.target.abfs.impl":"org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem","hadoop.security.group.mapping.ldap.search.group.hierarchy.levels":"0","yarn.resourcemanager.fs.state-store.retry-interval-ms":"1000","file.stream-buffer-size":"4096","yarn.resourcemanager.application-timeouts.monitor.interval-ms":"3000","mapreduce.map.output.compress.codec":"org.apache.hadoop.io.compress.DefaultCodec","mapreduce.map.speculative":"true","yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.hdfs-hash-file":"/runc-root/image-tag-to-hash","mapreduce.job.speculative.retry-after-speculate":"15000","yarn.nodemanager.linux-container-executor.cgroups.mount":"false","yarn.app.mapreduce.am.container.log.backups":"0","yarn.app.mapreduce.am.log.level":"INFO","yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin":"org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin","io.bytes.per.checksum":"512","mapreduce.job.reduce.slowstart.completedmaps":"0.05","yarn.timeline-service.http-authentication.type":"simple","hadoop.security.group.mapping.ldap.search.attr.group.name":"cn","yarn.nodemanager.resource-plugins.fpga.allowed-fpga-devices":"auto","yarn.timeline-service.client.internal-timers-ttl-secs":"420","fs.s3a.select.output.csv.quote.character":"\"","hadoop.http.logs.enabled":"true","fs.s3a.block.size":"32M","yarn.sharedcache.client-server.address":"0.0.0.0:8045","yarn.nodemanager.logaggregation.threadpool-size-max":"100","yarn.resourcemanager.hostname":"0.0.0.0","yarn.resourcemanager.delegation.key.update-interval":"86400000","mapreduce.reduce.shuffle.fetch.retry.enabled":"${yarn.nodemanager.recovery.enabled}","mapreduce.map.memory.mb":"-1","mapreduce.task.skip.start.attempts":"2","fs.AbstractFileSystem.hdfs.impl":"org.apache.hadoop.fs.Hdfs","yarn.nodemanager.disk-health-checker.enable":"true","fs.s3a.select.output.csv.quote.fields":"always","ipc.client.tcpnodelay":"true","ipc.client.rpc-timeout.ms":"0","yarn.nodemanager.webapp.rest-csrf.methods-to-ignore":"GET,OPTIONS,HEAD","yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts":"*********(redacted)","ipc.client.low-latency":"false","mapreduce.input.lineinputformat.linespermap":"1","yarn.router.interceptor.user.threadpool-size":"5","ipc.client.connect.max.retries.on.timeouts":"45","yarn.timeline-service.leveldb-timeline-store.read-cache-size":"104857600","fs.AbstractFileSystem.har.impl":"org.apache.hadoop.fs.HarFs","mapreduce.job.split.metainfo.maxsize":"10000000","yarn.am.liveness-monitor.expiry-interval-ms":"600000","yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs":"*********(redacted)","yarn.timeline-service.entity-group-fs-store.app-cache-size":"10","yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs":"360","fs.s3a.socket.recv.buffer":"8192","rpc.metrics.timeunit":"MILLISECONDS","yarn.resourcemanager.resource-tracker.address":"${yarn.resourcemanager.hostname}:8031","yarn.nodemanager.node-labels.provider.fetch-timeout-ms":"1200000","mapreduce.job.heap.memory-mb.ratio":"0.8","yarn.resourcemanager.leveldb-state-store.compaction-interval-secs":"3600","yarn.resourcemanager.webapp.rest-csrf.custom-header":"X-XSRF-Header","yarn.nodemanager.pluggable-device-framework.enabled":"false","yarn.scheduler.configuration.fs.path":"file://${hadoop.tmp.dir}/yarn/system/schedconf","mapreduce.client.output.filter":"FAILED","hadoop.http.filter.initializers":"org.apache.hadoop.http.lib.StaticUserWebFilter","mapreduce.reduce.memory.mb":"-1","yarn.timeline-service.hostname":"0.0.0.0","file.replication":"1","yarn.nodemanager.container-metrics.unregister-delay-ms":"10000","yarn.nodemanager.container-metrics.period-ms":"-1","mapreduce.fileoutputcommitter.task.cleanup.enabled":"false","yarn.nodemanager.log.retain-seconds":"10800","yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds":"3600","ipc.[port_number].callqueue.impl":"java.util.concurrent.LinkedBlockingQueue","yarn.resourcemanager.keytab":"/etc/krb5.keytab","hadoop.security.group.mapping.providers.combined":"true","mapreduce.reduce.merge.inmem.threshold":"1000","yarn.timeline-service.recovery.enabled":"false","fs.azure.saskey.usecontainersaskeyforallaccess":"true","yarn.sharedcache.nm.uploader.thread-count":"20","yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs":"3600","ipc.[port_number].weighted-cost.lockfree":"1","mapreduce.shuffle.ssl.enabled":"false","yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds":"259200000","yarn.nodemanager.opportunistic-containers-max-queue-length":"0","yarn.resourcemanager.state-store.max-completed-applications":"${yarn.resourcemanager.max-completed-applications}","mapreduce.job.speculative.minimum-allowed-tasks":"10","fs.s3a.aws.credentials.provider":"\n    org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,\n    org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,\n    com.amazonaws.auth.EnvironmentVariableCredentialsProvider,\n    org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider\n  ","yarn.log-aggregation.retain-seconds":"-1","yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb":"0","mapreduce.jobhistory.max-age-ms":"604800000","hadoop.http.cross-origin.allowed-methods":"GET,POST,HEAD","yarn.resourcemanager.opportunistic-container-allocation.enabled":"false","mapreduce.jobhistory.webapp.address":"0.0.0.0:19888","hadoop.system.tags":"YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL","yarn.log-aggregation.file-controller.TFile.class":"org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController","yarn.client.nodemanager-connect.max-wait-ms":"180000","yarn.resourcemanager.webapp.address":"${yarn.resourcemanager.hostname}:8088","mapreduce.jobhistory.recovery.enable":"false","mapreduce.reduce.shuffle.parallelcopies":"5","fs.AbstractFileSystem.webhdfs.impl":"org.apache.hadoop.fs.WebHdfs","fs.trash.interval":"0","yarn.app.mapreduce.client.max-retries":"3","hadoop.security.authentication":"simple","mapreduce.task.profile.reduce.params":"${mapreduce.task.profile.params}","yarn.app.mapreduce.am.resource.mb":"1536","mapreduce.input.fileinputformat.list-status.num-threads":"1","yarn.nodemanager.container-executor.class":"org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor","io.mapfile.bloom.size":"1048576","yarn.timeline-service.ttl-ms":"604800000","yarn.resourcemanager.nm-container-queuing.min-queue-length":"5","yarn.nodemanager.resource.cpu-vcores":"-1","mapreduce.job.reduces":"1","fs.s3a.multipart.size":"64M","fs.s3a.select.input.csv.comment.marker":"#","yarn.scheduler.minimum-allocation-vcores":"1","mapreduce.job.speculative.speculative-cap-total-tasks":"0.01","hadoop.ssl.client.conf":"ssl-client.xml","mapreduce.job.queuename":"default","mapreduce.job.encrypted-intermediate-data-key-size-bits":"128","fs.s3a.metadatastore.authoritative":"false","ipc.[port_number].weighted-cost.response":"1","yarn.nodemanager.webapp.xfs-filter.xframe-options":"SAMEORIGIN","ha.health-monitor.sleep-after-disconnect.ms":"1000","yarn.app.mapreduce.shuffle.log.limit.kb":"0","hadoop.security.group.mapping":"org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback","yarn.client.application-client-protocol.poll-timeout-ms":"-1","mapreduce.jobhistory.jhist.format":"binary","mapreduce.task.stuck.timeout-ms":"600000","yarn.resourcemanager.application.max-tag.length":"100","yarn.resourcemanager.ha.enabled":"false","dfs.client.ignore.namenode.default.kms.uri":"false","hadoop.http.staticuser.user":"dr.who","mapreduce.task.exit.timeout.check-interval-ms":"20000","mapreduce.jobhistory.intermediate-user-done-dir.permissions":"770","mapreduce.task.exit.timeout":"60000","yarn.nodemanager.linux-container-executor.resources-handler.class":"org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler","mapreduce.reduce.shuffle.memory.limit.percent":"0.25","yarn.resourcemanager.reservation-system.enable":"false","mapreduce.map.output.compress":"false","ha.zookeeper.acl":"world:anyone:rwcda","ipc.server.max.connections":"0","yarn.nodemanager.runtime.linux.docker.default-container-network":"host","yarn.router.webapp.address":"0.0.0.0:8089","yarn.scheduler.maximum-allocation-mb":"8192","yarn.resourcemanager.scheduler.monitor.policies":"org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy","yarn.sharedcache.cleaner.period-mins":"1440","yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidia-docker-v1.endpoint":"http://localhost:3476/v1.0/docker/cli","yarn.app.mapreduce.am.container.log.limit.kb":"0","ipc.client.connect.retry.interval":"1000","yarn.timeline-service.http-cross-origin.enabled":"false","fs.wasbs.impl":"org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure","yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms":"1000","yarn.federation.subcluster-resolver.class":"org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl","yarn.resourcemanager.zk-state-store.parent-path":"/rmstore","fs.s3a.select.input.csv.field.delimiter":",","mapreduce.jobhistory.cleaner.enable":"true","yarn.timeline-service.client.fd-flush-interval-secs":"10","hadoop.security.kms.client.encrypted.key.cache.expiry":"43200000","yarn.client.nodemanager-client-async.thread-pool-max-size":"500","mapreduce.map.maxattempts":"4","yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms":"1000","fs.s3a.committer.staging.tmp.path":"tmp/staging","yarn.nodemanager.sleep-delay-before-sigkill.ms":"250","yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms":"10","mapreduce.job.end-notification.retry.attempts":"0","yarn.nodemanager.resource.count-logical-processors-as-cores":"false","hadoop.registry.zk.root":"/registry","adl.feature.ownerandgroup.enableupn":"false","yarn.resourcemanager.zk-max-znode-size.bytes":"1048576","mapreduce.job.reduce.shuffle.consumer.plugin.class":"org.apache.hadoop.mapreduce.task.reduce.Shuffle","yarn.resourcemanager.delayed.delegation-token.removal-interval-ms":"*********(redacted)","yarn.nodemanager.localizer.cache.target-size-mb":"10240","fs.s3a.committer.staging.conflict-mode":"append","mapreduce.client.libjars.wildcard":"true","fs.s3a.committer.staging.unique-filenames":"true","yarn.nodemanager.node-attributes.provider.fetch-timeout-ms":"1200000","fs.s3a.list.version":"2","ftp.client-write-packet-size":"65536","ipc.[port_number].weighted-cost.lockexclusive":"100","fs.AbstractFileSystem.adl.impl":"org.apache.hadoop.fs.adl.Adl","yarn.nodemanager.container-log-monitor.enable":"false","hadoop.security.key.default.cipher":"AES/CTR/NoPadding","yarn.client.failover-retries":"0","fs.s3a.multipart.purge.age":"86400","mapreduce.job.local-fs.single-disk-limit.check.interval-ms":"5000","net.topology.node.switch.mapping.impl":"org.apache.hadoop.net.ScriptBasedMapping","yarn.nodemanager.amrmproxy.address":"0.0.0.0:8049","ipc.server.listen.queue.size":"256","ipc.[port_number].decay-scheduler.period-ms":"5000","yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs":"60","map.sort.class":"org.apache.hadoop.util.QuickSort","fs.viewfs.rename.strategy":"SAME_MOUNTPOINT","hadoop.security.kms.client.authentication.retry-count":"1","fs.permissions.umask-mode":"022","fs.s3a.assumed.role.credentials.provider":"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider","yarn.nodemanager.runtime.linux.runc.privileged-containers.allowed":"false","yarn.nodemanager.vmem-check-enabled":"true","yarn.nodemanager.numa-awareness.enabled":"false","yarn.nodemanager.recovery.compaction-interval-secs":"3600","yarn.app.mapreduce.client-am.ipc.max-retries":"3","yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.interval-seconds":"60","yarn.federation.registry.base-dir":"yarnfederation/","yarn.nodemanager.health-checker.run-before-startup":"false","mapreduce.job.max.map":"-1","mapreduce.job.local-fs.single-disk-limit.bytes":"-1","mapreduce.shuffle.pathcache.concurrency-level":"16","mapreduce.job.ubertask.maxreduces":"1","mapreduce.shuffle.pathcache.max-weight":"10485760","hadoop.security.kms.client.encrypted.key.cache.size":"500","hadoop.security.java.secure.random.algorithm":"SHA1PRNG","ha.failover-controller.cli-check.rpc-timeout.ms":"20000","mapreduce.jobhistory.jobname.limit":"50","fs.s3a.select.input.compression":"none","yarn.client.nodemanager-connect.retry-interval-ms":"10000","ipc.[port_number].scheduler.priority.levels":"4","yarn.timeline-service.state-store-class":"org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore","yarn.nodemanager.env-whitelist":"JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ","yarn.sharedcache.nested-level":"3","yarn.timeline-service.webapp.rest-csrf.methods-to-ignore":"GET,OPTIONS,HEAD","fs.azure.user.agent.prefix":"unknown","yarn.resourcemanager.zk-delegation-token-node.split-index":"*********(redacted)","yarn.nodemanager.numa-awareness.read-topology":"false","yarn.nodemanager.webapp.address":"${yarn.nodemanager.hostname}:8042","rpc.metrics.quantile.enable":"false","yarn.registry.class":"org.apache.hadoop.registry.client.impl.FSRegistryOperationsService","mapreduce.jobhistory.admin.acl":"*","yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size":"10","yarn.scheduler.queue-placement-rules":"user-group","hadoop.http.authentication.kerberos.keytab":"${user.home}/hadoop.keytab","yarn.resourcemanager.recovery.enabled":"false","fs.s3a.select.input.csv.header":"none","yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size":"500","yarn.timeline-service.webapp.rest-csrf.enabled":"false","yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb":"0"},"System Properties":{"java.io.tmpdir":"/tmp","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.maintenance.version":"1","java.specification.version":"17","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"Debian","java.vm.specification.version":"17","user.home":"/root","sun.arch.data.model":"64","sun.boot.library.path":"/usr/lib/jvm/java-17-openjdk-amd64/lib","user.dir":"/home/jovyan/work/Guías/01-RDDs","java.library.path":"/opt/hadoop/lib/native:/opt/hadoop/lib/native::/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib","os.arch":"amd64","java.vm.version":"17.0.14+7-Debian-1deb12u1","jetty.git.hash":"ec6782ff5ead824dabdcf47fa98f90a4aedff401","java.runtime.version":"17.0.14+7-Debian-1deb12u1","java.vm.info":"mixed mode, sharing","java.runtime.name":"OpenJDK Runtime Environment","java.version.date":"2025-01-21","file.separator":"/","java.class.version":"61.0","java.specification.name":"Java Platform API Specification","file.encoding":"UTF-8","jdk.reflect.useDirectMethodHandle":"false","user.timezone":"Etc/UTC","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","java.vm.compressedOopsMode":"32-bit","os.version":"5.15.167.4-microsoft-standard-WSL2","native.encoding":"UTF-8","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","user.language":"en","java.vendor.url":"https://tracker.debian.org/openjdk-17","os.name":"Linux","java.vm.vendor":"Debian","jdk.debug":"release","java.vendor.url.bug":"https://bugs.debian.org/openjdk-17","user.name":"root","java.vm.name":"OpenJDK 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit --conf spark.master=spark://spark-master:7077 --conf spark.eventLog.enabled=true --conf spark.eventLog.dir=file:///opt/spark/logs/history --conf spark.app.name=01-rdd1 --conf spark.history.fs.logDirectory=file:///opt/spark/logs/history pyspark-shell","java.home":"/usr/lib/jvm/java-17-openjdk-amd64","java.version":"17.0.14","sun.io.unicode.encoding":"UnicodeLittle"},"Metrics Properties":{"*.sink.servlet.class":"org.apache.spark.metrics.sink.MetricsServlet","*.sink.servlet.path":"/metrics/json","applications.sink.servlet.path":"/metrics/applications/json","master.sink.servlet.path":"/metrics/master/json"},"Classpath Entries":{"/opt/spark/jars/spark-kubernetes_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/netty-transport-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar":"System Classpath","/opt/spark/jars/kubernetes-model-common-6.7.2.jar":"System Classpath","/opt/spark/jars/logging-interceptor-3.12.12.jar":"System Classpath","/opt/spark/jars/paranamer-2.8.jar":"System Classpath","/opt/spark/jars/breeze_2.12-2.1.0.jar":"System Classpath","/opt/spark/jars/scala-compiler-2.12.18.jar":"System Classpath","/opt/spark/jars/bonecp-0.8.0.RELEASE.jar":"System Classpath","/opt/spark/jars/hadoop-yarn-server-web-proxy-3.3.4.jar":"System Classpath","/opt/spark/jars/hadoop-client-api-3.3.4.jar":"System Classpath","/opt/spark/jars/netty-handler-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/spark-network-shuffle_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/jersey-hk2-2.40.jar":"System Classpath","/opt/spark/jars/netty-buffer-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/leveldbjni-all-1.8.jar":"System Classpath","/opt/spark/jars/kubernetes-model-gatewayapi-6.7.2.jar":"System Classpath","/opt/spark/jars/commons-compress-1.23.0.jar":"System Classpath","/opt/spark/jars/log4j-api-2.20.0.jar":"System Classpath","/opt/spark/jars/super-csv-2.2.0.jar":"System Classpath","/opt/spark/jars/javolution-5.5.1.jar":"System Classpath","/opt/spark/jars/hive-shims-scheduler-2.3.9.jar":"System Classpath","/opt/spark/jars/spire-util_2.12-0.17.0.jar":"System Classpath","/opt/spark/jars/netty-transport-classes-epoll-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/gson-2.2.4.jar":"System Classpath","/opt/spark/jars/kubernetes-client-6.7.2.jar":"System Classpath","/opt/spark/jars/parquet-format-structures-1.13.1.jar":"System Classpath","/opt/spark/jars/parquet-column-1.13.1.jar":"System Classpath","/opt/spark/jars/avro-1.11.4.jar":"System Classpath","/opt/spark/jars/tink-1.9.0.jar":"System Classpath","/opt/spark/jars/spark-network-common_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/istack-commons-runtime-3.0.8.jar":"System Classpath","/opt/spark/jars/antlr-runtime-3.5.2.jar":"System Classpath","/opt/spark/jars/arrow-memory-core-12.0.1.jar":"System Classpath","/opt/spark/jars/commons-dbcp-1.4.jar":"System Classpath","/opt/spark/jars/chill_2.12-0.10.0.jar":"System Classpath","/opt/spark/jars/log4j-core-2.20.0.jar":"System Classpath","/opt/spark/jars/algebra_2.12-2.0.1.jar":"System Classpath","/opt/spark/jars/xz-1.9.jar":"System Classpath","/opt/spark/jars/orc-core-1.9.5-shaded-protobuf.jar":"System Classpath","/opt/spark/jars/kubernetes-model-metrics-6.7.2.jar":"System Classpath","/opt/spark/jars/antlr4-runtime-4.9.3.jar":"System Classpath","/opt/spark/jars/spark-kvstore_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/hive-service-rpc-3.1.3.jar":"System Classpath","/opt/spark/jars/activation-1.1.1.jar":"System Classpath","/opt/spark/jars/objenesis-3.3.jar":"System Classpath","/opt/spark/jars/joda-time-2.12.5.jar":"System Classpath","/opt/spark/jars/spark-sql_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/netty-codec-http-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/spark-mesos_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/spark-tags_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar":"System Classpath","/opt/spark/jars/hive-storage-api-2.8.1.jar":"System Classpath","/opt/spark/jars/jackson-databind-2.15.2.jar":"System Classpath","/opt/spark/jars/kubernetes-model-policy-6.7.2.jar":"System Classpath","/opt/spark/jars/cats-kernel_2.12-2.1.1.jar":"System Classpath","/opt/spark/jars/aopalliance-repackaged-2.6.1.jar":"System Classpath","/opt/spark/jars/lapack-3.0.3.jar":"System Classpath","/opt/spark/jars/xbean-asm9-shaded-4.23.jar":"System Classpath","/opt/spark/jars/hadoop-client-runtime-3.3.4.jar":"System Classpath","/opt/spark/jars/JTransforms-3.1.jar":"System Classpath","/opt/spark/jars/jakarta.servlet-api-4.0.3.jar":"System Classpath","/opt/spark/jars/jul-to-slf4j-2.0.7.jar":"System Classpath","/opt/spark/jars/commons-collections-3.2.2.jar":"System Classpath","/opt/spark/jars/oro-2.0.8.jar":"System Classpath","/opt/spark/jars/datanucleus-api-jdo-4.2.4.jar":"System Classpath","/opt/spark/jars/derby-10.14.2.0.jar":"System Classpath","/opt/spark/jars/kubernetes-model-coordination-6.7.2.jar":"System Classpath","/opt/spark/jars/javassist-3.29.2-GA.jar":"System Classpath","/opt/spark/jars/kubernetes-model-scheduling-6.7.2.jar":"System Classpath","/opt/spark/jars/netty-common-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/metrics-jvm-4.2.19.jar":"System Classpath","/opt/spark/jars/spark-launcher_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/commons-pool-1.5.4.jar":"System Classpath","/opt/spark/jars/commons-codec-1.16.1.jar":"System Classpath","/opt/spark/jars/breeze-macros_2.12-2.1.0.jar":"System Classpath","/opt/spark/jars/kubernetes-model-networking-6.7.2.jar":"System Classpath","/opt/spark/jars/py4j-0.10.9.7.jar":"System Classpath","/opt/spark/jars/annotations-17.0.0.jar":"System Classpath","/opt/spark/jars/jaxb-runtime-2.3.2.jar":"System Classpath","/opt/spark/jars/blas-3.0.3.jar":"System Classpath","/opt/spark/jars/json4s-ast_2.12-3.7.0-M11.jar":"System Classpath","/opt/spark/jars/scala-collection-compat_2.12-2.7.0.jar":"System Classpath","/opt/spark/jars/commons-lang-2.6.jar":"System Classpath","/opt/spark/jars/compress-lzf-1.1.2.jar":"System Classpath","/opt/spark/jars/zookeeper-jute-3.6.3.jar":"System Classpath","/opt/spark/jars/spark-repl_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/metrics-graphite-4.2.19.jar":"System Classpath","/opt/spark/jars/snakeyaml-engine-2.6.jar":"System Classpath","/opt/spark/jars/json4s-jackson_2.12-3.7.0-M11.jar":"System Classpath","/opt/spark/jars/orc-shims-1.9.5.jar":"System Classpath","/opt/spark/jars/kubernetes-model-discovery-6.7.2.jar":"System Classpath","/opt/spark/jars/jta-1.1.jar":"System Classpath","/opt/spark/jars/jersey-server-2.40.jar":"System Classpath","/opt/spark/jars/jersey-common-2.40.jar":"System Classpath","/opt/spark/jars/libthrift-0.12.0.jar":"System Classpath","/opt/spark/jars/metrics-json-4.2.19.jar":"System Classpath","/opt/spark/jars/avro-mapred-1.11.4.jar":"System Classpath","/opt/spark/jars/hive-serde-2.3.9.jar":"System Classpath","/opt/spark/jars/RoaringBitmap-0.9.45.jar":"System Classpath","/opt/spark/jars/metrics-core-4.2.19.jar":"System Classpath","/opt/spark/jars/parquet-encoding-1.13.1.jar":"System Classpath","/opt/spark/jars/hive-common-2.3.9.jar":"System Classpath","/opt/spark/jars/hk2-api-2.6.1.jar":"System Classpath","/opt/spark/jars/spark-unsafe_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/kubernetes-model-admissionregistration-6.7.2.jar":"System Classpath","/opt/spark/jars/snakeyaml-2.0.jar":"System Classpath","/opt/spark/jars/guava-14.0.1.jar":"System Classpath","/opt/spark/jars/slf4j-api-2.0.7.jar":"System Classpath","/opt/spark/jars/hive-metastore-2.3.9.jar":"System Classpath","/opt/spark/jars/jcl-over-slf4j-2.0.7.jar":"System Classpath","/opt/spark/jars/json4s-core_2.12-3.7.0-M11.jar":"System Classpath","/opt/spark/jars/kubernetes-model-autoscaling-6.7.2.jar":"System Classpath","/opt/spark/jars/univocity-parsers-2.9.1.jar":"System Classpath","/opt/spark/jars/avro-ipc-1.11.4.jar":"System Classpath","/opt/spark/jars/commons-text-1.10.0.jar":"System Classpath","/opt/spark/jars/kubernetes-model-apiextensions-6.7.2.jar":"System Classpath","/opt/spark/jars/netty-transport-classes-kqueue-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/jersey-client-2.40.jar":"System Classpath","/opt/spark/jars/netty-codec-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/spark-yarn_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/curator-client-2.13.0.jar":"System Classpath","/opt/spark/jars/jackson-mapper-asl-1.9.13.jar":"System Classpath","/opt/spark/jars/jsr305-3.0.0.jar":"System Classpath","/opt/spark/jars/spire_2.12-0.17.0.jar":"System Classpath","/opt/spark/jars/jdo-api-3.0.1.jar":"System Classpath","/opt/spark/jars/hive-exec-2.3.9-core.jar":"System Classpath","/opt/spark/jars/spark-hive-thriftserver_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/jakarta.annotation-api-1.3.5.jar":"System Classpath","/opt/spark/jars/scala-library-2.12.18.jar":"System Classpath","/opt/spark/jars/ivy-2.5.1.jar":"System Classpath","/opt/spark/jars/minlog-1.3.0.jar":"System Classpath","/opt/spark/jars/shims-0.9.45.jar":"System Classpath","/opt/spark/jars/netty-handler-proxy-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/jackson-annotations-2.15.2.jar":"System Classpath","/opt/spark/jars/kubernetes-model-core-6.7.2.jar":"System Classpath","/opt/spark/jars/kubernetes-model-certificates-6.7.2.jar":"System Classpath","/opt/spark/jars/mesos-1.4.3-shaded-protobuf.jar":"System Classpath","/opt/spark/jars/netty-all-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/scala-reflect-2.12.18.jar":"System Classpath","/opt/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar":"System Classpath","/opt/spark/conf/":"System Classpath","/opt/spark/jars/curator-recipes-2.13.0.jar":"System Classpath","/opt/spark/jars/jakarta.validation-api-2.0.2.jar":"System Classpath","/opt/spark/jars/parquet-common-1.13.1.jar":"System Classpath","/opt/spark/jars/orc-mapreduce-1.9.5-shaded-protobuf.jar":"System Classpath","/opt/spark/jars/log4j-slf4j2-impl-2.20.0.jar":"System Classpath","/opt/spark/jars/kubernetes-model-batch-6.7.2.jar":"System Classpath","/opt/spark/jars/kubernetes-httpclient-okhttp-6.7.2.jar":"System Classpath","/opt/spark/jars/spire-macros_2.12-0.17.0.jar":"System Classpath","/opt/spark/jars/datasketches-java-3.3.0.jar":"System Classpath","/opt/spark/jars/spark-core_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/metrics-jmx-4.2.19.jar":"System Classpath","/opt/spark/jars/spark-common-utils_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/hk2-locator-2.6.1.jar":"System Classpath","/opt/spark/jars/lz4-java-1.8.0.jar":"System Classpath","/opt/spark/jars/scala-xml_2.12-2.1.0.jar":"System Classpath","/opt/spark/jars/hive-shims-0.23-2.3.9.jar":"System Classpath","/opt/spark/jars/spire-platform_2.12-0.17.0.jar":"System Classpath","/opt/spark/jars/jakarta.xml.bind-api-2.3.2.jar":"System Classpath","/opt/spark/jars/commons-lang3-3.12.0.jar":"System Classpath","/opt/spark/jars/JLargeArrays-1.5.jar":"System Classpath","/opt/spark/jars/kubernetes-model-rbac-6.7.2.jar":"System Classpath","/opt/spark/jars/jakarta.inject-2.6.1.jar":"System Classpath","/opt/spark/jars/hive-shims-2.3.9.jar":"System Classpath","/opt/spark/jars/parquet-jackson-1.13.1.jar":"System Classpath","/opt/spark/jars/httpcore-4.4.16.jar":"System Classpath","/opt/spark/jars/jodd-core-3.5.2.jar":"System Classpath","/opt/spark/jars/jackson-core-2.15.2.jar":"System Classpath","/opt/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar":"System Classpath","/opt/spark/jars/json-1.8.jar":"System Classpath","/opt/spark/jars/spark-streaming_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/commons-crypto-1.1.0.jar":"System Classpath","/opt/spark/jars/arrow-vector-12.0.1.jar":"System Classpath","/opt/spark/jars/stax-api-1.0.1.jar":"System Classpath","/opt/hadoop/etc/hadoop/":"System Classpath","/opt/spark/jars/hive-beeline-2.3.9.jar":"System Classpath","/opt/spark/jars/datanucleus-rdbms-4.1.19.jar":"System Classpath","/opt/spark/jars/httpclient-4.5.14.jar":"System Classpath","/opt/spark/jars/commons-io-2.16.1.jar":"System Classpath","/opt/spark/jars/okhttp-3.12.12.jar":"System Classpath","/opt/spark/jars/commons-logging-1.1.3.jar":"System Classpath","/opt/spark/jars/datanucleus-core-4.1.17.jar":"System Classpath","/opt/spark/jars/jline-2.14.6.jar":"System Classpath","/opt/spark/jars/kryo-shaded-4.0.2.jar":"System Classpath","/opt/spark/jars/commons-cli-1.5.0.jar":"System Classpath","/opt/spark/jars/spark-sketch_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/jackson-module-scala_2.12-2.15.2.jar":"System Classpath","/opt/spark/jars/HikariCP-2.5.1.jar":"System Classpath","/opt/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar":"System Classpath","/opt/spark/jars/json4s-scalap_2.12-3.7.0-M11.jar":"System Classpath","/opt/spark/jars/chill-java-0.10.0.jar":"System Classpath","/opt/spark/jars/spark-hive_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/spark-graphx_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/arpack_combined_all-0.1.jar":"System Classpath","/opt/spark/jars/scala-parser-combinators_2.12-2.3.0.jar":"System Classpath","/opt/spark/jars/zookeeper-3.6.3.jar":"System Classpath","/opt/spark/jars/hive-llap-common-2.3.9.jar":"System Classpath","/opt/spark/jars/spark-mllib_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/hive-jdbc-2.3.9.jar":"System Classpath","/opt/spark/jars/netty-resolver-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/jpam-1.1.jar":"System Classpath","/opt/spark/jars/opencsv-2.3.jar":"System Classpath","/opt/spark/jars/netty-transport-native-unix-common-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/parquet-hadoop-1.13.1.jar":"System Classpath","/opt/spark/jars/curator-framework-2.13.0.jar":"System Classpath","/opt/spark/jars/jackson-core-asl-1.9.13.jar":"System Classpath","/opt/spark/jars/jackson-dataformat-yaml-2.15.2.jar":"System Classpath","/opt/spark/jars/arrow-memory-netty-12.0.1.jar":"System Classpath","/opt/spark/jars/stream-2.9.6.jar":"System Classpath","/opt/spark/jars/hive-cli-2.3.9.jar":"System Classpath","/opt/spark/jars/kubernetes-model-events-6.7.2.jar":"System Classpath","/opt/spark/jars/spark-mllib-local_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/jackson-datatype-jsr310-2.15.2.jar":"System Classpath","/opt/spark/jars/rocksdbjni-8.3.2.jar":"System Classpath","/opt/spark/jars/kubernetes-model-node-6.7.2.jar":"System Classpath","/opt/spark/jars/commons-compiler-3.1.9.jar":"System Classpath","/opt/spark/jars/ST4-4.0.4.jar":"System Classpath","/opt/spark/jars/log4j-1.2-api-2.20.0.jar":"System Classpath","/opt/spark/jars/datasketches-memory-2.1.0.jar":"System Classpath","/opt/spark/jars/kubernetes-client-api-6.7.2.jar":"System Classpath","/opt/spark/jars/arrow-format-12.0.1.jar":"System Classpath","/opt/spark/jars/arpack-3.0.3.jar":"System Classpath","/opt/spark/jars/kubernetes-model-resource-6.7.2.jar":"System Classpath","/opt/spark/jars/osgi-resource-locator-1.0.3.jar":"System Classpath","/opt/spark/jars/zstd-jni-1.5.5-4.jar":"System Classpath","/opt/spark/jars/jakarta.ws.rs-api-2.1.6.jar":"System Classpath","/opt/spark/jars/hadoop-shaded-guava-1.1.1.jar":"System Classpath","/opt/spark/jars/okio-1.17.6.jar":"System Classpath","/opt/spark/jars/hk2-utils-2.6.1.jar":"System Classpath","/opt/spark/jars/audience-annotations-0.5.0.jar":"System Classpath","/opt/spark/jars/flatbuffers-java-1.12.0.jar":"System Classpath","/opt/spark/jars/spark-catalyst_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/janino-3.1.9.jar":"System Classpath","/opt/spark/jars/aircompressor-0.27.jar":"System Classpath","/opt/spark/jars/commons-math3-3.6.1.jar":"System Classpath","/opt/spark/jars/transaction-api-1.1.jar":"System Classpath","/opt/spark/jars/kubernetes-model-storageclass-6.7.2.jar":"System Classpath","/opt/spark/jars/libfb303-0.9.3.jar":"System Classpath","/opt/spark/jars/snappy-java-1.1.10.5.jar":"System Classpath","/opt/spark/jars/hive-shims-common-2.3.9.jar":"System Classpath","/opt/spark/jars/jersey-container-servlet-core-2.40.jar":"System Classpath","/opt/spark/jars/netty-codec-http2-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/pickle-1.3.jar":"System Classpath","/opt/spark/jars/kubernetes-model-flowcontrol-6.7.2.jar":"System Classpath","/opt/spark/jars/threeten-extra-1.7.1.jar":"System Classpath","/opt/spark/jars/spark-sql-api_2.12-3.5.5.jar":"System Classpath","/opt/spark/jars/kubernetes-model-apps-6.7.2.jar":"System Classpath","/opt/spark/jars/javax.jdo-3.2.0-m3.jar":"System Classpath","/opt/spark/jars/kubernetes-model-extensions-6.7.2.jar":"System Classpath","/opt/spark/jars/netty-codec-socks-4.1.96.Final.jar":"System Classpath","/opt/spark/jars/jersey-container-servlet-2.40.jar":"System Classpath","/opt/spark/jars/zjsonpatch-0.3.0.jar":"System Classpath","/opt/spark/jars/commons-collections4-4.4.jar":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"01-rdd1","App ID":"app-20250310125742-0000","Timestamp":1741611460776,"User":"root"}
{"Event":"SparkListenerExecutorAdded","Timestamp":1741611469916,"Executor ID":"1","Executor Info":{"Host":"172.21.0.4","Total Cores":1,"Log Urls":{"stdout":"http://172.21.0.4:8081/logPage/?appId=app-20250310125742-0000&executorId=1&logType=stdout","stderr":"http://172.21.0.4:8081/logPage/?appId=app-20250310125742-0000&executorId=1&logType=stderr"},"Attributes":{},"Resources":{},"Resource Profile Id":0,"Registration Time":1741611469915}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"1","Host":"172.21.0.4","Port":39955},"Maximum Memory":455501414,"Timestamp":1741611470102,"Maximum Onheap Memory":455501414,"Maximum Offheap Memory":0}
{"Event":"SparkListenerExecutorAdded","Timestamp":1741611470105,"Executor ID":"0","Executor Info":{"Host":"172.21.0.3","Total Cores":1,"Log Urls":{"stdout":"http://172.21.0.3:8081/logPage/?appId=app-20250310125742-0000&executorId=0&logType=stdout","stderr":"http://172.21.0.3:8081/logPage/?appId=app-20250310125742-0000&executorId=0&logType=stderr"},"Attributes":{},"Resources":{},"Resource Profile Id":0,"Registration Time":1741611470105}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"172.21.0.3","Port":34995},"Maximum Memory":455501414,"Timestamp":1741611470278,"Maximum Onheap Memory":455501414,"Maximum Offheap Memory":0}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1741611531189,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/2206711701.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":13,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[12],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"8\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":11,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":8,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[0],"Properties":{"spark.rdd.scope":"{\"id\":\"10\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/2206711701.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/2206711701.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":13,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[12],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"8\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":11,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":8,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611531246,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"10\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/2206711701.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611531469,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611531520,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611531520,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611533596,"Failed":false,"Killed":false,"Accumulables":[{"ID":1,"Name":"number of output rows","Update":"5","Value":"5","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":2,"Name":"duration","Update":"145","Value":"145","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":3,"Name":"internal.metrics.executorDeserializeTime","Update":1127,"Value":1127,"Internal":true,"Count Failed Values":true},{"ID":4,"Name":"internal.metrics.executorDeserializeCpuTime","Update":919341900,"Value":919341900,"Internal":true,"Count Failed Values":true},{"ID":5,"Name":"internal.metrics.executorRunTime","Update":812,"Value":812,"Internal":true,"Count Failed Values":true},{"ID":6,"Name":"internal.metrics.executorCpuTime","Update":771571600,"Value":771571600,"Internal":true,"Count Failed Values":true},{"ID":7,"Name":"internal.metrics.resultSize","Update":2005,"Value":2005,"Internal":true,"Count Failed Values":true},{"ID":8,"Name":"internal.metrics.jvmGCTime","Update":29,"Value":29,"Internal":true,"Count Failed Values":true},{"ID":9,"Name":"internal.metrics.resultSerializationTime","Update":16,"Value":16,"Internal":true,"Count Failed Values":true},{"ID":35,"Name":"internal.metrics.input.recordsRead","Update":5,"Value":5,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":1127,"Executor Deserialize CPU Time":919341900,"Executor Run Time":812,"Executor CPU Time":771571600,"Peak Execution Memory":0,"Result Size":2005,"JVM GC Time":29,"Result Serialization Time":16,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":5},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611531469,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611533599,"Failed":false,"Killed":false,"Accumulables":[{"ID":1,"Name":"number of output rows","Update":"5","Value":"10","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":2,"Name":"duration","Update":"129","Value":"274","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":3,"Name":"internal.metrics.executorDeserializeTime","Update":1120,"Value":2247,"Internal":true,"Count Failed Values":true},{"ID":4,"Name":"internal.metrics.executorDeserializeCpuTime","Update":932282800,"Value":1851624700,"Internal":true,"Count Failed Values":true},{"ID":5,"Name":"internal.metrics.executorRunTime","Update":804,"Value":1616,"Internal":true,"Count Failed Values":true},{"ID":6,"Name":"internal.metrics.executorCpuTime","Update":767356300,"Value":1538927900,"Internal":true,"Count Failed Values":true},{"ID":7,"Name":"internal.metrics.resultSize","Update":2005,"Value":4010,"Internal":true,"Count Failed Values":true},{"ID":8,"Name":"internal.metrics.jvmGCTime","Update":35,"Value":64,"Internal":true,"Count Failed Values":true},{"ID":9,"Name":"internal.metrics.resultSerializationTime","Update":6,"Value":22,"Internal":true,"Count Failed Values":true},{"ID":35,"Name":"internal.metrics.input.recordsRead","Update":5,"Value":10,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":1120,"Executor Deserialize CPU Time":932282800,"Executor Run Time":804,"Executor CPU Time":767356300,"Peak Execution Memory":0,"Result Size":2005,"JVM GC Time":35,"Result Serialization Time":6,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":5},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/2206711701.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":13,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[12],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"8\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":9,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[8],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":11,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":8,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"6\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611531246,"Completion Time":1741611533613,"Accumulables":[{"ID":1,"Name":"number of output rows","Value":"10","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":2,"Name":"duration","Value":"274","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":3,"Name":"internal.metrics.executorDeserializeTime","Value":2247,"Internal":true,"Count Failed Values":true},{"ID":4,"Name":"internal.metrics.executorDeserializeCpuTime","Value":1851624700,"Internal":true,"Count Failed Values":true},{"ID":5,"Name":"internal.metrics.executorRunTime","Value":1616,"Internal":true,"Count Failed Values":true},{"ID":6,"Name":"internal.metrics.executorCpuTime","Value":1538927900,"Internal":true,"Count Failed Values":true},{"ID":7,"Name":"internal.metrics.resultSize","Value":4010,"Internal":true,"Count Failed Values":true},{"ID":8,"Name":"internal.metrics.jvmGCTime","Value":64,"Internal":true,"Count Failed Values":true},{"ID":9,"Name":"internal.metrics.resultSerializationTime","Value":22,"Internal":true,"Count Failed Values":true},{"ID":35,"Name":"internal.metrics.input.recordsRead","Value":10,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1741611533621,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":1,"Submission Time":1741611533850,"Stage Infos":[{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/476432680.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":20,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/476432680.py:1","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":14,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":17,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[16],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":18,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[17],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":16,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[15],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[1],"Properties":{"spark.rdd.scope":"{\"id\":\"21\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/476432680.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/476432680.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":20,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/476432680.py:1","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":14,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":17,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[16],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":18,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[17],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":16,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[15],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611533853,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"21\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/476432680.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611533863,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":1,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611533864,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":3,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611533864,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611534944,"Failed":false,"Killed":false,"Accumulables":[{"ID":38,"Name":"number of output rows","Update":"5","Value":"5","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":39,"Name":"duration","Update":"1594","Value":"1594","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":40,"Name":"internal.metrics.executorDeserializeTime","Update":61,"Value":61,"Internal":true,"Count Failed Values":true},{"ID":41,"Name":"internal.metrics.executorDeserializeCpuTime","Update":46420500,"Value":46420500,"Internal":true,"Count Failed Values":true},{"ID":42,"Name":"internal.metrics.executorRunTime","Update":996,"Value":996,"Internal":true,"Count Failed Values":true},{"ID":43,"Name":"internal.metrics.executorCpuTime","Update":60985000,"Value":60985000,"Internal":true,"Count Failed Values":true},{"ID":44,"Name":"internal.metrics.resultSize","Update":1791,"Value":1791,"Internal":true,"Count Failed Values":true},{"ID":46,"Name":"internal.metrics.resultSerializationTime","Update":1,"Value":1,"Internal":true,"Count Failed Values":true},{"ID":47,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":48,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":72,"Name":"internal.metrics.input.recordsRead","Update":5,"Value":5,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":61,"Executor Deserialize CPU Time":46420500,"Executor Run Time":996,"Executor CPU Time":60985000,"Peak Execution Memory":0,"Result Size":1791,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":5},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":1,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":2,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611533863,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611534951,"Failed":false,"Killed":false,"Accumulables":[{"ID":38,"Name":"number of output rows","Update":"5","Value":"10","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":39,"Name":"duration","Update":"1620","Value":"3214","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":40,"Name":"internal.metrics.executorDeserializeTime","Update":58,"Value":119,"Internal":true,"Count Failed Values":true},{"ID":41,"Name":"internal.metrics.executorDeserializeCpuTime","Update":41991900,"Value":88412400,"Internal":true,"Count Failed Values":true},{"ID":42,"Name":"internal.metrics.executorRunTime","Update":1007,"Value":2003,"Internal":true,"Count Failed Values":true},{"ID":43,"Name":"internal.metrics.executorCpuTime","Update":52489000,"Value":113474000,"Internal":true,"Count Failed Values":true},{"ID":44,"Name":"internal.metrics.resultSize","Update":1791,"Value":3582,"Internal":true,"Count Failed Values":true},{"ID":46,"Name":"internal.metrics.resultSerializationTime","Update":2,"Value":3,"Internal":true,"Count Failed Values":true},{"ID":47,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":48,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":72,"Name":"internal.metrics.input.recordsRead","Update":5,"Value":10,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":54390496,"JVMOffHeapMemory":68643256,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":48814,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":48814,"OffHeapUnifiedMemory":0,"DirectPoolMemory":8466064,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":6,"MinorGCTime":88,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":88},"Task Metrics":{"Executor Deserialize Time":58,"Executor Deserialize CPU Time":41991900,"Executor Run Time":1007,"Executor CPU Time":52489000,"Peak Execution Memory":0,"Result Size":1791,"JVM GC Time":0,"Result Serialization Time":2,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":5},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/476432680.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":20,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/476432680.py:1","Parent IDs":[19],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":14,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":19,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"20\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[18],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":17,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[16],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":18,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"19\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[17],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":16,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"17\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[15],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611533853,"Completion Time":1741611534956,"Accumulables":[{"ID":38,"Name":"number of output rows","Value":"10","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":39,"Name":"duration","Value":"3214","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":40,"Name":"internal.metrics.executorDeserializeTime","Value":119,"Internal":true,"Count Failed Values":true},{"ID":41,"Name":"internal.metrics.executorDeserializeCpuTime","Value":88412400,"Internal":true,"Count Failed Values":true},{"ID":42,"Name":"internal.metrics.executorRunTime","Value":2003,"Internal":true,"Count Failed Values":true},{"ID":43,"Name":"internal.metrics.executorCpuTime","Value":113474000,"Internal":true,"Count Failed Values":true},{"ID":44,"Name":"internal.metrics.resultSize","Value":3582,"Internal":true,"Count Failed Values":true},{"ID":46,"Name":"internal.metrics.resultSerializationTime","Value":3,"Internal":true,"Count Failed Values":true},{"ID":47,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":48,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":72,"Name":"internal.metrics.input.recordsRead","Value":10,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":1,"Completion Time":1741611534957,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":2,"Submission Time":1741611535018,"Stage Infos":[{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[2],"Properties":{"spark.rdd.scope":"{\"id\":\"29\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611535019,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"29\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611535028,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":2,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611535029,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":5,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611535029,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611535264,"Failed":false,"Killed":false,"Accumulables":[{"ID":75,"Name":"internal.metrics.executorDeserializeTime","Update":35,"Value":35,"Internal":true,"Count Failed Values":true},{"ID":76,"Name":"internal.metrics.executorDeserializeCpuTime","Update":19160700,"Value":19160700,"Internal":true,"Count Failed Values":true},{"ID":77,"Name":"internal.metrics.executorRunTime","Update":178,"Value":178,"Internal":true,"Count Failed Values":true},{"ID":78,"Name":"internal.metrics.executorCpuTime","Update":9815100,"Value":9815100,"Internal":true,"Count Failed Values":true},{"ID":79,"Name":"internal.metrics.resultSize","Update":1426,"Value":1426,"Internal":true,"Count Failed Values":true},{"ID":81,"Name":"internal.metrics.resultSerializationTime","Update":1,"Value":1,"Internal":true,"Count Failed Values":true},{"ID":82,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":83,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":35,"Executor Deserialize CPU Time":19160700,"Executor Run Time":178,"Executor CPU Time":9815100,"Peak Execution Memory":0,"Result Size":1426,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":2,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":4,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611535028,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611535271,"Failed":false,"Killed":false,"Accumulables":[{"ID":75,"Name":"internal.metrics.executorDeserializeTime","Update":37,"Value":72,"Internal":true,"Count Failed Values":true},{"ID":76,"Name":"internal.metrics.executorDeserializeCpuTime","Update":19568700,"Value":38729400,"Internal":true,"Count Failed Values":true},{"ID":77,"Name":"internal.metrics.executorRunTime","Update":185,"Value":363,"Internal":true,"Count Failed Values":true},{"ID":78,"Name":"internal.metrics.executorCpuTime","Update":8385800,"Value":18200900,"Internal":true,"Count Failed Values":true},{"ID":79,"Name":"internal.metrics.resultSize","Update":1426,"Value":2852,"Internal":true,"Count Failed Values":true},{"ID":81,"Name":"internal.metrics.resultSerializationTime","Update":1,"Value":2,"Internal":true,"Count Failed Values":true},{"ID":82,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":83,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":37,"Executor Deserialize CPU Time":19568700,"Executor Run Time":185,"Executor CPU Time":8385800,"Peak Execution Memory":0,"Result Size":1426,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611535019,"Completion Time":1741611535274,"Accumulables":[{"ID":75,"Name":"internal.metrics.executorDeserializeTime","Value":72,"Internal":true,"Count Failed Values":true},{"ID":76,"Name":"internal.metrics.executorDeserializeCpuTime","Value":38729400,"Internal":true,"Count Failed Values":true},{"ID":77,"Name":"internal.metrics.executorRunTime","Value":363,"Internal":true,"Count Failed Values":true},{"ID":78,"Name":"internal.metrics.executorCpuTime","Value":18200900,"Internal":true,"Count Failed Values":true},{"ID":79,"Name":"internal.metrics.resultSize","Value":2852,"Internal":true,"Count Failed Values":true},{"ID":81,"Name":"internal.metrics.resultSerializationTime","Value":2,"Internal":true,"Count Failed Values":true},{"ID":82,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":83,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":2,"Completion Time":1741611535275,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":3,"Submission Time":1741611535311,"Stage Infos":[{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[3],"Properties":{"spark.rdd.scope":"{\"id\":\"32\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611535312,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"32\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611535324,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":3,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611535324,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":7,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611535324,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611535393,"Failed":false,"Killed":false,"Accumulables":[{"ID":110,"Name":"internal.metrics.executorDeserializeTime","Update":39,"Value":39,"Internal":true,"Count Failed Values":true},{"ID":111,"Name":"internal.metrics.executorDeserializeCpuTime","Update":19863700,"Value":19863700,"Internal":true,"Count Failed Values":true},{"ID":112,"Name":"internal.metrics.executorRunTime","Update":2,"Value":2,"Internal":true,"Count Failed Values":true},{"ID":113,"Name":"internal.metrics.executorCpuTime","Update":2211000,"Value":2211000,"Internal":true,"Count Failed Values":true},{"ID":114,"Name":"internal.metrics.resultSize","Update":922,"Value":922,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":39,"Executor Deserialize CPU Time":19863700,"Executor Run Time":2,"Executor CPU Time":2211000,"Peak Execution Memory":0,"Result Size":922,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":3,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":6,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611535324,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611535403,"Failed":false,"Killed":false,"Accumulables":[{"ID":110,"Name":"internal.metrics.executorDeserializeTime","Update":52,"Value":91,"Internal":true,"Count Failed Values":true},{"ID":111,"Name":"internal.metrics.executorDeserializeCpuTime","Update":28512300,"Value":48376000,"Internal":true,"Count Failed Values":true},{"ID":112,"Name":"internal.metrics.executorRunTime","Update":4,"Value":6,"Internal":true,"Count Failed Values":true},{"ID":113,"Name":"internal.metrics.executorCpuTime","Update":3793500,"Value":6004500,"Internal":true,"Count Failed Values":true},{"ID":114,"Name":"internal.metrics.resultSize","Update":972,"Value":1894,"Internal":true,"Count Failed Values":true},{"ID":116,"Name":"internal.metrics.resultSerializationTime","Update":2,"Value":2,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":52,"Executor Deserialize CPU Time":28512300,"Executor Run Time":4,"Executor CPU Time":3793500,"Peak Execution Memory":0,"Result Size":972,"JVM GC Time":0,"Result Serialization Time":2,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":3,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611535312,"Completion Time":1741611535405,"Accumulables":[{"ID":110,"Name":"internal.metrics.executorDeserializeTime","Value":91,"Internal":true,"Count Failed Values":true},{"ID":111,"Name":"internal.metrics.executorDeserializeCpuTime","Value":48376000,"Internal":true,"Count Failed Values":true},{"ID":112,"Name":"internal.metrics.executorRunTime","Value":6,"Internal":true,"Count Failed Values":true},{"ID":113,"Name":"internal.metrics.executorCpuTime","Value":6004500,"Internal":true,"Count Failed Values":true},{"ID":114,"Name":"internal.metrics.resultSize","Value":1894,"Internal":true,"Count Failed Values":true},{"ID":116,"Name":"internal.metrics.resultSerializationTime","Value":2,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":3,"Completion Time":1741611535406,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":4,"Submission Time":1741611541261,"Stage Infos":[{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":22,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[4],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":22,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611541262,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":4,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611541265,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":4,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":8,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611541265,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611541367,"Failed":false,"Killed":false,"Accumulables":[{"ID":145,"Name":"internal.metrics.executorDeserializeTime","Update":13,"Value":13,"Internal":true,"Count Failed Values":true},{"ID":146,"Name":"internal.metrics.executorDeserializeCpuTime","Update":8679400,"Value":8679400,"Internal":true,"Count Failed Values":true},{"ID":147,"Name":"internal.metrics.executorRunTime","Update":81,"Value":81,"Internal":true,"Count Failed Values":true},{"ID":148,"Name":"internal.metrics.executorCpuTime","Update":2538300,"Value":2538300,"Internal":true,"Count Failed Values":true},{"ID":149,"Name":"internal.metrics.resultSize","Update":1396,"Value":1396,"Internal":true,"Count Failed Values":true},{"ID":151,"Name":"internal.metrics.resultSerializationTime","Update":1,"Value":1,"Internal":true,"Count Failed Values":true},{"ID":152,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":153,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":13,"Executor Deserialize CPU Time":8679400,"Executor Run Time":81,"Executor CPU Time":2538300,"Peak Execution Memory":0,"Result Size":1396,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":4,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":22,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611541262,"Completion Time":1741611541369,"Accumulables":[{"ID":145,"Name":"internal.metrics.executorDeserializeTime","Value":13,"Internal":true,"Count Failed Values":true},{"ID":146,"Name":"internal.metrics.executorDeserializeCpuTime","Value":8679400,"Internal":true,"Count Failed Values":true},{"ID":147,"Name":"internal.metrics.executorRunTime","Value":81,"Internal":true,"Count Failed Values":true},{"ID":148,"Name":"internal.metrics.executorCpuTime","Value":2538300,"Internal":true,"Count Failed Values":true},{"ID":149,"Name":"internal.metrics.resultSize","Value":1396,"Internal":true,"Count Failed Values":true},{"ID":151,"Name":"internal.metrics.resultSerializationTime","Value":1,"Internal":true,"Count Failed Values":true},{"ID":152,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":153,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":4,"Completion Time":1741611541369,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":5,"Submission Time":1741611541384,"Stage Infos":[{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":23,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[5],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":23,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611541385,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":5,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611541390,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":5,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":9,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611541390,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611541518,"Failed":false,"Killed":false,"Accumulables":[{"ID":180,"Name":"internal.metrics.executorDeserializeTime","Update":15,"Value":15,"Internal":true,"Count Failed Values":true},{"ID":181,"Name":"internal.metrics.executorDeserializeCpuTime","Update":8798200,"Value":8798200,"Internal":true,"Count Failed Values":true},{"ID":182,"Name":"internal.metrics.executorRunTime","Update":103,"Value":103,"Internal":true,"Count Failed Values":true},{"ID":183,"Name":"internal.metrics.executorCpuTime","Update":4009900,"Value":4009900,"Internal":true,"Count Failed Values":true},{"ID":184,"Name":"internal.metrics.resultSize","Update":1372,"Value":1372,"Internal":true,"Count Failed Values":true},{"ID":187,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":188,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":15,"Executor Deserialize CPU Time":8798200,"Executor Run Time":103,"Executor CPU Time":4009900,"Peak Execution Memory":0,"Result Size":1372,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":5,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":23,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611541385,"Completion Time":1741611541520,"Accumulables":[{"ID":180,"Name":"internal.metrics.executorDeserializeTime","Value":15,"Internal":true,"Count Failed Values":true},{"ID":181,"Name":"internal.metrics.executorDeserializeCpuTime","Value":8798200,"Internal":true,"Count Failed Values":true},{"ID":182,"Name":"internal.metrics.executorRunTime","Value":103,"Internal":true,"Count Failed Values":true},{"ID":183,"Name":"internal.metrics.executorCpuTime","Value":4009900,"Internal":true,"Count Failed Values":true},{"ID":184,"Name":"internal.metrics.resultSize","Value":1372,"Internal":true,"Count Failed Values":true},{"ID":187,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":188,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":5,"Completion Time":1741611541520,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":6,"Submission Time":1741611600663,"Stage Infos":[{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[6],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611600664,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":6,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611600669,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":6,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":10,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611600669,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611600761,"Failed":false,"Killed":false,"Accumulables":[{"ID":215,"Name":"internal.metrics.executorDeserializeTime","Update":14,"Value":14,"Internal":true,"Count Failed Values":true},{"ID":216,"Name":"internal.metrics.executorDeserializeCpuTime","Update":7162200,"Value":7162200,"Internal":true,"Count Failed Values":true},{"ID":217,"Name":"internal.metrics.executorRunTime","Update":71,"Value":71,"Internal":true,"Count Failed Values":true},{"ID":218,"Name":"internal.metrics.executorCpuTime","Update":3849100,"Value":3849100,"Internal":true,"Count Failed Values":true},{"ID":219,"Name":"internal.metrics.resultSize","Update":1353,"Value":1353,"Internal":true,"Count Failed Values":true},{"ID":222,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":223,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":14,"Executor Deserialize CPU Time":7162200,"Executor Run Time":71,"Executor CPU Time":3849100,"Peak Execution Memory":0,"Result Size":1353,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":6,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":28,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611600664,"Completion Time":1741611600762,"Accumulables":[{"ID":215,"Name":"internal.metrics.executorDeserializeTime","Value":14,"Internal":true,"Count Failed Values":true},{"ID":216,"Name":"internal.metrics.executorDeserializeCpuTime","Value":7162200,"Internal":true,"Count Failed Values":true},{"ID":217,"Name":"internal.metrics.executorRunTime","Value":71,"Internal":true,"Count Failed Values":true},{"ID":218,"Name":"internal.metrics.executorCpuTime","Value":3849100,"Internal":true,"Count Failed Values":true},{"ID":219,"Name":"internal.metrics.resultSize","Value":1353,"Internal":true,"Count Failed Values":true},{"ID":222,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":223,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":6,"Completion Time":1741611600762,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":7,"Submission Time":1741611600773,"Stage Infos":[{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":29,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[7],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":29,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611600773,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":7,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611600781,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":7,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":11,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611600781,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611600902,"Failed":false,"Killed":false,"Accumulables":[{"ID":250,"Name":"internal.metrics.executorDeserializeTime","Update":15,"Value":15,"Internal":true,"Count Failed Values":true},{"ID":251,"Name":"internal.metrics.executorDeserializeCpuTime","Update":10118400,"Value":10118400,"Internal":true,"Count Failed Values":true},{"ID":252,"Name":"internal.metrics.executorRunTime","Update":96,"Value":96,"Internal":true,"Count Failed Values":true},{"ID":253,"Name":"internal.metrics.executorCpuTime","Update":3855100,"Value":3855100,"Internal":true,"Count Failed Values":true},{"ID":254,"Name":"internal.metrics.resultSize","Update":1372,"Value":1372,"Internal":true,"Count Failed Values":true},{"ID":257,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":258,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":15,"Executor Deserialize CPU Time":10118400,"Executor Run Time":96,"Executor CPU Time":3855100,"Peak Execution Memory":0,"Result Size":1372,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":7,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":29,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611600773,"Completion Time":1741611600905,"Accumulables":[{"ID":250,"Name":"internal.metrics.executorDeserializeTime","Value":15,"Internal":true,"Count Failed Values":true},{"ID":251,"Name":"internal.metrics.executorDeserializeCpuTime","Value":10118400,"Internal":true,"Count Failed Values":true},{"ID":252,"Name":"internal.metrics.executorRunTime","Value":96,"Internal":true,"Count Failed Values":true},{"ID":253,"Name":"internal.metrics.executorCpuTime","Value":3855100,"Internal":true,"Count Failed Values":true},{"ID":254,"Name":"internal.metrics.resultSize","Value":1372,"Internal":true,"Count Failed Values":true},{"ID":257,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":258,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":7,"Completion Time":1741611600906,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":8,"Submission Time":1741611600973,"Stage Infos":[{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[25],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":24,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"38\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":25,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"38\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[8],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[25],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":24,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"38\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":25,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"38\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611600976,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":8,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611600987,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":8,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":287,"Update":"322","Internal":false,"Count Failed Values":true},{"ID":289,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":12,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611600987,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611601384,"Failed":true,"Killed":false,"Accumulables":[{"ID":287,"Name":"internal.metrics.executorRunTime","Update":322,"Value":322,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":322,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":8,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":0,"Attempt":1,"Partition ID":0,"Launch Time":1741611601391,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":8,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":287,"Update":"413","Internal":false,"Count Failed Values":true},{"ID":289,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":13,"Index":0,"Attempt":1,"Partition ID":0,"Launch Time":1741611601391,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611601864,"Failed":true,"Killed":false,"Accumulables":[{"ID":287,"Name":"internal.metrics.executorRunTime","Update":413,"Value":735,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":413,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":8,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":0,"Attempt":2,"Partition ID":0,"Launch Time":1741611601867,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":8,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":287,"Update":"29","Internal":false,"Count Failed Values":true},{"ID":289,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":14,"Index":0,"Attempt":2,"Partition ID":0,"Launch Time":1741611601867,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611601913,"Failed":true,"Killed":false,"Accumulables":[{"ID":287,"Name":"internal.metrics.executorRunTime","Update":29,"Value":764,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":29,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":8,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":0,"Attempt":3,"Partition ID":0,"Launch Time":1741611601916,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":8,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":287,"Update":"11","Internal":false,"Count Failed Values":true},{"ID":289,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":15,"Index":0,"Attempt":3,"Partition ID":0,"Launch Time":1741611601916,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611601947,"Failed":true,"Killed":false,"Accumulables":[{"ID":287,"Name":"internal.metrics.executorRunTime","Update":11,"Value":775,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":11,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":8,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":30,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[25],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":24,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"38\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":25,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"38\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[24],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611600976,"Completion Time":1741611601955,"Failure Reason":"Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 15) (172.21.0.4 executor 1): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:","Accumulables":[{"ID":287,"Name":"internal.metrics.executorRunTime","Value":775,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":8,"Completion Time":1741611601958,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job aborted due to stage failure: Task 0 in stage 8.0 failed 4 times, most recent failure: Lost task 0.3 in stage 8.0 (TID 15) (172.21.0.4 executor 1): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":2856},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$abortStage$2","File Name":"DAGScheduler.scala","Line Number":2792},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$abortStage$2$adapted","File Name":"DAGScheduler.scala","Line Number":2791},{"Declaring Class":"scala.collection.mutable.ResizableArray","Method Name":"foreach","File Name":"ResizableArray.scala","Line Number":62},{"Declaring Class":"scala.collection.mutable.ResizableArray","Method Name":"foreach$","File Name":"ResizableArray.scala","Line Number":55},{"Declaring Class":"scala.collection.mutable.ArrayBuffer","Method Name":"foreach","File Name":"ArrayBuffer.scala","Line Number":49},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"abortStage","File Name":"DAGScheduler.scala","Line Number":2791},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$handleTaskSetFailed$1","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$handleTaskSetFailed$1$adapted","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"scala.Option","Method Name":"foreach","File Name":"Option.scala","Line Number":407},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleTaskSetFailed","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":3060},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":2994},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":2983},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":49},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"runJob","File Name":"DAGScheduler.scala","Line Number":989},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2393},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2414},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2433},{"Declaring Class":"org.apache.spark.api.python.PythonRDD$","Method Name":"runJob","File Name":"PythonRDD.scala","Line Number":181},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"runJob","File Name":"PythonRDD.scala","Line Number":-1},{"Declaring Class":"jdk.internal.reflect.NativeMethodAccessorImpl","Method Name":"invoke0","File Name":"NativeMethodAccessorImpl.java","Line Number":-2},{"Declaring Class":"jdk.internal.reflect.NativeMethodAccessorImpl","Method Name":"invoke","File Name":"NativeMethodAccessorImpl.java","Line Number":77},{"Declaring Class":"jdk.internal.reflect.DelegatingMethodAccessorImpl","Method Name":"invoke","File Name":"DelegatingMethodAccessorImpl.java","Line Number":43},{"Declaring Class":"java.lang.reflect.Method","Method Name":"invoke","File Name":"Method.java","Line Number":569},{"Declaring Class":"py4j.reflection.MethodInvoker","Method Name":"invoke","File Name":"MethodInvoker.java","Line Number":244},{"Declaring Class":"py4j.reflection.ReflectionEngine","Method Name":"invoke","File Name":"ReflectionEngine.java","Line Number":374},{"Declaring Class":"py4j.Gateway","Method Name":"invoke","File Name":"Gateway.java","Line Number":282},{"Declaring Class":"py4j.commands.AbstractCommand","Method Name":"invokeMethod","File Name":"AbstractCommand.java","Line Number":132},{"Declaring Class":"py4j.commands.CallCommand","Method Name":"execute","File Name":"CallCommand.java","Line Number":79},{"Declaring Class":"py4j.ClientServerConnection","Method Name":"waitForCommands","File Name":"ClientServerConnection.java","Line Number":182},{"Declaring Class":"py4j.ClientServerConnection","Method Name":"run","File Name":"ClientServerConnection.java","Line Number":106},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}]}}}
{"Event":"SparkListenerJobStart","Job ID":9,"Submission Time":1741611724898,"Stage Infos":[{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/476432680.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":41,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/476432680.py:1","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":35,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":39,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"48\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[38],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":38,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"49\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[9],"Properties":{"spark.rdd.scope":"{\"id\":\"50\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/476432680.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/476432680.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":41,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/476432680.py:1","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":35,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":39,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"48\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[38],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":38,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"49\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611724899,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"50\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/476432680.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":9,"Stage Attempt ID":0,"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611724903,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":9,"Stage Attempt ID":0,"Task Info":{"Task ID":17,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611724903,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":9,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":17,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611724903,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611725067,"Failed":false,"Killed":false,"Accumulables":[{"ID":320,"Name":"number of output rows","Update":"5","Value":"5","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":321,"Name":"duration","Update":"9","Value":"9","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":322,"Name":"internal.metrics.executorDeserializeTime","Update":12,"Value":12,"Internal":true,"Count Failed Values":true},{"ID":323,"Name":"internal.metrics.executorDeserializeCpuTime","Update":8294100,"Value":8294100,"Internal":true,"Count Failed Values":true},{"ID":324,"Name":"internal.metrics.executorRunTime","Update":144,"Value":144,"Internal":true,"Count Failed Values":true},{"ID":325,"Name":"internal.metrics.executorCpuTime","Update":11916000,"Value":11916000,"Internal":true,"Count Failed Values":true},{"ID":326,"Name":"internal.metrics.resultSize","Update":1748,"Value":1748,"Internal":true,"Count Failed Values":true},{"ID":329,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":330,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":354,"Name":"internal.metrics.input.recordsRead","Update":5,"Value":5,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":12,"Executor Deserialize CPU Time":8294100,"Executor Run Time":144,"Executor CPU Time":11916000,"Peak Execution Memory":0,"Result Size":1748,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":5},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":9,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":16,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611724903,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611725072,"Failed":false,"Killed":false,"Accumulables":[{"ID":320,"Name":"number of output rows","Update":"5","Value":"10","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":321,"Name":"duration","Update":"6","Value":"15","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":322,"Name":"internal.metrics.executorDeserializeTime","Update":8,"Value":20,"Internal":true,"Count Failed Values":true},{"ID":323,"Name":"internal.metrics.executorDeserializeCpuTime","Update":5375300,"Value":13669400,"Internal":true,"Count Failed Values":true},{"ID":324,"Name":"internal.metrics.executorRunTime","Update":139,"Value":283,"Internal":true,"Count Failed Values":true},{"ID":325,"Name":"internal.metrics.executorCpuTime","Update":3207900,"Value":15123900,"Internal":true,"Count Failed Values":true},{"ID":326,"Name":"internal.metrics.resultSize","Update":1834,"Value":3582,"Internal":true,"Count Failed Values":true},{"ID":327,"Name":"internal.metrics.jvmGCTime","Update":12,"Value":12,"Internal":true,"Count Failed Values":true},{"ID":328,"Name":"internal.metrics.resultSerializationTime","Update":13,"Value":13,"Internal":true,"Count Failed Values":true},{"ID":329,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":330,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":354,"Name":"internal.metrics.input.recordsRead","Update":5,"Value":10,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":8,"Executor Deserialize CPU Time":5375300,"Executor Run Time":139,"Executor CPU Time":3207900,"Peak Execution Memory":0,"Result Size":1834,"JVM GC Time":12,"Result Serialization Time":13,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":5},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":9,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/476432680.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":41,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/476432680.py:1","Parent IDs":[40],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":35,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":36,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[35],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":39,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"48\",\"name\":\"map\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[38],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":37,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"46\",\"name\":\"WholeStageCodegen (1)\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[36],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":38,"Name":"SQLExecutionRDD","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[37],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":40,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"49\",\"name\":\"mapPartitions\"}","Callsite":"javaToPython at NativeMethodAccessorImpl.java:0","Parent IDs":[39],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611724899,"Completion Time":1741611725074,"Accumulables":[{"ID":320,"Name":"number of output rows","Value":"10","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":321,"Name":"duration","Value":"15","Internal":true,"Count Failed Values":true,"Metadata":"sql"},{"ID":322,"Name":"internal.metrics.executorDeserializeTime","Value":20,"Internal":true,"Count Failed Values":true},{"ID":323,"Name":"internal.metrics.executorDeserializeCpuTime","Value":13669400,"Internal":true,"Count Failed Values":true},{"ID":324,"Name":"internal.metrics.executorRunTime","Value":283,"Internal":true,"Count Failed Values":true},{"ID":325,"Name":"internal.metrics.executorCpuTime","Value":15123900,"Internal":true,"Count Failed Values":true},{"ID":326,"Name":"internal.metrics.resultSize","Value":3582,"Internal":true,"Count Failed Values":true},{"ID":327,"Name":"internal.metrics.jvmGCTime","Value":12,"Internal":true,"Count Failed Values":true},{"ID":328,"Name":"internal.metrics.resultSerializationTime","Value":13,"Internal":true,"Count Failed Values":true},{"ID":329,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":330,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":354,"Name":"internal.metrics.input.recordsRead","Value":10,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":9,"Completion Time":1741611725074,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":10,"Submission Time":1741611728286,"Stage Infos":[{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[10],"Properties":{"spark.rdd.scope":"{\"id\":\"58\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611728286,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"58\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":10,"Stage Attempt ID":0,"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611728290,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":10,"Stage Attempt ID":0,"Task Info":{"Task ID":19,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611728291,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":10,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":19,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611728291,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611728394,"Failed":false,"Killed":false,"Accumulables":[{"ID":357,"Name":"internal.metrics.executorDeserializeTime","Update":12,"Value":12,"Internal":true,"Count Failed Values":true},{"ID":358,"Name":"internal.metrics.executorDeserializeCpuTime","Update":6677700,"Value":6677700,"Internal":true,"Count Failed Values":true},{"ID":359,"Name":"internal.metrics.executorRunTime","Update":84,"Value":84,"Internal":true,"Count Failed Values":true},{"ID":360,"Name":"internal.metrics.executorCpuTime","Update":2712800,"Value":2712800,"Internal":true,"Count Failed Values":true},{"ID":361,"Name":"internal.metrics.resultSize","Update":1383,"Value":1383,"Internal":true,"Count Failed Values":true},{"ID":364,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":365,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":12,"Executor Deserialize CPU Time":6677700,"Executor Run Time":84,"Executor CPU Time":2712800,"Peak Execution Memory":0,"Result Size":1383,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":10,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":18,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611728290,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611728402,"Failed":false,"Killed":false,"Accumulables":[{"ID":357,"Name":"internal.metrics.executorDeserializeTime","Update":13,"Value":25,"Internal":true,"Count Failed Values":true},{"ID":358,"Name":"internal.metrics.executorDeserializeCpuTime","Update":6415500,"Value":13093200,"Internal":true,"Count Failed Values":true},{"ID":359,"Name":"internal.metrics.executorRunTime","Update":82,"Value":166,"Internal":true,"Count Failed Values":true},{"ID":360,"Name":"internal.metrics.executorCpuTime","Update":2514400,"Value":5227200,"Internal":true,"Count Failed Values":true},{"ID":361,"Name":"internal.metrics.resultSize","Update":1469,"Value":2852,"Internal":true,"Count Failed Values":true},{"ID":362,"Name":"internal.metrics.jvmGCTime","Update":9,"Value":9,"Internal":true,"Count Failed Values":true},{"ID":363,"Name":"internal.metrics.resultSerializationTime","Update":9,"Value":9,"Internal":true,"Count Failed Values":true},{"ID":364,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":365,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":13,"Executor Deserialize CPU Time":6415500,"Executor Run Time":82,"Executor CPU Time":2514400,"Peak Execution Memory":0,"Result Size":1469,"JVM GC Time":9,"Result Serialization Time":9,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":10,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611728286,"Completion Time":1741611728403,"Accumulables":[{"ID":357,"Name":"internal.metrics.executorDeserializeTime","Value":25,"Internal":true,"Count Failed Values":true},{"ID":358,"Name":"internal.metrics.executorDeserializeCpuTime","Value":13093200,"Internal":true,"Count Failed Values":true},{"ID":359,"Name":"internal.metrics.executorRunTime","Value":166,"Internal":true,"Count Failed Values":true},{"ID":360,"Name":"internal.metrics.executorCpuTime","Value":5227200,"Internal":true,"Count Failed Values":true},{"ID":361,"Name":"internal.metrics.resultSize","Value":2852,"Internal":true,"Count Failed Values":true},{"ID":362,"Name":"internal.metrics.jvmGCTime","Value":9,"Internal":true,"Count Failed Values":true},{"ID":363,"Name":"internal.metrics.resultSerializationTime","Value":9,"Internal":true,"Count Failed Values":true},{"ID":364,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":365,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":10,"Completion Time":1741611728403,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":11,"Submission Time":1741611728417,"Stage Infos":[{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[11],"Properties":{"spark.rdd.scope":"{\"id\":\"60\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611728418,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"60\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":11,"Stage Attempt ID":0,"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611728424,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":11,"Stage Attempt ID":0,"Task Info":{"Task ID":21,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611728425,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":11,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":20,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611728424,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611728447,"Failed":false,"Killed":false,"Accumulables":[{"ID":392,"Name":"internal.metrics.executorDeserializeTime","Update":14,"Value":14,"Internal":true,"Count Failed Values":true},{"ID":393,"Name":"internal.metrics.executorDeserializeCpuTime","Update":6024900,"Value":6024900,"Internal":true,"Count Failed Values":true},{"ID":395,"Name":"internal.metrics.executorCpuTime","Update":879500,"Value":879500,"Internal":true,"Count Failed Values":true},{"ID":396,"Name":"internal.metrics.resultSize","Update":886,"Value":886,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":14,"Executor Deserialize CPU Time":6024900,"Executor Run Time":0,"Executor CPU Time":879500,"Peak Execution Memory":0,"Result Size":886,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":11,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":21,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611728425,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611728448,"Failed":false,"Killed":false,"Accumulables":[{"ID":392,"Name":"internal.metrics.executorDeserializeTime","Update":15,"Value":29,"Internal":true,"Count Failed Values":true},{"ID":393,"Name":"internal.metrics.executorDeserializeCpuTime","Update":7736300,"Value":13761200,"Internal":true,"Count Failed Values":true},{"ID":395,"Name":"internal.metrics.executorCpuTime","Update":929300,"Value":1808800,"Internal":true,"Count Failed Values":true},{"ID":396,"Name":"internal.metrics.resultSize","Update":879,"Value":1765,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":15,"Executor Deserialize CPU Time":7736300,"Executor Run Time":0,"Executor CPU Time":929300,"Peak Execution Memory":0,"Result Size":879,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":11,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611728418,"Completion Time":1741611728449,"Accumulables":[{"ID":392,"Name":"internal.metrics.executorDeserializeTime","Value":29,"Internal":true,"Count Failed Values":true},{"ID":393,"Name":"internal.metrics.executorDeserializeCpuTime","Value":13761200,"Internal":true,"Count Failed Values":true},{"ID":395,"Name":"internal.metrics.executorCpuTime","Value":1808800,"Internal":true,"Count Failed Values":true},{"ID":396,"Name":"internal.metrics.resultSize","Value":1765,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":11,"Completion Time":1741611728449,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":12,"Submission Time":1741611730438,"Stage Infos":[{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":42,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[12],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":42,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611730438,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":12,"Stage Attempt ID":0,"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611730443,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":12,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":22,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611730443,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611730513,"Failed":false,"Killed":false,"Accumulables":[{"ID":427,"Name":"internal.metrics.executorDeserializeTime","Update":7,"Value":7,"Internal":true,"Count Failed Values":true},{"ID":428,"Name":"internal.metrics.executorDeserializeCpuTime","Update":4148300,"Value":4148300,"Internal":true,"Count Failed Values":true},{"ID":429,"Name":"internal.metrics.executorRunTime","Update":58,"Value":58,"Internal":true,"Count Failed Values":true},{"ID":430,"Name":"internal.metrics.executorCpuTime","Update":1889700,"Value":1889700,"Internal":true,"Count Failed Values":true},{"ID":431,"Name":"internal.metrics.resultSize","Update":1353,"Value":1353,"Internal":true,"Count Failed Values":true},{"ID":434,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":435,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":7,"Executor Deserialize CPU Time":4148300,"Executor Run Time":58,"Executor CPU Time":1889700,"Peak Execution Memory":0,"Result Size":1353,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":12,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":42,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611730438,"Completion Time":1741611730514,"Accumulables":[{"ID":427,"Name":"internal.metrics.executorDeserializeTime","Value":7,"Internal":true,"Count Failed Values":true},{"ID":428,"Name":"internal.metrics.executorDeserializeCpuTime","Value":4148300,"Internal":true,"Count Failed Values":true},{"ID":429,"Name":"internal.metrics.executorRunTime","Value":58,"Internal":true,"Count Failed Values":true},{"ID":430,"Name":"internal.metrics.executorCpuTime","Value":1889700,"Internal":true,"Count Failed Values":true},{"ID":431,"Name":"internal.metrics.resultSize","Value":1353,"Internal":true,"Count Failed Values":true},{"ID":434,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":435,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":12,"Completion Time":1741611730514,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":13,"Submission Time":1741611730524,"Stage Infos":[{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[13],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611730525,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":13,"Stage Attempt ID":0,"Task Info":{"Task ID":23,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611730535,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":13,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":23,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611730535,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611730708,"Failed":false,"Killed":false,"Accumulables":[{"ID":462,"Name":"internal.metrics.executorDeserializeTime","Update":16,"Value":16,"Internal":true,"Count Failed Values":true},{"ID":463,"Name":"internal.metrics.executorDeserializeCpuTime","Update":5743500,"Value":5743500,"Internal":true,"Count Failed Values":true},{"ID":464,"Name":"internal.metrics.executorRunTime","Update":146,"Value":146,"Internal":true,"Count Failed Values":true},{"ID":465,"Name":"internal.metrics.executorCpuTime","Update":3285000,"Value":3285000,"Internal":true,"Count Failed Values":true},{"ID":466,"Name":"internal.metrics.resultSize","Update":1372,"Value":1372,"Internal":true,"Count Failed Values":true},{"ID":469,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":470,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":16,"Executor Deserialize CPU Time":5743500,"Executor Run Time":146,"Executor CPU Time":3285000,"Peak Execution Memory":0,"Result Size":1372,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":13,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":43,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611730525,"Completion Time":1741611730710,"Accumulables":[{"ID":462,"Name":"internal.metrics.executorDeserializeTime","Value":16,"Internal":true,"Count Failed Values":true},{"ID":463,"Name":"internal.metrics.executorDeserializeCpuTime","Value":5743500,"Internal":true,"Count Failed Values":true},{"ID":464,"Name":"internal.metrics.executorRunTime","Value":146,"Internal":true,"Count Failed Values":true},{"ID":465,"Name":"internal.metrics.executorCpuTime","Value":3285000,"Internal":true,"Count Failed Values":true},{"ID":466,"Name":"internal.metrics.resultSize","Value":1372,"Internal":true,"Count Failed Values":true},{"ID":469,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":470,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":13,"Completion Time":1741611730710,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":14,"Submission Time":1741611730753,"Stage Infos":[{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":44,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[14],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":44,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611730754,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":14,"Stage Attempt ID":0,"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611730761,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":14,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":499,"Update":"63","Internal":false,"Count Failed Values":true},{"ID":501,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":24,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611730761,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611730841,"Failed":true,"Killed":false,"Accumulables":[{"ID":499,"Name":"internal.metrics.executorRunTime","Update":63,"Value":63,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":63,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":14,"Stage Attempt ID":0,"Task Info":{"Task ID":25,"Index":0,"Attempt":1,"Partition ID":0,"Launch Time":1741611730843,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":14,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":499,"Update":"117","Internal":false,"Count Failed Values":true},{"ID":501,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":25,"Index":0,"Attempt":1,"Partition ID":0,"Launch Time":1741611730843,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611730976,"Failed":true,"Killed":false,"Accumulables":[{"ID":499,"Name":"internal.metrics.executorRunTime","Update":117,"Value":180,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":117,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":14,"Stage Attempt ID":0,"Task Info":{"Task ID":26,"Index":0,"Attempt":2,"Partition ID":0,"Launch Time":1741611730977,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":14,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":499,"Update":"12","Internal":false,"Count Failed Values":true},{"ID":501,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":26,"Index":0,"Attempt":2,"Partition ID":0,"Launch Time":1741611730977,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611731008,"Failed":true,"Killed":false,"Accumulables":[{"ID":499,"Name":"internal.metrics.executorRunTime","Update":12,"Value":192,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":12,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":14,"Stage Attempt ID":0,"Task Info":{"Task ID":27,"Index":0,"Attempt":3,"Partition ID":0,"Launch Time":1741611731010,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":14,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":499,"Update":"27","Internal":false,"Count Failed Values":true},{"ID":501,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":27,"Index":0,"Attempt":3,"Partition ID":0,"Launch Time":1741611731010,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611731068,"Failed":true,"Killed":false,"Accumulables":[{"ID":499,"Name":"internal.metrics.executorRunTime","Update":27,"Value":219,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":27,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":14,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":44,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611730754,"Completion Time":1741611731071,"Failure Reason":"Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 27) (172.21.0.3 executor 0): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:","Accumulables":[{"ID":499,"Name":"internal.metrics.executorRunTime","Value":219,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":14,"Completion Time":1741611731073,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 27) (172.21.0.3 executor 0): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":2856},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$abortStage$2","File Name":"DAGScheduler.scala","Line Number":2792},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$abortStage$2$adapted","File Name":"DAGScheduler.scala","Line Number":2791},{"Declaring Class":"scala.collection.mutable.ResizableArray","Method Name":"foreach","File Name":"ResizableArray.scala","Line Number":62},{"Declaring Class":"scala.collection.mutable.ResizableArray","Method Name":"foreach$","File Name":"ResizableArray.scala","Line Number":55},{"Declaring Class":"scala.collection.mutable.ArrayBuffer","Method Name":"foreach","File Name":"ArrayBuffer.scala","Line Number":49},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"abortStage","File Name":"DAGScheduler.scala","Line Number":2791},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$handleTaskSetFailed$1","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$handleTaskSetFailed$1$adapted","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"scala.Option","Method Name":"foreach","File Name":"Option.scala","Line Number":407},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleTaskSetFailed","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":3060},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":2994},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":2983},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":49},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"runJob","File Name":"DAGScheduler.scala","Line Number":989},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2393},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2414},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2433},{"Declaring Class":"org.apache.spark.api.python.PythonRDD$","Method Name":"runJob","File Name":"PythonRDD.scala","Line Number":181},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"runJob","File Name":"PythonRDD.scala","Line Number":-1},{"Declaring Class":"jdk.internal.reflect.NativeMethodAccessorImpl","Method Name":"invoke0","File Name":"NativeMethodAccessorImpl.java","Line Number":-2},{"Declaring Class":"jdk.internal.reflect.NativeMethodAccessorImpl","Method Name":"invoke","File Name":"NativeMethodAccessorImpl.java","Line Number":77},{"Declaring Class":"jdk.internal.reflect.DelegatingMethodAccessorImpl","Method Name":"invoke","File Name":"DelegatingMethodAccessorImpl.java","Line Number":43},{"Declaring Class":"java.lang.reflect.Method","Method Name":"invoke","File Name":"Method.java","Line Number":569},{"Declaring Class":"py4j.reflection.MethodInvoker","Method Name":"invoke","File Name":"MethodInvoker.java","Line Number":244},{"Declaring Class":"py4j.reflection.ReflectionEngine","Method Name":"invoke","File Name":"ReflectionEngine.java","Line Number":374},{"Declaring Class":"py4j.Gateway","Method Name":"invoke","File Name":"Gateway.java","Line Number":282},{"Declaring Class":"py4j.commands.AbstractCommand","Method Name":"invokeMethod","File Name":"AbstractCommand.java","Line Number":132},{"Declaring Class":"py4j.commands.CallCommand","Method Name":"execute","File Name":"CallCommand.java","Line Number":79},{"Declaring Class":"py4j.ClientServerConnection","Method Name":"waitForCommands","File Name":"ClientServerConnection.java","Line Number":182},{"Declaring Class":"py4j.ClientServerConnection","Method Name":"run","File Name":"ClientServerConnection.java","Line Number":106},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}]}}}
{"Event":"SparkListenerJobStart","Job ID":15,"Submission Time":1741611747350,"Stage Infos":[{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":45,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[15],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":45,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611747351,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":15,"Stage Attempt ID":0,"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611747354,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":15,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":28,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611747354,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611747430,"Failed":false,"Killed":false,"Accumulables":[{"ID":532,"Name":"internal.metrics.executorDeserializeTime","Update":9,"Value":9,"Internal":true,"Count Failed Values":true},{"ID":533,"Name":"internal.metrics.executorDeserializeCpuTime","Update":4533300,"Value":4533300,"Internal":true,"Count Failed Values":true},{"ID":534,"Name":"internal.metrics.executorRunTime","Update":61,"Value":61,"Internal":true,"Count Failed Values":true},{"ID":535,"Name":"internal.metrics.executorCpuTime","Update":1999200,"Value":1999200,"Internal":true,"Count Failed Values":true},{"ID":536,"Name":"internal.metrics.resultSize","Update":1353,"Value":1353,"Internal":true,"Count Failed Values":true},{"ID":539,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":540,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":9,"Executor Deserialize CPU Time":4533300,"Executor Run Time":61,"Executor CPU Time":1999200,"Peak Execution Memory":0,"Result Size":1353,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":15,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":45,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611747351,"Completion Time":1741611747431,"Accumulables":[{"ID":532,"Name":"internal.metrics.executorDeserializeTime","Value":9,"Internal":true,"Count Failed Values":true},{"ID":533,"Name":"internal.metrics.executorDeserializeCpuTime","Value":4533300,"Internal":true,"Count Failed Values":true},{"ID":534,"Name":"internal.metrics.executorRunTime","Value":61,"Internal":true,"Count Failed Values":true},{"ID":535,"Name":"internal.metrics.executorCpuTime","Value":1999200,"Internal":true,"Count Failed Values":true},{"ID":536,"Name":"internal.metrics.resultSize","Value":1353,"Internal":true,"Count Failed Values":true},{"ID":539,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":540,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":15,"Completion Time":1741611747431,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":16,"Submission Time":1741611747440,"Stage Infos":[{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":46,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[16],"Properties":{}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":46,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611747441,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{}}
{"Event":"SparkListenerTaskStart","Stage ID":16,"Stage Attempt ID":0,"Task Info":{"Task ID":29,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611747443,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":16,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":29,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611747443,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611747554,"Failed":false,"Killed":false,"Accumulables":[{"ID":567,"Name":"internal.metrics.executorDeserializeTime","Update":7,"Value":7,"Internal":true,"Count Failed Values":true},{"ID":568,"Name":"internal.metrics.executorDeserializeCpuTime","Update":4675900,"Value":4675900,"Internal":true,"Count Failed Values":true},{"ID":569,"Name":"internal.metrics.executorRunTime","Update":97,"Value":97,"Internal":true,"Count Failed Values":true},{"ID":570,"Name":"internal.metrics.executorCpuTime","Update":2759500,"Value":2759500,"Internal":true,"Count Failed Values":true},{"ID":571,"Name":"internal.metrics.resultSize","Update":1372,"Value":1372,"Internal":true,"Count Failed Values":true},{"ID":574,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":575,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":7,"Executor Deserialize CPU Time":4675900,"Executor Run Time":97,"Executor CPU Time":2759500,"Peak Execution Memory":0,"Result Size":1372,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":16,"Stage Attempt ID":0,"Stage Name":"runJob at PythonRDD.scala:181","Number of Tasks":1,"RDD Info":[{"RDD ID":46,"Name":"PythonRDD","Callsite":"RDD at PythonRDD.scala:53","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\norg.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\norg.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611747441,"Completion Time":1741611747555,"Accumulables":[{"ID":567,"Name":"internal.metrics.executorDeserializeTime","Value":7,"Internal":true,"Count Failed Values":true},{"ID":568,"Name":"internal.metrics.executorDeserializeCpuTime","Value":4675900,"Internal":true,"Count Failed Values":true},{"ID":569,"Name":"internal.metrics.executorRunTime","Value":97,"Internal":true,"Count Failed Values":true},{"ID":570,"Name":"internal.metrics.executorCpuTime","Value":2759500,"Internal":true,"Count Failed Values":true},{"ID":571,"Name":"internal.metrics.resultSize","Value":1372,"Internal":true,"Count Failed Values":true},{"ID":574,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":575,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":16,"Completion Time":1741611747555,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":17,"Submission Time":1741611752532,"Stage Infos":[{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[17],"Properties":{"spark.rdd.scope":"{\"id\":\"70\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/3891185373.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611752533,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"70\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/3891185373.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":17,"Stage Attempt ID":0,"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611752539,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":17,"Stage Attempt ID":0,"Task Info":{"Task ID":31,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611752540,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":17,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":30,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611752539,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752648,"Failed":false,"Killed":false,"Accumulables":[{"ID":602,"Name":"internal.metrics.executorDeserializeTime","Update":32,"Value":32,"Internal":true,"Count Failed Values":true},{"ID":603,"Name":"internal.metrics.executorDeserializeCpuTime","Update":15063000,"Value":15063000,"Internal":true,"Count Failed Values":true},{"ID":604,"Name":"internal.metrics.executorRunTime","Update":66,"Value":66,"Internal":true,"Count Failed Values":true},{"ID":605,"Name":"internal.metrics.executorCpuTime","Update":4285700,"Value":4285700,"Internal":true,"Count Failed Values":true},{"ID":606,"Name":"internal.metrics.resultSize","Update":1383,"Value":1383,"Internal":true,"Count Failed Values":true},{"ID":609,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":610,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":32,"Executor Deserialize CPU Time":15063000,"Executor Run Time":66,"Executor CPU Time":4285700,"Peak Execution Memory":0,"Result Size":1383,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":17,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":31,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611752540,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752658,"Failed":false,"Killed":false,"Accumulables":[{"ID":602,"Name":"internal.metrics.executorDeserializeTime","Update":30,"Value":62,"Internal":true,"Count Failed Values":true},{"ID":603,"Name":"internal.metrics.executorDeserializeCpuTime","Update":17143500,"Value":32206500,"Internal":true,"Count Failed Values":true},{"ID":604,"Name":"internal.metrics.executorRunTime","Update":77,"Value":143,"Internal":true,"Count Failed Values":true},{"ID":605,"Name":"internal.metrics.executorCpuTime","Update":6133600,"Value":10419300,"Internal":true,"Count Failed Values":true},{"ID":606,"Name":"internal.metrics.resultSize","Update":1383,"Value":2766,"Internal":true,"Count Failed Values":true},{"ID":609,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":610,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":30,"Executor Deserialize CPU Time":17143500,"Executor Run Time":77,"Executor CPU Time":6133600,"Peak Execution Memory":0,"Result Size":1383,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":17,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611752533,"Completion Time":1741611752659,"Accumulables":[{"ID":602,"Name":"internal.metrics.executorDeserializeTime","Value":62,"Internal":true,"Count Failed Values":true},{"ID":603,"Name":"internal.metrics.executorDeserializeCpuTime","Value":32206500,"Internal":true,"Count Failed Values":true},{"ID":604,"Name":"internal.metrics.executorRunTime","Value":143,"Internal":true,"Count Failed Values":true},{"ID":605,"Name":"internal.metrics.executorCpuTime","Value":10419300,"Internal":true,"Count Failed Values":true},{"ID":606,"Name":"internal.metrics.resultSize","Value":2766,"Internal":true,"Count Failed Values":true},{"ID":609,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":610,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":17,"Completion Time":1741611752659,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":18,"Submission Time":1741611752668,"Stage Infos":[{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[18],"Properties":{"spark.rdd.scope":"{\"id\":\"72\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/3891185373.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611752669,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"72\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/3891185373.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":18,"Stage Attempt ID":0,"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611752672,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":18,"Stage Attempt ID":0,"Task Info":{"Task ID":33,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611752672,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":18,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":33,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611752672,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752686,"Failed":false,"Killed":false,"Accumulables":[{"ID":637,"Name":"internal.metrics.executorDeserializeTime","Update":8,"Value":8,"Internal":true,"Count Failed Values":true},{"ID":638,"Name":"internal.metrics.executorDeserializeCpuTime","Update":4378700,"Value":4378700,"Internal":true,"Count Failed Values":true},{"ID":640,"Name":"internal.metrics.executorCpuTime","Update":426800,"Value":426800,"Internal":true,"Count Failed Values":true},{"ID":641,"Name":"internal.metrics.resultSize","Update":879,"Value":879,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":8,"Executor Deserialize CPU Time":4378700,"Executor Run Time":0,"Executor CPU Time":426800,"Peak Execution Memory":0,"Result Size":879,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":18,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":32,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611752672,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752696,"Failed":false,"Killed":false,"Accumulables":[{"ID":637,"Name":"internal.metrics.executorDeserializeTime","Update":9,"Value":17,"Internal":true,"Count Failed Values":true},{"ID":638,"Name":"internal.metrics.executorDeserializeCpuTime","Update":5142000,"Value":9520700,"Internal":true,"Count Failed Values":true},{"ID":640,"Name":"internal.metrics.executorCpuTime","Update":633500,"Value":1060300,"Internal":true,"Count Failed Values":true},{"ID":641,"Name":"internal.metrics.resultSize","Update":972,"Value":1851,"Internal":true,"Count Failed Values":true},{"ID":642,"Name":"internal.metrics.jvmGCTime","Update":8,"Value":8,"Internal":true,"Count Failed Values":true},{"ID":643,"Name":"internal.metrics.resultSerializationTime","Update":8,"Value":8,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":9,"Executor Deserialize CPU Time":5142000,"Executor Run Time":0,"Executor CPU Time":633500,"Peak Execution Memory":0,"Result Size":972,"JVM GC Time":8,"Result Serialization Time":8,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":18,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611752669,"Completion Time":1741611752697,"Accumulables":[{"ID":637,"Name":"internal.metrics.executorDeserializeTime","Value":17,"Internal":true,"Count Failed Values":true},{"ID":638,"Name":"internal.metrics.executorDeserializeCpuTime","Value":9520700,"Internal":true,"Count Failed Values":true},{"ID":640,"Name":"internal.metrics.executorCpuTime","Value":1060300,"Internal":true,"Count Failed Values":true},{"ID":641,"Name":"internal.metrics.resultSize","Value":1851,"Internal":true,"Count Failed Values":true},{"ID":642,"Name":"internal.metrics.jvmGCTime","Value":8,"Internal":true,"Count Failed Values":true},{"ID":643,"Name":"internal.metrics.resultSerializationTime","Value":8,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":18,"Completion Time":1741611752697,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":19,"Submission Time":1741611752708,"Stage Infos":[{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:3","Number of Tasks":2,"RDD Info":[{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[19],"Properties":{"spark.rdd.scope":"{\"id\":\"74\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/3891185373.py:3","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:3","Number of Tasks":2,"RDD Info":[{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611752708,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"74\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/3891185373.py:3","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611752712,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":35,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611752712,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":674,"Update":"13","Internal":false,"Count Failed Values":true},{"ID":676,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":34,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611752712,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752733,"Failed":true,"Killed":false,"Accumulables":[{"ID":674,"Name":"internal.metrics.executorRunTime","Update":13,"Value":13,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":13,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":36,"Index":0,"Attempt":1,"Partition ID":0,"Launch Time":1741611752734,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":674,"Update":"13","Internal":false,"Count Failed Values":true},{"ID":676,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":35,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611752712,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752736,"Failed":true,"Killed":false,"Accumulables":[{"ID":674,"Name":"internal.metrics.executorRunTime","Update":13,"Value":26,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":13,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":37,"Index":1,"Attempt":1,"Partition ID":1,"Launch Time":1741611752737,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":674,"Update":"3","Internal":false,"Count Failed Values":true},{"ID":676,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":36,"Index":0,"Attempt":1,"Partition ID":0,"Launch Time":1741611752734,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752746,"Failed":true,"Killed":false,"Accumulables":[{"ID":674,"Name":"internal.metrics.executorRunTime","Update":3,"Value":29,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":3,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":38,"Index":0,"Attempt":2,"Partition ID":0,"Launch Time":1741611752747,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":674,"Update":"3","Internal":false,"Count Failed Values":true},{"ID":676,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":37,"Index":1,"Attempt":1,"Partition ID":1,"Launch Time":1741611752737,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752748,"Failed":true,"Killed":false,"Accumulables":[{"ID":674,"Name":"internal.metrics.executorRunTime","Update":3,"Value":32,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":3,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":39,"Index":1,"Attempt":2,"Partition ID":1,"Launch Time":1741611752749,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":674,"Update":"4","Internal":false,"Count Failed Values":true},{"ID":676,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":38,"Index":0,"Attempt":2,"Partition ID":0,"Launch Time":1741611752747,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752760,"Failed":true,"Killed":false,"Accumulables":[{"ID":674,"Name":"internal.metrics.executorRunTime","Update":4,"Value":36,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":4,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":40,"Index":0,"Attempt":3,"Partition ID":0,"Launch Time":1741611752761,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":674,"Update":"7","Internal":false,"Count Failed Values":true},{"ID":676,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":39,"Index":1,"Attempt":2,"Partition ID":1,"Launch Time":1741611752749,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752765,"Failed":true,"Killed":false,"Accumulables":[{"ID":674,"Name":"internal.metrics.executorRunTime","Update":7,"Value":43,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":7,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":19,"Stage Attempt ID":0,"Task Info":{"Task ID":41,"Index":1,"Attempt":3,"Partition ID":1,"Launch Time":1741611752766,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":674,"Update":"4","Internal":false,"Count Failed Values":true},{"ID":676,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":40,"Index":0,"Attempt":3,"Partition ID":0,"Launch Time":1741611752761,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752777,"Failed":true,"Killed":false,"Accumulables":[{"ID":674,"Name":"internal.metrics.executorRunTime","Update":4,"Value":47,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":4,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":19,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/3891185373.py:3","Number of Tasks":2,"RDD Info":[{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611752708,"Completion Time":1741611752784,"Failure Reason":"Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 40) (172.21.0.3 executor 0): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:","Accumulables":[{"ID":674,"Name":"internal.metrics.executorRunTime","Value":47,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":19,"Completion Time":1741611752784,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job aborted due to stage failure: Task 0 in stage 19.0 failed 4 times, most recent failure: Lost task 0.3 in stage 19.0 (TID 40) (172.21.0.3 executor 0): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":2856},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$abortStage$2","File Name":"DAGScheduler.scala","Line Number":2792},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$abortStage$2$adapted","File Name":"DAGScheduler.scala","Line Number":2791},{"Declaring Class":"scala.collection.mutable.ResizableArray","Method Name":"foreach","File Name":"ResizableArray.scala","Line Number":62},{"Declaring Class":"scala.collection.mutable.ResizableArray","Method Name":"foreach$","File Name":"ResizableArray.scala","Line Number":55},{"Declaring Class":"scala.collection.mutable.ArrayBuffer","Method Name":"foreach","File Name":"ArrayBuffer.scala","Line Number":49},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"abortStage","File Name":"DAGScheduler.scala","Line Number":2791},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$handleTaskSetFailed$1","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$handleTaskSetFailed$1$adapted","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"scala.Option","Method Name":"foreach","File Name":"Option.scala","Line Number":407},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleTaskSetFailed","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":3060},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":2994},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":2983},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":49},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"runJob","File Name":"DAGScheduler.scala","Line Number":989},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2393},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2414},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2433},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2458},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"$anonfun$collect$1","File Name":"RDD.scala","Line Number":1049},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":151},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":112},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":410},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"collect","File Name":"RDD.scala","Line Number":1048},{"Declaring Class":"org.apache.spark.api.python.PythonRDD$","Method Name":"collectAndServe","File Name":"PythonRDD.scala","Line Number":195},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"collectAndServe","File Name":"PythonRDD.scala","Line Number":-1},{"Declaring Class":"jdk.internal.reflect.NativeMethodAccessorImpl","Method Name":"invoke0","File Name":"NativeMethodAccessorImpl.java","Line Number":-2},{"Declaring Class":"jdk.internal.reflect.NativeMethodAccessorImpl","Method Name":"invoke","File Name":"NativeMethodAccessorImpl.java","Line Number":77},{"Declaring Class":"jdk.internal.reflect.DelegatingMethodAccessorImpl","Method Name":"invoke","File Name":"DelegatingMethodAccessorImpl.java","Line Number":43},{"Declaring Class":"java.lang.reflect.Method","Method Name":"invoke","File Name":"Method.java","Line Number":569},{"Declaring Class":"py4j.reflection.MethodInvoker","Method Name":"invoke","File Name":"MethodInvoker.java","Line Number":244},{"Declaring Class":"py4j.reflection.ReflectionEngine","Method Name":"invoke","File Name":"ReflectionEngine.java","Line Number":374},{"Declaring Class":"py4j.Gateway","Method Name":"invoke","File Name":"Gateway.java","Line Number":282},{"Declaring Class":"py4j.commands.AbstractCommand","Method Name":"invokeMethod","File Name":"AbstractCommand.java","Line Number":132},{"Declaring Class":"py4j.commands.CallCommand","Method Name":"execute","File Name":"CallCommand.java","Line Number":79},{"Declaring Class":"py4j.ClientServerConnection","Method Name":"waitForCommands","File Name":"ClientServerConnection.java","Line Number":182},{"Declaring Class":"py4j.ClientServerConnection","Method Name":"run","File Name":"ClientServerConnection.java","Line Number":106},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}]}}}
{"Event":"SparkListenerTaskEnd","Stage ID":19,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":674,"Update":"11","Internal":false,"Count Failed Values":true},{"ID":676,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":41,"Index":1,"Attempt":3,"Partition ID":1,"Launch Time":1741611752766,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611752793,"Failed":true,"Killed":false,"Accumulables":[]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":11,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerJobStart","Job ID":20,"Submission Time":1741611836246,"Stage Infos":[{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[20],"Properties":{"spark.rdd.scope":"{\"id\":\"75\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611836246,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"75\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":20,"Stage Attempt ID":0,"Task Info":{"Task ID":42,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611836249,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":20,"Stage Attempt ID":0,"Task Info":{"Task ID":43,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611836249,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":20,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":42,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611836249,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611836357,"Failed":false,"Killed":false,"Accumulables":[{"ID":707,"Name":"internal.metrics.executorDeserializeTime","Update":9,"Value":9,"Internal":true,"Count Failed Values":true},{"ID":708,"Name":"internal.metrics.executorDeserializeCpuTime","Update":4989500,"Value":4989500,"Internal":true,"Count Failed Values":true},{"ID":709,"Name":"internal.metrics.executorRunTime","Update":92,"Value":92,"Internal":true,"Count Failed Values":true},{"ID":710,"Name":"internal.metrics.executorCpuTime","Update":3082400,"Value":3082400,"Internal":true,"Count Failed Values":true},{"ID":711,"Name":"internal.metrics.resultSize","Update":1383,"Value":1383,"Internal":true,"Count Failed Values":true},{"ID":714,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":715,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":9,"Executor Deserialize CPU Time":4989500,"Executor Run Time":92,"Executor CPU Time":3082400,"Peak Execution Memory":0,"Result Size":1383,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":20,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":43,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611836249,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611836357,"Failed":false,"Killed":false,"Accumulables":[{"ID":707,"Name":"internal.metrics.executorDeserializeTime","Update":8,"Value":17,"Internal":true,"Count Failed Values":true},{"ID":708,"Name":"internal.metrics.executorDeserializeCpuTime","Update":5237300,"Value":10226800,"Internal":true,"Count Failed Values":true},{"ID":709,"Name":"internal.metrics.executorRunTime","Update":84,"Value":176,"Internal":true,"Count Failed Values":true},{"ID":710,"Name":"internal.metrics.executorCpuTime","Update":3009800,"Value":6092200,"Internal":true,"Count Failed Values":true},{"ID":711,"Name":"internal.metrics.resultSize","Update":1469,"Value":2852,"Internal":true,"Count Failed Values":true},{"ID":712,"Name":"internal.metrics.jvmGCTime","Update":9,"Value":9,"Internal":true,"Count Failed Values":true},{"ID":713,"Name":"internal.metrics.resultSerializationTime","Update":10,"Value":10,"Internal":true,"Count Failed Values":true},{"ID":714,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":715,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":8,"Executor Deserialize CPU Time":5237300,"Executor Run Time":84,"Executor CPU Time":3009800,"Peak Execution Memory":0,"Result Size":1469,"JVM GC Time":9,"Result Serialization Time":10,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":20,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":21,"Name":"PythonRDD","Callsite":"collect at /tmp/ipykernel_21/4281115291.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611836246,"Completion Time":1741611836358,"Accumulables":[{"ID":707,"Name":"internal.metrics.executorDeserializeTime","Value":17,"Internal":true,"Count Failed Values":true},{"ID":708,"Name":"internal.metrics.executorDeserializeCpuTime","Value":10226800,"Internal":true,"Count Failed Values":true},{"ID":709,"Name":"internal.metrics.executorRunTime","Value":176,"Internal":true,"Count Failed Values":true},{"ID":710,"Name":"internal.metrics.executorCpuTime","Value":6092200,"Internal":true,"Count Failed Values":true},{"ID":711,"Name":"internal.metrics.resultSize","Value":2852,"Internal":true,"Count Failed Values":true},{"ID":712,"Name":"internal.metrics.jvmGCTime","Value":9,"Internal":true,"Count Failed Values":true},{"ID":713,"Name":"internal.metrics.resultSerializationTime","Value":10,"Internal":true,"Count Failed Values":true},{"ID":714,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":715,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":20,"Completion Time":1741611836358,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":21,"Submission Time":1741611836371,"Stage Infos":[{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[21],"Properties":{"spark.rdd.scope":"{\"id\":\"77\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611836371,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"77\",\"name\":\"collect\"}","callSite.short":"collect at /tmp/ipykernel_21/4281115291.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":21,"Stage Attempt ID":0,"Task Info":{"Task ID":44,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611836376,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":21,"Stage Attempt ID":0,"Task Info":{"Task ID":45,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611836376,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":21,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":44,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611836376,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611836395,"Failed":false,"Killed":false,"Accumulables":[{"ID":742,"Name":"internal.metrics.executorDeserializeTime","Update":12,"Value":12,"Internal":true,"Count Failed Values":true},{"ID":743,"Name":"internal.metrics.executorDeserializeCpuTime","Update":6587700,"Value":6587700,"Internal":true,"Count Failed Values":true},{"ID":745,"Name":"internal.metrics.executorCpuTime","Update":704100,"Value":704100,"Internal":true,"Count Failed Values":true},{"ID":746,"Name":"internal.metrics.resultSize","Update":886,"Value":886,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":12,"Executor Deserialize CPU Time":6587700,"Executor Run Time":0,"Executor CPU Time":704100,"Peak Execution Memory":0,"Result Size":886,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":21,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":45,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611836376,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611836397,"Failed":false,"Killed":false,"Accumulables":[{"ID":742,"Name":"internal.metrics.executorDeserializeTime","Update":12,"Value":24,"Internal":true,"Count Failed Values":true},{"ID":743,"Name":"internal.metrics.executorDeserializeCpuTime","Update":7511600,"Value":14099300,"Internal":true,"Count Failed Values":true},{"ID":745,"Name":"internal.metrics.executorCpuTime","Update":871500,"Value":1575600,"Internal":true,"Count Failed Values":true},{"ID":746,"Name":"internal.metrics.resultSize","Update":879,"Value":1765,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":12,"Executor Deserialize CPU Time":7511600,"Executor Run Time":0,"Executor CPU Time":871500,"Peak Execution Memory":0,"Result Size":879,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":21,"Stage Attempt ID":0,"Stage Name":"collect at /tmp/ipykernel_21/4281115291.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611836371,"Completion Time":1741611836398,"Accumulables":[{"ID":742,"Name":"internal.metrics.executorDeserializeTime","Value":24,"Internal":true,"Count Failed Values":true},{"ID":743,"Name":"internal.metrics.executorDeserializeCpuTime","Value":14099300,"Internal":true,"Count Failed Values":true},{"ID":745,"Name":"internal.metrics.executorCpuTime","Value":1575600,"Internal":true,"Count Failed Values":true},{"ID":746,"Name":"internal.metrics.resultSize","Value":1765,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":21,"Completion Time":1741611836398,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":22,"Submission Time":1741611843420,"Stage Infos":[{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":47,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[22],"Properties":{"spark.rdd.scope":"{\"id\":\"79\",\"name\":\"collect\"}","callSite.short":"count at /tmp/ipykernel_21/4142313113.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":47,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611843420,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"79\",\"name\":\"collect\"}","callSite.short":"count at /tmp/ipykernel_21/4142313113.py:1","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":22,"Stage Attempt ID":0,"Task Info":{"Task ID":46,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611843425,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":22,"Stage Attempt ID":0,"Task Info":{"Task ID":47,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611843425,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":22,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":46,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611843425,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611843546,"Failed":false,"Killed":false,"Accumulables":[{"ID":777,"Name":"internal.metrics.executorDeserializeTime","Update":15,"Value":15,"Internal":true,"Count Failed Values":true},{"ID":778,"Name":"internal.metrics.executorDeserializeCpuTime","Update":9936200,"Value":9936200,"Internal":true,"Count Failed Values":true},{"ID":779,"Name":"internal.metrics.executorRunTime","Update":98,"Value":98,"Internal":true,"Count Failed Values":true},{"ID":780,"Name":"internal.metrics.executorCpuTime","Update":4994800,"Value":4994800,"Internal":true,"Count Failed Values":true},{"ID":781,"Name":"internal.metrics.resultSize","Update":1323,"Value":1323,"Internal":true,"Count Failed Values":true},{"ID":784,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":785,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":15,"Executor Deserialize CPU Time":9936200,"Executor Run Time":98,"Executor CPU Time":4994800,"Peak Execution Memory":0,"Result Size":1323,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":22,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":47,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611843425,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611843547,"Failed":false,"Killed":false,"Accumulables":[{"ID":777,"Name":"internal.metrics.executorDeserializeTime","Update":13,"Value":28,"Internal":true,"Count Failed Values":true},{"ID":778,"Name":"internal.metrics.executorDeserializeCpuTime","Update":9605300,"Value":19541500,"Internal":true,"Count Failed Values":true},{"ID":779,"Name":"internal.metrics.executorRunTime","Update":97,"Value":195,"Internal":true,"Count Failed Values":true},{"ID":780,"Name":"internal.metrics.executorCpuTime","Update":3395800,"Value":8390600,"Internal":true,"Count Failed Values":true},{"ID":781,"Name":"internal.metrics.resultSize","Update":1323,"Value":2646,"Internal":true,"Count Failed Values":true},{"ID":784,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":785,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":13,"Executor Deserialize CPU Time":9605300,"Executor Run Time":97,"Executor CPU Time":3395800,"Peak Execution Memory":0,"Result Size":1323,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":22,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:1","Number of Tasks":2,"RDD Info":[{"RDD ID":47,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:1","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":0,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"0\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611843420,"Completion Time":1741611843549,"Accumulables":[{"ID":777,"Name":"internal.metrics.executorDeserializeTime","Value":28,"Internal":true,"Count Failed Values":true},{"ID":778,"Name":"internal.metrics.executorDeserializeCpuTime","Value":19541500,"Internal":true,"Count Failed Values":true},{"ID":779,"Name":"internal.metrics.executorRunTime","Value":195,"Internal":true,"Count Failed Values":true},{"ID":780,"Name":"internal.metrics.executorCpuTime","Value":8390600,"Internal":true,"Count Failed Values":true},{"ID":781,"Name":"internal.metrics.resultSize","Value":2646,"Internal":true,"Count Failed Values":true},{"ID":784,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":785,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":22,"Completion Time":1741611843550,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":23,"Submission Time":1741611843575,"Stage Infos":[{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":48,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:2","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[23],"Properties":{"spark.rdd.scope":"{\"id\":\"82\",\"name\":\"collect\"}","callSite.short":"count at /tmp/ipykernel_21/4142313113.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":48,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:2","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611843576,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"82\",\"name\":\"collect\"}","callSite.short":"count at /tmp/ipykernel_21/4142313113.py:2","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":23,"Stage Attempt ID":0,"Task Info":{"Task ID":48,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611843583,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":23,"Stage Attempt ID":0,"Task Info":{"Task ID":49,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611843583,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":23,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":49,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611843583,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611843829,"Failed":false,"Killed":false,"Accumulables":[{"ID":812,"Name":"internal.metrics.executorDeserializeTime","Update":16,"Value":16,"Internal":true,"Count Failed Values":true},{"ID":813,"Name":"internal.metrics.executorDeserializeCpuTime","Update":7922800,"Value":7922800,"Internal":true,"Count Failed Values":true},{"ID":814,"Name":"internal.metrics.executorRunTime","Update":212,"Value":212,"Internal":true,"Count Failed Values":true},{"ID":815,"Name":"internal.metrics.executorCpuTime","Update":6126700,"Value":6126700,"Internal":true,"Count Failed Values":true},{"ID":816,"Name":"internal.metrics.resultSize","Update":1366,"Value":1366,"Internal":true,"Count Failed Values":true},{"ID":818,"Name":"internal.metrics.resultSerializationTime","Update":1,"Value":1,"Internal":true,"Count Failed Values":true},{"ID":819,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":820,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":16,"Executor Deserialize CPU Time":7922800,"Executor Run Time":212,"Executor CPU Time":6126700,"Peak Execution Memory":0,"Result Size":1366,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":23,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":48,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611843583,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611843850,"Failed":false,"Killed":false,"Accumulables":[{"ID":812,"Name":"internal.metrics.executorDeserializeTime","Update":19,"Value":35,"Internal":true,"Count Failed Values":true},{"ID":813,"Name":"internal.metrics.executorDeserializeCpuTime","Update":11445000,"Value":19367800,"Internal":true,"Count Failed Values":true},{"ID":814,"Name":"internal.metrics.executorRunTime","Update":232,"Value":444,"Internal":true,"Count Failed Values":true},{"ID":815,"Name":"internal.metrics.executorCpuTime","Update":6064100,"Value":12190800,"Internal":true,"Count Failed Values":true},{"ID":816,"Name":"internal.metrics.resultSize","Update":1366,"Value":2732,"Internal":true,"Count Failed Values":true},{"ID":818,"Name":"internal.metrics.resultSerializationTime","Update":1,"Value":2,"Internal":true,"Count Failed Values":true},{"ID":819,"Name":"internal.metrics.memoryBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true},{"ID":820,"Name":"internal.metrics.diskBytesSpilled","Update":0,"Value":0,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":19,"Executor Deserialize CPU Time":11445000,"Executor Run Time":232,"Executor CPU Time":6064100,"Peak Execution Memory":0,"Result Size":1366,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":23,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:2","Number of Tasks":2,"RDD Info":[{"RDD ID":48,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:2","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":1,"Name":"ParallelCollectionRDD","Scope":"{\"id\":\"1\",\"name\":\"parallelize\"}","Callsite":"readRDDFromFile at PythonRDD.scala:289","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611843576,"Completion Time":1741611843853,"Accumulables":[{"ID":812,"Name":"internal.metrics.executorDeserializeTime","Value":35,"Internal":true,"Count Failed Values":true},{"ID":813,"Name":"internal.metrics.executorDeserializeCpuTime","Value":19367800,"Internal":true,"Count Failed Values":true},{"ID":814,"Name":"internal.metrics.executorRunTime","Value":444,"Internal":true,"Count Failed Values":true},{"ID":815,"Name":"internal.metrics.executorCpuTime","Value":12190800,"Internal":true,"Count Failed Values":true},{"ID":816,"Name":"internal.metrics.resultSize","Value":2732,"Internal":true,"Count Failed Values":true},{"ID":818,"Name":"internal.metrics.resultSerializationTime","Value":2,"Internal":true,"Count Failed Values":true},{"ID":819,"Name":"internal.metrics.memoryBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true},{"ID":820,"Name":"internal.metrics.diskBytesSpilled","Value":0,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":23,"Completion Time":1741611843854,"Job Result":{"Result":"JobSucceeded"}}
{"Event":"SparkListenerJobStart","Job ID":24,"Submission Time":1741611843921,"Stage Infos":[{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:3","Number of Tasks":2,"RDD Info":[{"RDD ID":49,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:3","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}],"Stage IDs":[24],"Properties":{"spark.rdd.scope":"{\"id\":\"85\",\"name\":\"collect\"}","callSite.short":"count at /tmp/ipykernel_21/4142313113.py:3","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:3","Number of Tasks":2,"RDD Info":[{"RDD ID":49,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:3","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611843922,"Accumulables":[],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0},"Properties":{"spark.rdd.scope":"{\"id\":\"85\",\"name\":\"collect\"}","callSite.short":"count at /tmp/ipykernel_21/4142313113.py:3","spark.rdd.scope.noOverride":"true"}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":50,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611843932,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":51,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611843934,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":849,"Update":"38","Internal":false,"Count Failed Values":true},{"ID":851,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":50,"Index":0,"Attempt":0,"Partition ID":0,"Launch Time":1741611843932,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611843992,"Failed":true,"Killed":false,"Accumulables":[{"ID":849,"Name":"internal.metrics.executorRunTime","Update":38,"Value":38,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":38,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":52,"Index":0,"Attempt":1,"Partition ID":0,"Launch Time":1741611843995,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":849,"Update":"46","Internal":false,"Count Failed Values":true},{"ID":851,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":51,"Index":1,"Attempt":0,"Partition ID":1,"Launch Time":1741611843934,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611844000,"Failed":true,"Killed":false,"Accumulables":[{"ID":849,"Name":"internal.metrics.executorRunTime","Update":46,"Value":84,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":46,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":53,"Index":1,"Attempt":1,"Partition ID":1,"Launch Time":1741611844002,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":849,"Update":"8","Internal":false,"Count Failed Values":true},{"ID":851,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":53,"Index":1,"Attempt":1,"Partition ID":1,"Launch Time":1741611844002,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611844032,"Failed":true,"Killed":false,"Accumulables":[{"ID":849,"Name":"internal.metrics.executorRunTime","Update":8,"Value":92,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":8,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":54,"Index":1,"Attempt":2,"Partition ID":1,"Launch Time":1741611844035,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":849,"Update":"15","Internal":false,"Count Failed Values":true},{"ID":851,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":52,"Index":0,"Attempt":1,"Partition ID":0,"Launch Time":1741611843995,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611844038,"Failed":true,"Killed":false,"Accumulables":[{"ID":849,"Name":"internal.metrics.executorRunTime","Update":15,"Value":107,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":15,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":55,"Index":0,"Attempt":2,"Partition ID":0,"Launch Time":1741611844040,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":849,"Update":"15","Internal":false,"Count Failed Values":true},{"ID":851,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":54,"Index":1,"Attempt":2,"Partition ID":1,"Launch Time":1741611844035,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611844076,"Failed":true,"Killed":false,"Accumulables":[{"ID":849,"Name":"internal.metrics.executorRunTime","Update":15,"Value":122,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":15,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":56,"Index":1,"Attempt":3,"Partition ID":1,"Launch Time":1741611844079,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":849,"Update":"15","Internal":false,"Count Failed Values":true},{"ID":851,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":55,"Index":0,"Attempt":2,"Partition ID":0,"Launch Time":1741611844040,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611844083,"Failed":true,"Killed":false,"Accumulables":[{"ID":849,"Name":"internal.metrics.executorRunTime","Update":15,"Value":137,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":15,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":24,"Stage Attempt ID":0,"Task Info":{"Task ID":57,"Index":0,"Attempt":3,"Partition ID":0,"Launch Time":1741611844085,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Killed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":849,"Update":"16","Internal":false,"Count Failed Values":true},{"ID":851,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":57,"Index":0,"Attempt":3,"Partition ID":0,"Launch Time":1741611844085,"Executor ID":"0","Host":"172.21.0.3","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611844121,"Failed":true,"Killed":false,"Accumulables":[{"ID":849,"Name":"internal.metrics.executorRunTime","Update":16,"Value":153,"Internal":true,"Count Failed Values":true}]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":16,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":24,"Stage Attempt ID":0,"Stage Name":"count at /tmp/ipykernel_21/4142313113.py:3","Number of Tasks":2,"RDD Info":[{"RDD ID":49,"Name":"PythonRDD","Callsite":"count at /tmp/ipykernel_21/4142313113.py:3","Parent IDs":[32],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":32,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[31],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0},{"RDD ID":31,"Name":"/home/jovyan/work/data/flight-data/csv/2015-summary.csv","Scope":"{\"id\":\"44\",\"name\":\"textFile\"}","Callsite":"textFile at NativeMethodAccessorImpl.java:0","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use Off Heap":false,"Deserialized":false,"Replication":1},"Barrier":false,"DeterministicLevel":"DETERMINATE","Number of Partitions":2,"Number of Cached Partitions":0,"Memory Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\norg.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\norg.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.base/java.lang.reflect.Method.invoke(Method.java:569)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:282)\npy4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\npy4j.commands.CallCommand.execute(CallCommand.java:79)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:840)","Submission Time":1741611843922,"Completion Time":1741611844127,"Failure Reason":"Job aborted due to stage failure: Task 0 in stage 24.0 failed 4 times, most recent failure: Lost task 0.3 in stage 24.0 (TID 57) (172.21.0.3 executor 0): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:","Accumulables":[{"ID":849,"Name":"internal.metrics.executorRunTime","Value":153,"Internal":true,"Count Failed Values":true}],"Resource Profile Id":0,"Shuffle Push Enabled":false,"Shuffle Push Mergers Count":0}}
{"Event":"SparkListenerJobEnd","Job ID":24,"Completion Time":1741611844128,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job aborted due to stage failure: Task 0 in stage 24.0 failed 4 times, most recent failure: Lost task 0.3 in stage 24.0 (TID 57) (172.21.0.3 executor 0): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":2856},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$abortStage$2","File Name":"DAGScheduler.scala","Line Number":2792},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$abortStage$2$adapted","File Name":"DAGScheduler.scala","Line Number":2791},{"Declaring Class":"scala.collection.mutable.ResizableArray","Method Name":"foreach","File Name":"ResizableArray.scala","Line Number":62},{"Declaring Class":"scala.collection.mutable.ResizableArray","Method Name":"foreach$","File Name":"ResizableArray.scala","Line Number":55},{"Declaring Class":"scala.collection.mutable.ArrayBuffer","Method Name":"foreach","File Name":"ArrayBuffer.scala","Line Number":49},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"abortStage","File Name":"DAGScheduler.scala","Line Number":2791},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$handleTaskSetFailed$1","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"$anonfun$handleTaskSetFailed$1$adapted","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"scala.Option","Method Name":"foreach","File Name":"Option.scala","Line Number":407},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleTaskSetFailed","File Name":"DAGScheduler.scala","Line Number":1247},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":3060},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":2994},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":2983},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":49},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"runJob","File Name":"DAGScheduler.scala","Line Number":989},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2393},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2414},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2433},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":2458},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"$anonfun$collect$1","File Name":"RDD.scala","Line Number":1049},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":151},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":112},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":410},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"collect","File Name":"RDD.scala","Line Number":1048},{"Declaring Class":"org.apache.spark.api.python.PythonRDD$","Method Name":"collectAndServe","File Name":"PythonRDD.scala","Line Number":195},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"collectAndServe","File Name":"PythonRDD.scala","Line Number":-1},{"Declaring Class":"jdk.internal.reflect.NativeMethodAccessorImpl","Method Name":"invoke0","File Name":"NativeMethodAccessorImpl.java","Line Number":-2},{"Declaring Class":"jdk.internal.reflect.NativeMethodAccessorImpl","Method Name":"invoke","File Name":"NativeMethodAccessorImpl.java","Line Number":77},{"Declaring Class":"jdk.internal.reflect.DelegatingMethodAccessorImpl","Method Name":"invoke","File Name":"DelegatingMethodAccessorImpl.java","Line Number":43},{"Declaring Class":"java.lang.reflect.Method","Method Name":"invoke","File Name":"Method.java","Line Number":569},{"Declaring Class":"py4j.reflection.MethodInvoker","Method Name":"invoke","File Name":"MethodInvoker.java","Line Number":244},{"Declaring Class":"py4j.reflection.ReflectionEngine","Method Name":"invoke","File Name":"ReflectionEngine.java","Line Number":374},{"Declaring Class":"py4j.Gateway","Method Name":"invoke","File Name":"Gateway.java","Line Number":282},{"Declaring Class":"py4j.commands.AbstractCommand","Method Name":"invokeMethod","File Name":"AbstractCommand.java","Line Number":132},{"Declaring Class":"py4j.commands.CallCommand","Method Name":"execute","File Name":"CallCommand.java","Line Number":79},{"Declaring Class":"py4j.ClientServerConnection","Method Name":"waitForCommands","File Name":"ClientServerConnection.java","Line Number":182},{"Declaring Class":"py4j.ClientServerConnection","Method Name":"run","File Name":"ClientServerConnection.java","Line Number":106},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}]}}}
{"Event":"SparkListenerTaskEnd","Stage ID":24,"Stage Attempt ID":0,"Task Type":"ResultTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":779},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":1100},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":769},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":462},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":160},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":372},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"lambda$openFileWithOptions$0","File Name":"ChecksumFileSystem.java","Line Number":896},{"Declaring Class":"org.apache.hadoop.util.LambdaUtils","Method Name":"eval","File Name":"LambdaUtils.java","Line Number":52},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"openFileWithOptions","File Name":"ChecksumFileSystem.java","Line Number":894},{"Declaring Class":"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder","Method Name":"build","File Name":"FileSystem.java","Line Number":4768},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":115},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"liftedTree1$1","File Name":"HadoopRDD.scala","Line Number":290},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":289},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":247},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":99},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":52},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.api.python.PythonRDD","Method Name":"compute","File Name":"PythonRDD.scala","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":367},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":331},{"Declaring Class":"org.apache.spark.scheduler.ResultTask","Method Name":"runTask","File Name":"ResultTask.scala","Line Number":93},{"Declaring Class":"org.apache.spark.TaskContext","Method Name":"runTaskWithListeners","File Name":"TaskContext.scala","Line Number":166},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":141},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"$anonfun$run$4","File Name":"Executor.scala","Line Number":620},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally","File Name":"SparkErrorUtils.scala","Line Number":64},{"Declaring Class":"org.apache.spark.util.SparkErrorUtils","Method Name":"tryWithSafeFinally$","File Name":"SparkErrorUtils.scala","Line Number":61},{"Declaring Class":"org.apache.spark.util.Utils$","Method Name":"tryWithSafeFinally","File Name":"Utils.scala","Line Number":94},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":623},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1136},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":635},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":840}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n","Accumulator Updates":[{"ID":849,"Update":"20","Internal":false,"Count Failed Values":true},{"ID":851,"Update":"0","Internal":false,"Count Failed Values":true}]},"Task Info":{"Task ID":56,"Index":1,"Attempt":3,"Partition ID":1,"Launch Time":1741611844079,"Executor ID":"1","Host":"172.21.0.4","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1741611844126,"Failed":true,"Killed":false,"Accumulables":[]},"Task Executor Metrics":{"JVMHeapMemory":0,"JVMOffHeapMemory":0,"OnHeapExecutionMemory":0,"OffHeapExecutionMemory":0,"OnHeapStorageMemory":0,"OffHeapStorageMemory":0,"OnHeapUnifiedMemory":0,"OffHeapUnifiedMemory":0,"DirectPoolMemory":0,"MappedPoolMemory":0,"ProcessTreeJVMVMemory":0,"ProcessTreeJVMRSSMemory":0,"ProcessTreePythonVMemory":0,"ProcessTreePythonRSSMemory":0,"ProcessTreeOtherVMemory":0,"ProcessTreeOtherRSSMemory":0,"MinorGCCount":0,"MinorGCTime":0,"MajorGCCount":0,"MajorGCTime":0,"TotalGCTime":0},"Task Metrics":{"Executor Deserialize Time":0,"Executor Deserialize CPU Time":0,"Executor Run Time":20,"Executor CPU Time":0,"Peak Execution Memory":0,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Read Metrics":{"Remote Blocks Fetched":0,"Local Blocks Fetched":0,"Fetch Wait Time":0,"Remote Bytes Read":0,"Remote Bytes Read To Disk":0,"Local Bytes Read":0,"Total Records Read":0,"Remote Requests Duration":0,"Push Based Shuffle":{"Corrupt Merged Block Chunks":0,"Merged Fetch Fallback Count":0,"Merged Remote Blocks Fetched":0,"Merged Local Blocks Fetched":0,"Merged Remote Chunks Fetched":0,"Merged Local Chunks Fetched":0,"Merged Remote Bytes Read":0,"Merged Local Bytes Read":0,"Merged Remote Requests Duration":0}},"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Bytes Read":0,"Records Read":0},"Output Metrics":{"Bytes Written":0,"Records Written":0},"Updated Blocks":[]}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1741612385021}
