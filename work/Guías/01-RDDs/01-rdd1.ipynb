{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1452f549-7f7b-4f55-bdf3-6cc0a946f6db",
   "metadata": {},
   "source": [
    "# Uso de RDD's en Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb853ffb-7e32-4b9f-91b3-140a4e37b301",
   "metadata": {},
   "source": [
    "## Creación de un RDD \n",
    "Lo primero es obtener el objeto SparkContext. Recomendación personal: Hacerlo a partir del objeto SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c7356c-d909-415a-b237-31554630371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"01-rdd1\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"hdfs:///spark/logs/history\") \\\n",
    "    .config(\"spark.history.fs.logDirectory\", \"hdfs:///spark/logs/history\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version  # Verifica la versión de Spark\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc90f67-aeb0-4d5f-9544-fe1b20a04abf",
   "metadata": {},
   "source": [
    "### A partir de una colección local\n",
    "Para crear un RDD a partir de una colección se usa el método parallelize. A continuación se muestran dos ejemplos: Uno para enteros y otro para strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f52f5a-68e9-4532-a5e2-d0b563d9ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_int = sc.parallelize(range(10))\n",
    "rdd_st = sc.parallelize (\"Big Data aplicado. Curso de especialización de Inteligencia Artificial y Big Data\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1437eba-7e56-4845-9153-ecb89524fb67",
   "metadata": {},
   "source": [
    "También hay algún parámetro opcional, como el número de particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9319d82e-8b9b-41c9-8e75-ea5b123edd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_int_par = sc.parallelize(range(20),4)\n",
    "\n",
    "\n",
    "rdd_st_par = sc.parallelize (\"Big Data aplicado. Curso de especialización de Inteligencia Artificial y Big Data\".split(),5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51873a2d-893b-4852-806f-c8648ff2a497",
   "metadata": {},
   "source": [
    "### A partir de fuentes de datos\n",
    "Para ello tenemos dos opciones:\n",
    "- textfile: Para crear un RDD a partir de un archivo de texto\n",
    "- wholeTextFiles: Para crear un RDD a partir de varios archivos de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc07552-72bd-4926-ab49-d73ff1714414",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_file = sc.textFile(\"hdfs:/user/jovyan/data/flight-data/csv/2015-summary.csv\")\n",
    "#rdd_whole_files = sc.wholeTextFiles(\"/home/jovyan/work/data/flight-data/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6ea00-462b-462c-9d85-5f1189b08c4b",
   "metadata": {},
   "source": [
    "### A partir de DataFrames existentes\n",
    "Una forma muy sencilla de crear RDD's es a partir de un DataFrame o DataSet existente (se verán en próximas sesiones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57f4b27-0707-4b4f-b45b-18c5ed00629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0),\n",
       " Row(id=1),\n",
       " Row(id=2),\n",
       " Row(id=3),\n",
       " Row(id=4),\n",
       " Row(id=5),\n",
       " Row(id=6),\n",
       " Row(id=7),\n",
       " Row(id=8),\n",
       " Row(id=9)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2af1a-819f-499e-8a1f-304f167c7424",
   "metadata": {},
   "source": [
    "Así obtenemos un RDD formado por objetos de tipo *Row*. Para poder manejar estos datos, es necesario convertir estos objetos tipo *Row* al tipo correcto o extraer los valores, como se muestra en el sigiuente ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a61b8ae-7f15-457c-b109-d79282b6bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).toDF(\"id\").rdd.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657e217-f480-4bc0-98af-0aaec91de766",
   "metadata": {},
   "source": [
    "## Acciones\n",
    "A continuación vamos a ver algunas de las acciones más frecuentes:\n",
    "### collect\n",
    "Permite mostrar todos los elementos de un RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aacbacd3-af7b-4f94-85c6-f29ff2806917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "['Big', 'Data', 'aplicado.', 'Curso', 'de', 'especialización', 'de', 'Inteligencia', 'Artificial', 'y', 'Big', 'Data']\n",
      "['DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count', 'United States,Romania,15', 'United States,Croatia,1', 'United States,Ireland,344', 'Egypt,United States,15', 'United States,India,62', 'United States,Singapore,1', 'United States,Grenada,62', 'Costa Rica,United States,588', 'Senegal,United States,40', 'Moldova,United States,1', 'United States,Sint Maarten,325', 'United States,Marshall Islands,39', 'Guyana,United States,64', 'Malta,United States,1', 'Anguilla,United States,41', 'Bolivia,United States,30', 'United States,Paraguay,6', 'Algeria,United States,4', 'Turks and Caicos Islands,United States,230', 'United States,Gibraltar,1', 'Saint Vincent and the Grenadines,United States,1', 'Italy,United States,382', 'United States,Federated States of Micronesia,69', 'United States,Russia,161', 'Pakistan,United States,12', 'United States,Netherlands,660', 'Iceland,United States,181', 'Marshall Islands,United States,42', 'Luxembourg,United States,155', 'Honduras,United States,362', 'The Bahamas,United States,955', 'United States,Senegal,42', 'El Salvador,United States,561', 'Samoa,United States,25', 'United States,Angola,13', 'Switzerland,United States,294', 'United States,Anguilla,38', 'Sint Maarten,United States,325', 'Hong Kong,United States,332', 'Trinidad and Tobago,United States,211', 'Latvia,United States,19', 'United States,Ecuador,300', 'Suriname,United States,1', 'Mexico,United States,7140', 'United States,Cyprus,1', 'Ecuador,United States,268', 'United States,Portugal,134', 'United States,Costa Rica,608', 'United States,Guatemala,318', 'United States,Suriname,34', 'Colombia,United States,873', 'United States,Cape Verde,14', 'United States,Jamaica,712', 'Norway,United States,121', 'United States,Malaysia,3', 'United States,Morocco,19', 'Thailand,United States,3', 'United States,Samoa,25', 'Venezuela,United States,290', 'United States,Palau,31', 'United States,Venezuela,246', 'Panama,United States,510', 'Antigua and Barbuda,United States,126', 'United States,Chile,185', 'Morocco,United States,15', 'United States,Finland,28', 'Azerbaijan,United States,21', 'United States,Greece,23', 'United States,The Bahamas,986', 'New Zealand,United States,111', 'Liberia,United States,2', 'United States,Hong Kong,414', 'Hungary,United States,2', 'United States,China,920', 'United States,Vietnam,2', 'Burkina Faso,United States,1', 'Sweden,United States,118', 'United States,Kuwait,28', 'United States,Dominican Republic,1420', 'United States,Egypt,12', 'Israel,United States,134', 'United States,United States,370002', 'Ethiopia,United States,13', 'United States,Luxembourg,134', 'United States,Poland,33', 'Martinique,United States,44', 'United States,Saint Barthelemy,41', 'Saint Barthelemy,United States,39', 'Barbados,United States,154', 'United States,Turkey,129', 'Djibouti,United States,1', 'United States,Azerbaijan,21', 'United States,Estonia,1', 'Germany,United States,1468', 'United States,South Korea,827', 'United States,El Salvador,508', 'Ireland,United States,335', 'United States,Hungary,3', 'Zambia,United States,1', 'Malaysia,United States,2', 'United States,Ethiopia,12', 'United States,Panama,465', 'United States,Aruba,342', 'United States,Thailand,4', 'United States,Turks and Caicos Islands,236', 'Croatia,United States,2', 'United States,Pakistan,12', 'Cyprus,United States,1', 'United States,Honduras,407', 'Fiji,United States,24', 'Qatar,United States,108', 'Saint Kitts and Nevis,United States,139', 'Kuwait,United States,32', 'Taiwan,United States,266', 'Haiti,United States,226', 'Canada,United States,8399', 'Federated States of Micronesia,United States,69', 'United States,Liberia,2', 'Jamaica,United States,666', 'United States,Malta,2', 'Dominican Republic,United States,1353', 'Japan,United States,1548', 'United States,Lithuania,1', 'Finland,United States,26', 'United States,Guadeloupe,59', 'United States,Ukraine,13', 'United States,France,952', 'United States,Norway,115', 'Aruba,United States,346', 'French Guiana,United States,5', 'United States,Kiribati,35', 'India,United States,61', 'British Virgin Islands,United States,107', 'Brazil,United States,853', 'United States,Germany,1336', 'United States,New Zealand,74', 'French Polynesia,United States,43', 'United Arab Emirates,United States,320', 'Singapore,United States,3', 'United States,Mexico,7187', 'United States,Sweden,119', 'Netherlands,United States,776', 'United States,Martinique,43', 'United States,United Arab Emirates,313', 'United States,Bulgaria,1', 'Denmark,United States,153', 'China,United States,772', 'United States,Nicaragua,201', 'United States,Philippines,126', 'United States,Georgia,1', 'United States,Belgium,228', 'Cayman Islands,United States,314', 'Argentina,United States,180', 'Peru,United States,279', 'South Africa,United States,36', 'United States,Iceland,202', 'United States,Argentina,141', 'Spain,United States,420', 'Bermuda,United States,183', 'United States,Nigeria,50', 'United States,Austria,63', 'United States,\"Bonaire, Sint Eustatius, and Saba\",59', 'Kiribati,United States,26', 'Saudi Arabia,United States,83', 'Czech Republic,United States,13', 'United States,Israel,127', 'Belgium,United States,259', 'United States,Saint Lucia,136', 'United States,Bahrain,1', 'United States,British Virgin Islands,80', 'Curacao,United States,90', 'Georgia,United States,2', 'United States,Denmark,152', 'United States,Guyana,63', 'Philippines,United States,134', 'Grenada,United States,53', 'Cape Verde,United States,20', \"Cote d'Ivoire,United States,1\", 'Ukraine,United States,14', 'United States,Papua New Guinea,1', 'Russia,United States,176', 'United States,Saudi Arabia,70', 'Guatemala,United States,397', 'Saint Lucia,United States,123', 'Paraguay,United States,60', 'United States,Curacao,83', 'Kosovo,United States,1', 'United States,Taiwan,235', 'Tunisia,United States,3', 'United States,South Africa,40', 'Niger,United States,2', 'Turkey,United States,138', 'United Kingdom,United States,2025', 'Romania,United States,14', 'United States,Greenland,4', 'Papua New Guinea,United States,3', 'United States,Spain,442', 'Iraq,United States,1', 'United States,Italy,438', 'Cuba,United States,466', 'United States,Switzerland,305', 'Dominica,United States,20', 'United States,Japan,1496', 'Portugal,United States,127', 'United States,Brazil,619', 'Bahrain,United States,19', 'United States,Peru,337', 'Indonesia,United States,1', 'United States,Belize,193', 'United States,United Kingdom,1970', 'Belize,United States,188', 'United States,Ghana,20', 'United States,Indonesia,2', 'United States,Fiji,25', 'United States,Canada,8483', 'United States,Antigua and Barbuda,117', 'United States,French Polynesia,40', 'Nicaragua,United States,179', 'United States,Latvia,15', 'United States,Dominica,27', 'United States,Czech Republic,12', 'United States,Australia,258', 'United States,Cook Islands,13', 'Austria,United States,62', 'Jordan,United States,44', 'Palau,United States,30', 'South Korea,United States,1048', 'Angola,United States,15', 'Ghana,United States,18', 'New Caledonia,United States,1', 'Guadeloupe,United States,56', 'France,United States,935', 'Poland,United States,32', 'Nigeria,United States,59', 'United States,Uruguay,13', 'Greenland,United States,2', 'United States,Bermuda,193', 'Chile,United States,174', 'United States,Cuba,478', 'United States,Montenegro,1', 'United States,Colombia,867', 'United States,Barbados,130', 'United States,Qatar,109', 'Australia,United States,329', 'United States,Cayman Islands,310', 'United States,Jordan,44', 'United States,Namibia,1', 'United States,Trinidad and Tobago,217', 'United States,Bolivia,13', 'Cook Islands,United States,13', 'Bulgaria,United States,3', 'United States,Saint Kitts and Nevis,145', 'Uruguay,United States,43', 'United States,Haiti,225', '\"Bonaire, Sint Eustatius, and Saba\",United States,58', 'Greece,United States,30']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print (rdd_int.collect())\n",
    "print (rdd_st.collect())\n",
    "print (rdd_file.collect())\n",
    "#print (rdd_whole_files.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9960c1e-521c-441f-842a-52d94bc5d83c",
   "metadata": {},
   "source": [
    "### take\n",
    "Permite obtener un número determinado de elmentos del RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35d8e043-489e-44e9-89b8-ef317b8a8bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "['Big', 'Data', 'aplicado.']\n"
     ]
    }
   ],
   "source": [
    "print (rdd_int.take(3))\n",
    "print (rdd_st.take(3))\n",
    "#print (rdd_file.take(2))\n",
    "#print (rdd_whole_files.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc451b6e-bbc1-4711-9605-31e208259445",
   "metadata": {},
   "source": [
    "### count\n",
    "Devuelve el número de elementos de un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b91a6ea-3c23-4d6d-b56c-de2df2b85ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 13:04:03 WARN TaskSetManager: Lost task 0.0 in stage 24.0 (TID 50) (172.21.0.4 executor 1): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n",
      "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n",
      "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
      "\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n",
      "\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n",
      "\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n",
      "\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n",
      "\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/03/10 13:04:04 ERROR TaskSetManager: Task 0 in stage 24.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 4 times, most recent failure: Lost task 0.3 in stage 24.0 (TID 57) (172.21.0.3 executor 0): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m (rdd_int.count())\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m (rdd_st.count())\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[43mrdd_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m (rdd_whole_files.count())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py:2316\u001b[39m, in \u001b[36mRDD.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2295\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   2296\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2297\u001b[39m \u001b[33;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[32m   2298\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2314\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   2315\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py:2291\u001b[39m, in \u001b[36mRDD.sum\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRDD[NumberOrArray]\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mNumberOrArray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2271\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2272\u001b[39m \u001b[33;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[32m   2273\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2289\u001b[39m \u001b[33;03m    6.0\u001b[39;00m\n\u001b[32m   2290\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m   2292\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\n\u001b[32m   2293\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py:2044\u001b[39m, in \u001b[36mRDD.fold\u001b[39m\u001b[34m(self, zeroValue, op)\u001b[39m\n\u001b[32m   2039\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[32m   2041\u001b[39m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[32m   2042\u001b[39m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[32m   2043\u001b[39m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2044\u001b[39m vals = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2045\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pyspark/rdd.py:1833\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1832\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1833\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1834\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 4 times, most recent failure: Lost task 0.3 in stage 24.0 (TID 57) (172.21.0.3 executor 0): java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.FileNotFoundException: File file:/home/jovyan/work/data/flight-data/csv/2015-summary.csv does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:290)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:289)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:247)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:99)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "print (rdd_int.count())\n",
    "print (rdd_st.count())\n",
    "print (rdd_file.count())\n",
    "print (rdd_whole_files.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b88e9f12-54a1-42dd-91a3-2cb3ce943265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f05a9273-0312-4b04-934f-82ab4848a71e",
   "metadata": {},
   "source": [
    "### reduce\n",
    "Permite, mediante una función especificada por el programador reducir el RDD a un único valor. Esta función recibe dos parámetros y devuelve un único resultado. Se pueden emplear también funciones *lambda*.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3bc55e2-f783-4615-b609-46932e827918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "especialización\n",
      "\"Bonaire, Sint Eustatius, and Saba\",United States,58\n"
     ]
    }
   ],
   "source": [
    "# Sumamos todos los elementos\n",
    "print(rdd_int.reduce (lambda x,y: x+y))\n",
    "\n",
    "def word_length_reducer(word1,word2):\n",
    "    if (len(word1) > len (word2)):\n",
    "        return word1\n",
    "    else:\n",
    "        return word2\n",
    "\n",
    "\n",
    "print (rdd_st.reduce (word_length_reducer))\n",
    "print ( rdd_file.reduce (word_length_reducer)) # Cambiar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4b816-f313-4982-9aaa-a618f1185218",
   "metadata": {},
   "source": [
    "### first\n",
    "Devuelve el primer elemento de un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c4d0126-cd65-48b2-ab40-129e703e5112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_int.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfecb66f-e1dc-47ed-ae3c-d3eaf63a5e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Big'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144738f8-6fad-4f25-8fbd-3ad7db58d361",
   "metadata": {},
   "source": [
    "### max/min\n",
    "Devuelve el valor máximo/mínimo de un RDD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22a93988-4941-46a2-b57f-3b9f848eb1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_int.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7702912-26b8-4bf9-a4ab-09366318cefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb4294e6-68f6-40aa-8cc3-23cbc168983e",
   "metadata": {},
   "source": [
    "## Transformaciones\n",
    "### map\n",
    "Permite aplicar una función especificada por el programador a cada uno de los elementos del RDD devolviendo un RDD del mismo tamaño que el original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ac0437a-0872-4d73-9e5b-5e75805e8639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_int.map (lambda x: 2*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5b0ddd9-d9b1-4840-b350-0b9b768bda3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B', 'i', 'g'],\n",
       " ['D', 'a', 't', 'a'],\n",
       " ['a', 'p', 'l', 'i', 'c', 'a', 'd', 'o', '.'],\n",
       " ['C', 'u', 'r', 's', 'o'],\n",
       " ['d', 'e'],\n",
       " ['e', 's', 'p', 'e', 'c', 'i', 'a', 'l', 'i', 'z', 'a', 'c', 'i', 'ó', 'n'],\n",
       " ['d', 'e'],\n",
       " ['I', 'n', 't', 'e', 'l', 'i', 'g', 'e', 'n', 'c', 'i', 'a'],\n",
       " ['A', 'r', 't', 'i', 'f', 'i', 'c', 'i', 'a', 'l'],\n",
       " ['y'],\n",
       " ['B', 'i', 'g'],\n",
       " ['D', 'a', 't', 'a']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st.map (lambda x: list(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f53640-c65a-47e5-a6cf-5e379426857f",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "Permite realizar operaciones map que no sean 1:1. Por ejemplo, en el siguiente código aplicamos la misma función que en el caso anterior pero el resultado es muy distinto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b67db13-2bbe-45d7-995c-d4a13a2634ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'i',\n",
       " 'g',\n",
       " 'D',\n",
       " 'a',\n",
       " 't',\n",
       " 'a',\n",
       " 'a',\n",
       " 'p',\n",
       " 'l',\n",
       " 'i',\n",
       " 'c',\n",
       " 'a',\n",
       " 'd',\n",
       " 'o',\n",
       " '.',\n",
       " 'C',\n",
       " 'u',\n",
       " 'r',\n",
       " 's',\n",
       " 'o',\n",
       " 'd',\n",
       " 'e',\n",
       " 'e',\n",
       " 's',\n",
       " 'p',\n",
       " 'e',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " 'l',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 'c',\n",
       " 'i',\n",
       " 'ó',\n",
       " 'n',\n",
       " 'd',\n",
       " 'e',\n",
       " 'I',\n",
       " 'n',\n",
       " 't',\n",
       " 'e',\n",
       " 'l',\n",
       " 'i',\n",
       " 'g',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " 'A',\n",
       " 'r',\n",
       " 't',\n",
       " 'i',\n",
       " 'f',\n",
       " 'i',\n",
       " 'c',\n",
       " 'i',\n",
       " 'a',\n",
       " 'l',\n",
       " 'y',\n",
       " 'B',\n",
       " 'i',\n",
       " 'g',\n",
       " 'D',\n",
       " 'a',\n",
       " 't',\n",
       " 'a']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st.flatMap(lambda x: list(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86060939-674b-4bf1-807d-6e72c7ea8e29",
   "metadata": {},
   "source": [
    "### distinct\n",
    "Elimina los duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3125e293-1fbc-41fe-a6f8-693f807286cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Curso',\n",
       " 'aplicado.',\n",
       " 'especialización',\n",
       " 'Big',\n",
       " 'de',\n",
       " 'y',\n",
       " 'Inteligencia',\n",
       " 'Artificial',\n",
       " 'Data']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st.distinct().collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6013f71-0876-43bd-8c95-6a35979ac584",
   "metadata": {},
   "source": [
    "### filter\n",
    "Permite seleccionar los elementos del RDD que cumplen determinada condición\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a75745d-6218-48e0-91d2-519ca9f8ba20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_int.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e30ea757-ba87-4dd7-9391-1b20318715a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aplicado.', 'Curso', 'especialización', 'Inteligencia', 'Artificial']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st.filter(lambda x: len(x) >= 5).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "424814d0-b3c5-4057-93ec-b59d250170d7",
   "metadata": {},
   "source": [
    "### sortBy\n",
    "Permite reordenar el RDD en función de un criterio que puede ser especificado mediante una función lambda. Si queremos hacer orden inverso tenemos que poner el valor en negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69a0a9a2-57dd-4fda-9b8f-1468d9fb569f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y',\n",
       " 'de',\n",
       " 'de',\n",
       " 'Big',\n",
       " 'Big',\n",
       " 'Data',\n",
       " 'Data',\n",
       " 'Curso',\n",
       " 'aplicado.',\n",
       " 'Artificial',\n",
       " 'Inteligencia',\n",
       " 'especialización']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Orden ascendente\n",
    "rdd_st.sortBy(lambda x: len(x)).collect()\n",
    "\n",
    "# Orden descendente\n",
    "# rdd_st.sortBy(lambda x: len(x)*-1).collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d666b92-f6f4-4478-b33d-8a790a2395ea",
   "metadata": {},
   "source": [
    "### randomsplit\n",
    "Permite dividir un RDD convirtiéndolo en un array de RDD’s en función de un array de pesos especificado por el programador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d29d4d31-9b8f-4165-ab80-22d19278e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 5, 7, 9]\n",
      "[1, 3, 4, 6, 8]\n",
      "['aplicado.', 'de', 'especialización', 'de', 'y', 'Big', 'Data']\n",
      "['Big', 'Data', 'Curso', 'Inteligencia', 'Artificial']\n"
     ]
    }
   ],
   "source": [
    "for rdd in rdd_int.randomSplit([0.4, 0.6]):\n",
    "    print(rdd.collect())\n",
    "\n",
    "for rdd in rdd_st.randomSplit([0.5,0.5]):\n",
    "    print(rdd.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26842be-99d8-40c0-9b61-e162c17e6498",
   "metadata": {},
   "source": [
    "## Otras operaciones\n",
    "### foreachPartition\n",
    "Permite especificar que función aplicar a cada partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfdb906b-348a-413b-aa57-8e675327130f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def f (iterator):\n",
    "    for x in iterator:\n",
    "        print(type(x))\n",
    "        print (x)\n",
    "\n",
    "rdd_int_par.pipe(\"wc -l\").collect()\n",
    "print(rdd_int_par.foreachPartition(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fc9f76-0896-4930-abdd-b3e92f12bcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: []\n",
      "Partition 1: []\n",
      "Partition 2: []\n",
      "Partition 3: [1]\n",
      "Partition 4: []\n",
      "Partition 5: []\n",
      "Partition 6: []\n",
      "Partition 7: [2]\n",
      "Partition 8: []\n",
      "Partition 9: []\n",
      "Partition 10: []\n",
      "Partition 11: [3]\n",
      "Partition 12: []\n",
      "Partition 13: []\n",
      "Partition 14: []\n",
      "Partition 15: [4]\n",
      "Partition 16: []\n",
      "Partition 17: []\n",
      "Partition 18: []\n",
      "Partition 19: [5]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "partitions = rdd.glom().collect()  # glom() devuelve una lista de particiones\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"Partition {i}: {partition}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ca30e-4519-4c7b-81ff-bc2cb61b20d5",
   "metadata": {},
   "source": [
    "### glom\n",
    "La función *glom* convierte cada partición del DataFrame en Arrays. Es una función que puede ser muy útil, pero si los arrays son muy grandes podría causar errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df185e5f-5826-40aa-ac2a-3120d63d4fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Big', 'Data'],\n",
       " ['aplicado.', 'Curso'],\n",
       " ['de', 'especialización'],\n",
       " ['de', 'Inteligencia'],\n",
       " ['Artificial', 'y', 'Big', 'Data']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st_par.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c7004-068f-4858-9090-daa11e138aea",
   "metadata": {},
   "source": [
    "### Almacenar RDD's en archivos\n",
    "Es posible almacenar los RDD's en archivos de texto. Dos métodos:\n",
    "- **saveAsTextFile**: Almacena el RDD en archivos de texto. Hay que especificar la ruta y (opcionalmente) un códec de compresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44a633eb-3f7a-4140-aedf-c8868f7f2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_st.saveAsTextFile(\"/home/jovyan/texto.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc34f4-56d4-417f-9abb-037fa804cf39",
   "metadata": {},
   "source": [
    "- **saveAsPickleFile**: En un entorno *HDFS* una *secuenceFile* es un archivo de texto formado por pares clave-valor binarios. Este método permite escribir un RDD como un *sequenceFile*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1da0cc4a-9c77-4fff-a447-001677ef2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_st.saveAsPickleFile(\"/home/jovyan/secuencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707df96-415e-4c3c-ab37-7eb6b7fb0bd6",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "Permite almacenar estados intermedios para no tener que repetir la secuencia de operaciones desde el principio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58e5f23d-91c6-45ca-9027-5c9a75ba3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir(\"/home/jovyan/checkpoints\")\n",
    "rdd_st.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2953561e-362f-435f-882e-f80e17d6acb1",
   "metadata": {},
   "source": [
    "### MapPartitions\n",
    "El método **pipe** nos permite enviar, de forma similar a las tuberías de un shell, RDD's a la entrada de un comando del SO. A continuación se muestra un ejemplo sencillo con el comando **wc**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d6b1d51-09dd-4286-86ed-ef3fbe89a6b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '2', '2', '2', '4']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_st_par.pipe(\"wc -l\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c2832-6abf-47ab-b6f0-81d9a238c12e",
   "metadata": {},
   "source": [
    "Observando el código anterior destaca el hecho de que Spark ejecuta operaciones a nivel de partición. La función *map* es realmente un alias para ejecutar la función *mapPartitions* a nivel de fila. Este hecho hace posible ejecutar la operación *map* a nivel de fila, empleando la función *mapPartitions* e iteradores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62054b49-322a-4623-86eb-6c5925d14a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Big', 'Data'],\n",
       " ['aplicado.', 'Curso'],\n",
       " ['de', 'especialización'],\n",
       " ['de', 'Inteligencia'],\n",
       " ['Artificial', 'y', 'Big', 'Data']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosa = rdd_st_par.mapPartitions(lambda x: [list(x)])\n",
    "print(type(cosa))\n",
    "cosa.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e633b0-e1ca-43ab-b113-50bac96ea094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
