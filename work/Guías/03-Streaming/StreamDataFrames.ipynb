{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056de036-70b4-4300-8999-1b3750c83f2d",
   "metadata": {},
   "source": [
    "# Uso de DataFrames en Structured Streaming\n",
    "Structured Streaming permite realizar operaciones com Stream DataFrames/DataSets muy similares a aquellas permitidas para los DataFrames/DataSets estáticos, con algunas restricciones. En general, si queremos realizar procesamiento en streaming usando DataFrames/DataSets el procedimiento es el siguiente\n",
    "  1. Crear un objeto SparkSession. Para utilizar determinados orígenes/destinos de datos puede ser necesario usar alguna opción especial. Abajo podemos observar un ejemplo general y otro específico para usar Kafka como fuente/destino de datos.\n",
    "  2. Crear el Stream DataFrame/DataSet empleando alguna de las fuentes de datos disponibles\n",
    "  3. Realizar operaciones de transformación sobre el DataFrame/DataSet hasta dar a los datos la forma que se necesite.\n",
    "  4. Iniciar el procesamiento en Streaming, especificando un *sink* (destino de los datos)\n",
    "![Flujo de trabajo en Spark Structured Streaming](./images/flujo_sss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d231d7-38bd-4a48-8414-cb197b0d8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización del objeto SparkSession para socket y archivo\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "import string\n",
    "\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"StructuredNetworkWordCount\") \\\n",
    "            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6673079-10b0-4250-8a54-ec6cbcd2196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using packages ['org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0', 'org.apache.kafka:kafka-clients:3.5.1']\n"
     ]
    }
   ],
   "source": [
    "# Inicialización del objeto SparkSession para Kafka\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "scala_version = '2.12'  # TODO: Ensure this is correct\n",
    "spark_version = pyspark.__version__\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.5.1'\n",
    "]\n",
    "\n",
    "args = os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "if not args:\n",
    "    args = f'--packages {\",\".join(packages)}'\n",
    "    print('Using packages', packages)\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = f'{args} pyspark-shell'\n",
    "else:\n",
    "    print(f'Found existing args: {args}') \n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "   .master(\"local\")\\\n",
    "   .appName(\"kafka-example\")\\\n",
    "   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76507f47-0529-47b4-a4c9-11c95a039768",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Creación de Streaming DataFrames \n",
    "Los Streaming DataFrames pueden crearse a través de la interfaz DataStreamReader que se obtiene a través de la función readStream() del objeto SparkSession.\n",
    "Existen diversas fuentes de entrada:\n",
    "- **Archivo**: Lee los ficheros de un directorio como un flujo de datos.Los archivos se procesan en orden según la hora de modificación del archivo. La opción latestFirst permite invertir el orden. Se soportan varios formatos, como texto, CSV, JSON, ORC o Parquet. Los archivos deben estar colocados directamente en el directorio indicado.\n",
    "- **Kafka**: Lee datos desde Kafka. Compatible con la versión 0.10.0 o superiores.\n",
    "- **Socket (pruebas)**: Lee texto en formato UTF8 a través de una conexión tipo socket. Esta opción solo es recomendada para pruebas\n",
    "- **Rate source (pruebas)**: Genera datos a un número especificado de filas por segundo, cada una de ellas con un *timestamp* y un *valor* de tipo *Long* que contiene el conteo de mensajes (empezando en 0). Esta fuente se utiliza para pruebas y *benchmarks*\n",
    "- **Rate per Micro-Batch source (pruebas)**:\n",
    "\n",
    "### Archivo\n",
    "Al utilizar la fuente de entrada *archivo* tenemos varias opciones que configurar:\n",
    "- **path**: Ruta al directorio de entrada\n",
    "- **maxFilesPerTrigger**: Máximo número de archivos nuevos para considerar en cada intervalo (por defecto: no hay)\n",
    "- **latestFirst**: Indica que lor achivos más recientes se han de procesar primero. (por defecto: *falso*).\n",
    "- **fileNameOnly**: Indica si comprobamos los nuevos archivo en base a su nombre en vez de a la ruta absoluta. Así, varios ficheros con el mismo nombre pero ubicados en rutas distintas serían considerados el mismo archivo.\n",
    "- **maxFileAge**: Antigüedad máxima de archivo, los que sobrepasen dicha antigüedad serán ignorados. Si las opciones *latestFirst* y *maxFilesPerTrigger* tienen valores ésta será ignorada. La antigüedad máxima see considera respecto al *timestamp* del archivo más nuevo, no al actual (por defecto: 1 semana).\n",
    "- **cleanSource**: Opción de \"limpiar\" los archivos completados después de su procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd5bbd-e233-4b04-a23b-81b67da308cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a86fe5-bcbc-4dc6-b335-6cf04c136427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de file source\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", IntegerType(), True)\n",
    "])\n",
    "streaming_file_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .csv(\"/home/jovyan/work/data/flight-data/csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f571a-4d06-4e05-8dab-0d55e1e40faf",
   "metadata": {},
   "source": [
    "### Socket\n",
    "Al utilizar la fuente **socket** (sólo pruebas) deben especificarse las siguientes opciones:\n",
    "- **host**: Nombre de host al que conectarse, obligatorio.\n",
    "- **port**: Número de puerto al que conectarse, obligatorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f13dcf0-1125-4f02-b9cf-1370fc59f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de socket source\n",
    "\n",
    "socket_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f8b06-3a27-4799-8461-b4e6b477f3a3",
   "metadata": {},
   "source": [
    "### Kafka\n",
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bc1cf1-44c9-4931-9a54-6682e4bab51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo de kafka\n",
    "\n",
    "# Suscribirse a un topic (entrada)\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-1:9092\") \\\n",
    "  .option(\"subscribe\", \"entrada\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c805829-6be6-4d25-9bb7-21a030a4296a",
   "metadata": {},
   "source": [
    "## 2. Operaciones con Streaming DataFrames\n",
    "Se pueden aplicar distintos tipos de operaciones sobre *Streaming DataFrames*, desde operacions estilo SQL (select, where, groupBy...) a operaciones tipadas estilo RDD (map, filter, flatMap...).\n",
    "### Operaciones básicas\n",
    "La mayoría de las operaciones comunes sobre *DataFrames* están soportadas también en Streaming. Más adealente se hablará de las que no lo están"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cf60a-1668-4198-b831-1f354d8588b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo operaciones básicas\n",
    "\n",
    "df = ...  # streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: DateType }\n",
    "\n",
    "# Select the devices which have signal more than 10\n",
    "df.select(\"device\").where(\"signal > 10\")\n",
    "\n",
    "# Running count of the number of updates for each device type\n",
    "df.groupBy(\"deviceType\").count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85532be6-ce57-428d-8bb3-719d9882b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza operaciones de streaming, por ejemplo, contar los registros por cada grupo\n",
    "counts = streaming_file_df.groupBy(\"columna_a_agrupar\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0aa3e9-b8f1-404d-8e1d-fbfec0383375",
   "metadata": {},
   "source": [
    "### Ventanas\n",
    "Las operaciones de agregación sobre una ventana deslizante de tiempo de evento son directas en Structured Streaming y muy similares a las agregaciones agrupadas. La diferencia es que en las **agregaciones basadas en ventanas** los valores agregados se mantienen para cada ventana de tiempo de evento a la que pertenece una determinada fila.\n",
    "Imaginemos que en el ejemplo de **wordCounts**, además de los datos, cada fila contiene el instante en que fue generada. En vez de realizar un conteo total de las ocurrencias de cada palabra queremos contar dichas ocurrencias en intervalos de 10 minutos, actualizando los resultados cada 5. Las tablas de resultados tendrían un aspecto muy similar al siguiente: \n",
    "![ventanas deslizantes](./images/structured-streaming-window.png)\n",
    "Ya que las agrupaciones y las ventanas son operaciones similares, pueden usarse las operaciones *groupBy()* y *window()* para expresar agregaciones con ventanas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ed305-764b-4cb0-b466-a508a68ee813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo ventanas REVISAR\n",
    "\n",
    "words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }\n",
    "\n",
    "# Group the data by window and word and compute the count of each group\n",
    "windowedCounts = words.groupBy(\n",
    "    window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n",
    "    words.word\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b51920-cdf0-4faf-a5d9-75e34c666d69",
   "metadata": {},
   "source": [
    "### Manejo de datos tardíos y marcas de agua (Watermarking)\n",
    "¿Qué pasa si algún evento llega tarde a la aplicación? Es posible que el tiempo de evento de los datos sea anterior al intervalo de la ventana. Structured Streaming puede mantener el estado intermedio para agregaciones parciales por un largo periodo de tiempo, consiguiendo así que los datos tardíos actualicen agregaciones de ventanas antiguas, como se muestra en la siguiente ilustración:\n",
    "![datos tardíos](./images/structured-streaming-late-data.png)\n",
    "Para ejecutar esta consulta durante largos periodos de tiempo es necesario especificar la cantidad de memoria que reserval sistema para almacenar estados intermedios. Es decir, el sistema necesita saber cuando puede  eliminar de memoria una agregación antigua debido a que la aplicación no va a recibir más datos para esa ventana. Debido a esto, en la versión 2.1 de Spark se introduce el concepto de **watermarking**. Podemos definir la marca de agua (*watermark*) de una consulta especificando la columna de *tiempo de evento* y el umbral de cómo de tarde se espera que puedan llegar los datos a la aplicación. Es decir, los datos tardíos que lleguen durante ese umbral serán añadidos y los que lleguen más tarde descartados. Para definir una marca de agua basta con usar la función *withWatermark()*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c3771-836f-4077-9781-f54fc9af2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo Watermark:\n",
    "words = ...  # streaming DataFrame of schema { timestamp: Timestamp, word: String }\n",
    "\n",
    "# Group the data by window and word and compute the count of each group\n",
    "windowedCounts = words \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(words.timestamp, \"10 minutes\", \"5 minutes\"),\n",
    "        words.word) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4def9-9038-48e8-8cd4-4b178f5cd635",
   "metadata": {},
   "source": [
    "EJEMPLOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e06b59-69da-4a2a-80fe-8f6dc7226060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "grouped_streaming_file_df = streaming_file_df.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
    "\n",
    "#windowed_file_df = streaming_file_df \\\n",
    "#   .withWaterMark(\"timestamp\", \"10 minutes\") \\\n",
    "#    .grooupBy(\n",
    "#       window("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd781a1-fa51-4964-856f-90bb3f60c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# Split the lines into words\n",
    "words = socket_df.select(\n",
    "        explode(\n",
    "split(socket_df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    " # Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c48ac1b-f72e-48f8-bcf3-24939b683f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "words = df.select(\n",
    "        explode(\n",
    "split(df.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    " # Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52ecad-80cf-4c1e-a9c7-70fab236f0be",
   "metadata": {},
   "source": [
    "## 3. Inicio del procesamiento en Streaming\n",
    "Una vez está definido el DataFrame resultado, todo lo que queda es inicial la computación en streaming. Para ello, se usa el objeto *DataStreamWriter* devuelto por la función *writeStream()* de la clase *DataFrame*. Es necesario espeficar algunos de los siguientes aspectos:\n",
    "- **sink**: Sumidero o destino de los datos. Hay que especificar formato, localización, etc.\n",
    "- **Output mode**: Permite especificar qué se escribe en el *sink* de salida.\n",
    "- **Query**: Opcionalmente, se puede especificar un nombre único para identificar la consulta.\n",
    "- **Trigger interval**: Intervalo \n",
    "- **Checkpoint location**: En algunos *sinks* es necesario especificar la localización donde se va a almacenar toda la información de control par garantizar la tolerancia a fallos de extremo a extremo (*end-to-end fault-tolerance*).\n",
    "### Modos de salida\n",
    "- **Modo adición (por defecto)**: Sólo se envían al destino las nuevas filas añadidas a la Tabla Resultado durante el último intervalo. Por lo tanto, sólo soportan este modo consultas que no cambian las filas que ya han sido añadidas a la Tabla Resultado ( select, where, map, flatmap, filter, join, etc.).\n",
    "- **Modo completo**: La Tabla Resultado al completo es enviada al destino de datos en cada intervalo. Este modo soporte consultas de agregación.\n",
    "- **Modo actualización**: Sólo se envían al destino las filas modificadas durante el último intervalo.\n",
    "No todos los modos son adecuados para todos los tipos de consultas. A continuación se muestra una tabla de compatibilidades e incompatibilidades:\n",
    "|Tipo de consulta | subtipo | Modos soportados | notas |\n",
    "| --------------- | ------- | ---------------- | ----- |\n",
    "| Consultas con agregación | Agregación en evento-tiempo con watermark | Append, Update, Complete | texto |\n",
    "| | Otras agregaciones | Complete, Update | texto |\n",
    "| Consultas con mapGroupsWithState | | Update | No se permiten agregaciones |\n",
    "| Consultas con flatMapGroupsWithState | Modo de operación Append | Append | Se permiten agregaciones **después** de la transformación *mapGroupsWithState* |\n",
    "| | Modo de operación Update | Update | No se permiten agregaciones **después** de la transformación *mapGroupsWithState* |\n",
    "| Consultas con joins | | Append | Los modos update y complete no está soportados todavía |\n",
    "| Otras consultas | | Append, Update | El modo complete no está soportado |\n",
    "### Sinks (Destinos de datos)\n",
    "Hay los siguientes tipos de *sinks*:\n",
    "- File sink: Almacena la salida en un directorio\n",
    "- Kafka sink: Almacena la salida en uno o varios topics de Kafka\n",
    "- Foreach sink: Realiza un procesamiento adicional sobre las filas de la salida.\n",
    "- Console sink (pruebas): Imprime la salida por consola cada intervalo. Soporta los modos completo y adición. NOTA: En jupyter lo imprime en la consola, no en el notebook.\n",
    "- Memory sink (pruebas): Almacena la salida en memoria. Soporta los modos adición y completo. NOTA: Para hacer pruebas en jupyter es el más indicado. En los ejemplos se muestra como se pueden visualizar los datos.\n",
    "\n",
    "Algunos *sinks* no son tolerantes a fallos y están pensados sólo para pruebas y depuración. A continuación se muestra una tabla comparativa de *sinks*:\n",
    "\n",
    "| Sink | Modos soportados | opciones | Tolerancia a fallos | Notas |\n",
    "| ---- | ---------------- | -------- | ------------------- | ----- |\n",
    "| File | Adición | path: la ruta al directorio de salida | Sí (exactly-once) | Soporta escritura a tablas particionadas. |\n",
    "| Kafka | Adición, actualización y completo | Más adelante | Sí (at-least-once) | Más adelante |\n",
    "| Foreach | Adición, actualización y completo | - | Depende de la implementación | Más adelante | \n",
    "| Console | Adición, actualización y completo | *numRows*: número de filas a imprimir en cada intervalo (por defecto: 20). *truncate*: si truncar o no cuando la salida es demadsiado larga (por defecto: true) | No | |\n",
    "| Memory | Adición, completo | - | No | El nombre de la tabla es el de la consulta |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b1fb6-1ac9-4775-bd7d-15c1767de382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File sink\n",
    "#1 \n",
    "query = streaming_file_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/home/jovyan/output\") \\\n",
    "    .option(\"topic\", \"notificaciones\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/chk-point-dir\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79aa47cb-ce84-498b-91a8-2a71ccbfe586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka sink\n",
    "\n",
    "from pyspark.sql.functions import to_json\n",
    "kafka_output_topic = \"salida\"\n",
    "\n",
    "kafka_output_config = {\n",
    "    \"kafka.bootstrap.servers\": \"kafka-1:9092\",  # Coloca aquí los servidores de arranque de Kafka\n",
    "    \"topic\": kafka_output_topic\n",
    "}\n",
    "\n",
    "query2 = wordCounts \\\n",
    "    .selectExpr(\"CAST(word AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .options(**kafka_output_config) \\\n",
    "    .option(\"checkpointLocation\", \"/home/jovyan/checkpoints\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caefadb9-1a1c-4d48-9ea4-8e45648d4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreach\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e2d89d-6f5d-4057-aac0-6757888d9a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ForeachBatch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55271675-9a22-4e6c-a696-d9ee11613e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Console sink\n",
    "from time import sleep  # Agregar importación para sleep\n",
    "query = grouped_streaming_file_df.writeStream.format(\"console\").outputMode(\"complete\").start()\n",
    "#query.awaitTermination()\n",
    "\n",
    "print (\"CONSULTA ACTIVA!!!!\")\n",
    "#while True:\n",
    "   \n",
    "#    print(query.status)\n",
    "#    print(spark.sql('SELECT * FROM consulta_file2').show())\n",
    "#    sleep(1)\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe412c-c592-4e21-afe4-7d2343fbc529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory sink\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep  # Agregar importación para sleep\n",
    "\n",
    " # Start running the query that prints the running counts to the console\n",
    "query = wordCounts \\\n",
    "          .writeStream \\\n",
    "          .outputMode(\"complete\") \\\n",
    "          .format(\"memory\") \\\n",
    "          .queryName(\"consulta1\") \\\n",
    "          .start()\n",
    "\n",
    "\n",
    "\n",
    "while query.isActive:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM consulta1').show())\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cde0ce-dc44-49d0-a5db-d9f312bb0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory sink 2\n",
    "from IPython.display import display, clear_output\n",
    "from time import sleep  # Agregar importación para sleep\n",
    "\n",
    " # Start running the query that prints the running counts to the console\n",
    "query = grouped_streaming_file_df \\\n",
    "          .writeStream \\\n",
    "          .outputMode(\"complete\") \\\n",
    "          .format(\"memory\") \\\n",
    "          .queryName(\"consulta2\") \\\n",
    "          .start()\n",
    "\n",
    "\n",
    "\n",
    "while query.isActive:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM consulta2').show())\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489661c-459b-48dd-8527-8dd922de5515",
   "metadata": {},
   "source": [
    "### Disparadores\n",
    "La configuración de los disparadores de una consulta en streaming define la temporalización del procesamiento de datos, si se va ejecutar como una consulta micro-batch o como una consulta de procesamiento continuo. Estos son los distintos tipos de disparadores soportados:\n",
    "#### Por defecto\n",
    "Si no se especifica el tipo de disparador la consulta se ejecutará en modo **micro-batch**, donde cada micro-batch se generará en el momento en que el anterior haya completado su procesamiento\n",
    "#### Micro-batches de intervalo fijo\n",
    "La consulta se ejecutará en modo **micro-batch**, donde cada micro-batch se lanzará en intervalos especificados por el usuario.\n",
    "- Si el micro-batch anterior finaliza durante el intervalo, el motor esperará a que éste termine antes de lanzar el siguiente micro-batch\n",
    "- Si el micro-batch anterior lleva más tiempo que el intervalo, el siguiente se lanzará una vez termine el procesamiento.\n",
    "- Si no hay nuevos datos disponibles, no se lanzará ningún micro-batch.\n",
    "#### One-time micro-batch (obsoleto)\n",
    "La consulta ejecutará **un único** micro-batch para procesar todos los datos disponibles y luego detenerse. Está obsoleto y se recomienda el uso del disparador **Available-now micro-batch** en su lugar.\n",
    "#### Available-now micro-batch\n",
    "Similar al anterior. La diferencia radica en que los datos serán procesados (posiblemente) en múltiples micro-batches según las opciones de la fuente de datos (por ejemplo *maxFilesPerTrigger*), lo que proporciona una mayor escalabilidad.\n",
    "- Este disparador proporciona una fuerte garantía de procesamiento: Independientemente del número de batches que falten de la anterior ejecución, adegura que todos los datos disponibles serán ejecutados antes de la terminación. Los batchs incompletos serán ejecutados primero.\n",
    "- La *marca de agua* avanza por cada batch. REVISAR\n",
    "#### Continuo con intervalo fixed-point (experimental)\n",
    "La consulta se ejecutará en modo de procesamiento continuo de baja latencia. Es un modo experimetal y presenta una serie de incompatibilidades:\n",
    "- *Operaciones*: Sólo proyecciones (select, map, flatMap, mapPartitions, etc.) y selecciones (where, filter, etc.).\n",
    "- *Fuentes*:\n",
    "  - Kafka: Todas las opciones permitidas\n",
    "  - Rate source: sólo numPartitions y rowsPerSecond\n",
    "- *Destinos*:\n",
    "  - Kafka: Todas las opciones permitidas.\n",
    "  - Memory: Válido para depurar.\n",
    "  - Console: Todas las opciones permitidas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406a0a3-8337-4ab9-ba23-d4a819a66894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por defecto\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18704031-5f79-4780-9cae-83980ac28d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed-interval micro-batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e9a5a-f090-481b-a966-43ffa6e13818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time micro-batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d4f7a-6496-4af8-95d8-6a6b872cfd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available-now micro-batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a699008-1501-4944-a23d-918920bf86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous mode\n",
    "\n",
    "spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load() \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"topic\", \"topic1\") \\\n",
    "  .trigger(continuous=\"1 second\") \\\n",
    "  .start()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
